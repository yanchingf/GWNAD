{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, 1, 0.1), 2*np.arange(0, 1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch import nn, optim\n",
    "import scipy.io as sio\n",
    "# import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "# import readligo as rl\n",
    "# from gwpy.timeseries import TimeSeries\n",
    "import math\n",
    "import random\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:\\\\OneDrive - HKUST Connect\\Research\\GWNMMAD\\GWAD\\Codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_wsl = 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "rTrain = 0.7;\n",
    "rTest = 0.2;\n",
    "# input_vector_length = 100\n",
    "batch_size = 32\n",
    "num_bins = 40\n",
    "coef_delta = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-AE + WSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class AutoEncoder_1det(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder_1det, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(101, 20),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(20, 101),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(202, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(10, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 202),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class WSClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WSClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(101, 32)  # 第一层全连接层，输入维度为4，输出维度为64\n",
    "        self.norm1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()  # 激活函数\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(32, 8)\n",
    "        self.norm2 = nn.BatchNorm1d(8)\n",
    "        self.fc4 = nn.Linear(8, 1)  # 第三层全连接层，输入维度为32，输出维度为类别数目\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        nn.init.kaiming_normal_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(self.relu(self.fc1(x)))\n",
    "        x = self.norm2(self.relu(self.fc2(x)))\n",
    "        return self.fc4(x)\n",
    "        # x = self.relu(x)\n",
    "#         x = self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WSClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(101, 32)  # 第一层全连接层，输入维度为4，输出维度为64\n",
    "        self.norm1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()  # 激活函数\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        # self.norm2 = nn.BatchNorm1d(8)\n",
    "        # self.fc4 = nn.Linear(8, 1)  # 第三层全连接层，输入维度为32，输出维度为类别数目\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        # nn.init.kaiming_normal_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(self.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "        # x = self.relu(x)\n",
    "#         x = self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WSClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(202, 32)  # 第一层全连接层，输入维度为4，输出维度为64\n",
    "        self.norm1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()  # 激活函数\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        # self.norm2 = nn.BatchNorm1d(8)\n",
    "        # self.fc4 = nn.Linear(8, 1)  # 第三层全连接层，输入维度为32，输出维度为类别数目\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        # nn.init.kaiming_normal_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(self.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "        # x = self.relu(x)\n",
    "#         x = self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ae = AutoEncoder().cuda()\n",
    "print(sum(p.numel() for p in ae.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wsc = WSClassifier().cuda()\n",
    "print(sum(p.numel() for p in wsc.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datatype = [\"noise\", \"bbh\", \"sg\", \"glitch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_wsl_total = 3000;\n",
    "N_wsl = {}\n",
    "N_wsl[\"noise\"] = int(0.75*N_wsl_total)\n",
    "N_wsl[\"bbh\"] = int(0.1*N_wsl_total)\n",
    "N_wsl[\"sg\"] = int(0.1*N_wsl_total)\n",
    "N_wsl[\"glitch\"] = int(0.05*N_wsl_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_wsl_total = 30000;\n",
    "N_wsl = {}\n",
    "N_wsl[\"noise\"] = int(0.75*N_wsl_total)\n",
    "N_wsl[\"bbh\"] = int(0.1*N_wsl_total)\n",
    "N_wsl[\"sg\"] = int(0.1*N_wsl_total)\n",
    "N_wsl[\"glitch\"] = int(0.05*N_wsl_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_wsl_total = 30000;\n",
    "N_wsl = {}\n",
    "N_wsl[\"noise\"] = int(0.85*N_wsl_total)\n",
    "N_wsl[\"bbh\"] = int(0.04*N_wsl_total)\n",
    "N_wsl[\"sg\"] = int(0.04*N_wsl_total)\n",
    "N_wsl[\"glitch\"] = int(0.07*N_wsl_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renorm_factor_0 = 20;\n",
    "renorm_factor_1 = 20;\n",
    "\n",
    "realbkg = np.load('E://GWNMMAD_data/Tw_dataset/Datasets/background.npz')['data'].reshape(-1,200) / renorm_factor_0;\n",
    "realbbh = np.load('E://GWNMMAD_data/Tw_dataset/Datasets/bbh_for_challenge.npy').reshape(-1,200) / renorm_factor_0;\n",
    "\n",
    "realsg = np.load('E://GWNMMAD_data/Tw_dataset/Datasets/sglf_for_challenge.npy').reshape(-1,200) / renorm_factor_0;\n",
    "# realglitch = np.load(\"../data/real_glitches_9998_4000Hz_25ms.npz\")[\"strain_time_data\"]\n",
    "realglitch_L = np.load(\"../Data_cached/real_glitches_snrlt5_60132_4000Hz_25ms.npz\")[\"strain_time_data\"][:30000].reshape(-1,1,200) / renorm_factor_1\n",
    "realglitch_H = np.load('../Data_cached/real_glitches_H_snrlt5_59732_4000Hz_25ms.npz')[\"strain_time_data\"][:30000].reshape(-1,1,200) / renorm_factor_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If one is using Chia-Jui's data\n",
    "\n",
    "renorm_factor_0 = 20;\n",
    "renorm_factor_1 = 20;\n",
    "\n",
    "realbkg_L = np.load('../Data_cached/real_bkg_2202000_63917s_4000Hz_50ms.npy')[:1000000].reshape(-1,1,200) / renorm_factor_0;\n",
    "realbkg_H = np.load('../Data_cached/real_bkg_H_1466640_58803s_4000Hz_50ms.npy')[:1000000].reshape(-1,1,200) / renorm_factor_0;\n",
    "\n",
    "realbkg = np.concatenate((realbkg_L, realbkg_H), axis = 1).reshape(-1,200)\n",
    "\n",
    "realbbh = np.load('../Data_cached/injected_BBH_1823_around_merger_time_63917_58803.npz')['strain'].reshape(-1,200) / renorm_factor_0;\n",
    "\n",
    "realsg = np.load('../Data_cached/injected_lfsg_1835_around_merger_time_63917_58803.npz')['strain'].reshape(-1,200) / renorm_factor_0;\n",
    "# realglitch = np.load(\"../data/real_glitches_9998_4000Hz_25ms.npz\")[\"strain_time_data\"]\n",
    "realglitch_L = np.load(\"../Data_cached/real_glitches_snrlt5_60132_4000Hz_25ms.npz\")[\"strain_time_data\"][:30000].reshape(-1,1,200) / renorm_factor_1\n",
    "realglitch_H = np.load('../Data_cached/real_glitches_H_snrlt5_59732_4000Hz_25ms.npz')[\"strain_time_data\"][:30000].reshape(-1,1,200) / renorm_factor_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal procedure for one glitch + one noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_first50k = realbkg.reshape(-1,2,200)[:30000]\n",
    "\n",
    "glitch_L_noise_H = np.concatenate((realglitch_L, noise_first50k[:,[1],:]), axis = 1)\n",
    "glitch_H_noise_L = np.concatenate((realglitch_H, noise_first50k[:,[0],:]), axis = 1)\n",
    "realglitch = np.vstack((glitch_L_noise_H, glitch_H_noise_L))\n",
    "np.random.shuffle(realglitch)\n",
    "realglitch = realglitch.reshape(-1,200)\n",
    "realbkg = realbkg[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realbkg_reserved = realbkg[-int(0.2 * len(realbkg)):]\n",
    "realbbh_reserved = realbbh[-int(0.2 * len(realbbh)):]\n",
    "realsg_reserved = realsg[-int(0.2 * len(realsg)):]\n",
    "realglitch_reserved = realglitch[-int(0.2 * len(realglitch)):]\n",
    "\n",
    "realbkg = realbkg[:-int(0.2 * len(realbkg))]\n",
    "realbbh = realbbh[:-int(0.2 * len(realbbh))-int(0.2 * len(realbbh))%2]\n",
    "realsg = realsg[:-int(0.2 * len(realsg))-int(0.2 * len(realsg))%2]\n",
    "realglitch = realglitch[:-int(0.2 * len(realglitch))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_fft = abs(np.fft.rfft(realbkg))\n",
    "bkg_fft = bkg_fft/np.linalg.norm([bkg_fft], axis=2).T\n",
    "\n",
    "bbh_fft = abs(np.fft.rfft(realbbh))\n",
    "bbh_fft = bbh_fft/np.linalg.norm([bbh_fft], axis=2).T\n",
    "\n",
    "sg_fft = abs(np.fft.rfft(realsg))\n",
    "sg_fft = sg_fft/np.linalg.norm([sg_fft], axis=2).T\n",
    "\n",
    "glitch_fft = abs(np.fft.rfft(realglitch))\n",
    "glitch_fft = glitch_fft/np.linalg.norm([glitch_fft], axis=2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novel procedure. For training sample, the model is purely glitch trained. For testing and WSL sample, the model is one glitch + one noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_glitch_for_AE = int(len(realglitch_L) * 0.8) - N_wsl['glitch'] // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_glitch_one_noise = int(len(realglitch_L) * 0.2) + N_wsl['glitch'] // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(realglitch_L)\n",
    "np.random.shuffle(realglitch_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realglitch_L.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realbkg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glitch_for_ae = np.concatenate((realglitch_L[:num_glitch_for_AE], realglitch_H[:num_glitch_for_AE]), axis=1).reshape(-1,200)\n",
    "glitch_for_ae_fft = abs(np.fft.rfft(glitch_for_ae))\n",
    "glitch_for_ae_fft = glitch_for_ae_fft/np.linalg.norm([glitch_for_ae_fft], axis=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_for_glitch_building = realbkg[:2 * num_one_glitch_one_noise].reshape(-1,2,200)\n",
    "\n",
    "glitch_L_noise_H = np.concatenate((realglitch_L[-num_one_glitch_one_noise:], noise_for_glitch_building[:,[1],:]), axis = 1)\n",
    "noise_L_glitch_H = np.concatenate((noise_for_glitch_building[:,[0],:], realglitch_H[-num_one_glitch_one_noise:]), axis = 1)\n",
    "one_glitch_one_noise = np.vstack((glitch_L_noise_H, noise_L_glitch_H))\n",
    "np.random.shuffle(one_glitch_one_noise)\n",
    "one_glitch_one_noise = one_glitch_one_noise.reshape(-1,200)\n",
    "\n",
    "realbkg = realbkg[2 * num_one_glitch_one_noise:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_glitch_one_noise_fft = abs(np.fft.rfft(one_glitch_one_noise))\n",
    "one_glitch_one_noise_fft = one_glitch_one_noise_fft/np.linalg.norm([one_glitch_one_noise_fft], axis=2).T\n",
    "\n",
    "one_glitch_one_noise = one_glitch_one_noise / np.linalg.norm([one_glitch_one_noise_fft], axis=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realbkg_reserved = realbkg[-int(0.2 * len(realbkg)):]\n",
    "realbbh_reserved = realbbh[-int(0.2 * len(realbbh)):]\n",
    "realsg_reserved = realsg[-int(0.2 * len(realsg)):]\n",
    "# realglitch_reserved = realglitch[-int(0.2 * len(realglitch)):]\n",
    "\n",
    "realbkg = realbkg[:-int(0.2 * len(realbkg))]\n",
    "realbbh = realbbh[:-int(0.2 * len(realbbh))-int(0.2 * len(realbbh))%2]\n",
    "realsg = realsg[:-int(0.2 * len(realsg))-int(0.2 * len(realsg))%2]\n",
    "# realglitch = realglitch[:-int(0.2 * len(realglitch))]\n",
    "bkg_fft = abs(np.fft.rfft(realbkg))\n",
    "bkg_fft = bkg_fft/np.linalg.norm([bkg_fft], axis=2).T\n",
    "\n",
    "bbh_fft = abs(np.fft.rfft(realbbh))\n",
    "bbh_fft = bbh_fft/np.linalg.norm([bbh_fft], axis=2).T\n",
    "\n",
    "sg_fft = abs(np.fft.rfft(realsg))\n",
    "sg_fft = sg_fft/np.linalg.norm([sg_fft], axis=2).T\n",
    "\n",
    "# glitch_fft = abs(np.fft.rfft(realglitch))\n",
    "# glitch_fft = glitch_fft/np.linalg.norm([glitch_fft], axis=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_fft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glitch_fft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh_fft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbh_fft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_fft = bkg_fft.reshape(-1,202)\n",
    "\n",
    "bbh_fft = bbh_fft.reshape(-1,202)\n",
    "\n",
    "sg_fft = sg_fft.reshape(-1,202)\n",
    "\n",
    "# glitch_fft = glitch_fft.reshape(-1,202)\n",
    "\n",
    "glitch_for_ae_fft = glitch_for_ae_fft.reshape(-1,202)\n",
    "\n",
    "one_glitch_one_noise_fft = one_glitch_one_noise_fft.reshape(-1,202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del realbkg_L, realbkg_H, realglitch_L, realglitch_H, glitch_L_noise_H, glitch_H_noise_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_raw = {};\n",
    "# dataset_raw[\"noise\"] = np.load(\"E://GWNMMAD_data/Tw_dataset/Datasets/background.npz\")['data'].reshape(-1,200);\n",
    "# dataset_raw[\"bbh\"] = np.load(\"E://GWNMMAD_data/Tw_dataset/Datasets/bbh_for_challenge.npy\").reshape(-1,200);\n",
    "# dataset_raw[\"sg\"] = np.load(\"E://GWNMMAD_data/Tw_dataset/Datasets/sglf_for_challenge.npy\").reshape(-1,200);\n",
    "# # realglitch = np.load(\"../data/real_glitches_9998_4000Hz_25ms.npz\")[\"strain_time_data\"]\n",
    "# dataset_raw[\"glitch\"] = np.load(\"../Data_cached/real_glitches_snrlt5_60132_4000Hz_25ms.npz\")[\"strain_time_data\"]\n",
    "\n",
    "# dataset_wsl = {};\n",
    "# dataset_ae = {};\n",
    "# dataset_wsl_fft = {};\n",
    "# dataset_ae_fft = {};\n",
    "\n",
    "# for dt in list_datatype:\n",
    "#     perm = np.random.permutation(len(dataset_raw[dt]))\n",
    "#     nwsl = N_wsl[dt]\n",
    "#     dataset_wsl[dt] = dataset_raw[dt][perm[:nwsl]]\n",
    "#     dataset_wsl[dt] = dataset_wsl[dt] / np.linalg.norm([dataset_wsl[dt]], axis=2).T\n",
    "#     dataset_wsl_fft[dt] = abs(np.fft.rfft(dataset_wsl[dt]))\n",
    "#     dataset_wsl_fft[dt] = dataset_wsl_fft[dt]/np.linalg.norm([dataset_wsl_fft[dt]], axis=2).T\n",
    "    \n",
    "#     dataset_ae[dt]  = dataset_raw[dt][perm[nwsl:]]\n",
    "#     dataset_ae[dt] = dataset_ae[dt] / np.linalg.norm([dataset_ae[dt]], axis=2).T\n",
    "#     dataset_ae_fft[dt] = abs(np.fft.rfft(dataset_ae[dt]))\n",
    "#     dataset_ae_fft[dt] = dataset_ae_fft[dt]/np.linalg.norm([dataset_ae_fft[dt]], axis=2).T\n",
    "    \n",
    "#     np.savetxt(\"../Data_Cached/SequentialTraining/WSL/perm_\"+dt+\"_2det.dat\", perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw_fft = {}\n",
    "\n",
    "dataset_raw_fft[\"noise\"] = bkg_fft\n",
    "dataset_raw_fft[\"bbh\"] = bbh_fft\n",
    "dataset_raw_fft[\"sg\"] = sg_fft\n",
    "dataset_raw_fft[\"glitch\"] = glitch_fft\n",
    "\n",
    "dataset_wsl = {};\n",
    "dataset_ae = {};\n",
    "dataset_wsl_fft = {};\n",
    "dataset_ae_fft = {};\n",
    "\n",
    "for dt in list_datatype:\n",
    "    # perm = np.random.permutation(len(dataset_raw_fft[dt]))\n",
    "    perm = np.loadtxt(\"../Data_Cached/SequentialTraining/WSL/perm_\"+dt+\"_2det_Chia-Jui_v7_GWAK.dat\").astype(int)\n",
    "    nwsl = N_wsl[dt]\n",
    "    dataset_wsl_fft[dt] = dataset_raw_fft[dt][perm[:nwsl]]\n",
    "    # dataset_wsl[dt] = dataset_wsl[dt] / np.linalg.norm([dataset_wsl[dt]], axis=2).T\n",
    "    # dataset_wsl_fft[dt] = abs(np.fft.rfft(dataset_wsl[dt]))\n",
    "    # dataset_wsl_fft[dt] = dataset_wsl_fft[dt]/np.linalg.norm([dataset_wsl_fft[dt]], axis=2).T\n",
    "    \n",
    "    dataset_ae_fft[dt]  = dataset_raw_fft[dt][perm[nwsl:]]\n",
    "    # dataset_ae[dt] = dataset_ae[dt] / np.linalg.norm([dataset_ae[dt]], axis=2).T\n",
    "    # dataset_ae_fft[dt] = abs(np.fft.rfft(dataset_ae[dt]))\n",
    "    # dataset_ae_fft[dt] = dataset_ae_fft[dt]/np.linalg.norm([dataset_ae_fft[dt]], axis=2).T\n",
    "    \n",
    "    # np.savetxt(\"../Data_Cached/SequentialTraining/WSL/perm_\"+dt+\"_2det_Chia-Jui_\"+version+\"_2.dat\", perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datatype = [\"noise\", \"bbh\", \"sg\", \"glitch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw_fft = {}\n",
    "\n",
    "dataset_raw_fft[\"noise\"] = bkg_fft\n",
    "dataset_raw_fft[\"bbh\"] = bbh_fft\n",
    "dataset_raw_fft[\"sg\"] = sg_fft\n",
    "# dataset_raw_fft[\"glitch\"] = glitch_fft\n",
    "\n",
    "# dataset_wsl = {};\n",
    "# dataset_ae = {};\n",
    "dataset_wsl_fft = {};\n",
    "dataset_ae_fft = {};\n",
    "\n",
    "for dt in list_datatype[0:3]:\n",
    "    perm = np.random.permutation(len(dataset_raw_fft[dt]))\n",
    "    # perm = np.loadtxt(\"../Data_Cached/SequentialTraining/WSL/perm_\"+dt+\"_2det_Chia-Jui_v7_GWAK.dat\").astype(int)\n",
    "    nwsl = N_wsl[dt]\n",
    "    dataset_wsl_fft[dt] = dataset_raw_fft[dt][perm[:nwsl]]\n",
    "    # dataset_wsl[dt] = dataset_wsl[dt] / np.linalg.norm([dataset_wsl[dt]], axis=2).T\n",
    "    # dataset_wsl_fft[dt] = abs(np.fft.rfft(dataset_wsl[dt]))\n",
    "    # dataset_wsl_fft[dt] = dataset_wsl_fft[dt]/np.linalg.norm([dataset_wsl_fft[dt]], axis=2).T\n",
    "    \n",
    "    dataset_ae_fft[dt]  = dataset_raw_fft[dt][perm[nwsl:]]\n",
    "    # dataset_ae[dt] = dataset_ae[dt] / np.linalg.norm([dataset_ae[dt]], axis=2).T\n",
    "    # dataset_ae_fft[dt] = abs(np.fft.rfft(dataset_ae[dt]))\n",
    "    # dataset_ae_fft[dt] = dataset_ae_fft[dt]/np.linalg.norm([dataset_ae_fft[dt]], axis=2).T\n",
    "    \n",
    "    # np.savetxt(\"../Data_Cached/SequentialTraining/WSL/perm_\"+dt+\"_2det_Chia-Jui_\"+version+\"_2.dat\", perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft['glitch'] = one_glitch_one_noise_fft[:N_wsl['glitch']]\n",
    "\n",
    "dataset_ae_fft['glitch'] = glitch_for_ae_fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(dataset_ae_fft['glitch'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_ae_fft.keys():\n",
    "    print(key)\n",
    "    print(dataset_ae_fft[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_ae_fft.keys():\n",
    "    print(key)\n",
    "    print(dataset_ae_fft[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_wsl_fft.keys():\n",
    "    print(key)\n",
    "    print(dataset_wsl_fft[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realglitch_reserved = one_glitch_one_noise_fft[N_wsl['glitch']:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_reserved_fft = abs(np.fft.rfft(realbkg_reserved))\n",
    "bkg_reserved_fft = bkg_reserved_fft/np.linalg.norm([bkg_reserved_fft], axis=2).T\n",
    "\n",
    "bbh_reserved_fft = abs(np.fft.rfft(realbbh_reserved))\n",
    "bbh_reserved_fft = bbh_reserved_fft/np.linalg.norm([bbh_reserved_fft], axis=2).T\n",
    "\n",
    "sg_reserved_fft = abs(np.fft.rfft(realsg_reserved))\n",
    "sg_reserved_fft = sg_reserved_fft/np.linalg.norm([sg_reserved_fft], axis=2).T\n",
    "\n",
    "glitch_reserved_fft = abs(np.fft.rfft(realglitch_reserved))\n",
    "glitch_reserved_fft = glitch_reserved_fft/np.linalg.norm([glitch_reserved_fft], axis=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_reserved_fft = bkg_reserved_fft.reshape(-1,202)\n",
    "\n",
    "bbh_reserved_fft = bbh_reserved_fft.reshape(-1,202)\n",
    "\n",
    "sg_reserved_fft = sg_reserved_fft.reshape(-1,202)\n",
    "\n",
    "glitch_reserved_fft = glitch_reserved_fft.reshape(-1,202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_reserved_test = {}\n",
    "\n",
    "N_reserved_test_total = 20000\n",
    "\n",
    "N_reserved_test['glitch'] = int(0.05 * N_reserved_test_total)\n",
    "N_reserved_test['noise'] = int(0.65 * N_reserved_test_total)\n",
    "N_reserved_test['bbh'] = int(0.15 * N_reserved_test_total)\n",
    "N_reserved_test['sg'] = int(0.15 * N_reserved_test_total)\n",
    "\n",
    "dataset_reserved_test_collected = np.vstack((glitch_reserved_fft[:N_reserved_test['glitch']], bkg_reserved_fft[:N_reserved_test['noise']],\n",
    "                                             bbh_reserved_fft[:N_reserved_test['bbh']], sg_reserved_fft[:N_reserved_test['sg']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_reserved_test_collected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft_collected = np.empty((0, dataset_wsl_fft[\"glitch\"].shape[1]))\n",
    "for dt in sequence:\n",
    "    dataset_wsl_fft_collected = np.vstack((dataset_wsl_fft_collected, dataset_wsl_fft[dt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = dataset_wsl_fft_collected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v9\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = [\"glitch\", \"noise\", \"bbh\", \"sg\"];\n",
    "ind2datatype = {};\n",
    "datatype2ind = {};\n",
    "for i, dt in enumerate(sequence):\n",
    "    ind2datatype[i] = dt;\n",
    "    datatype2ind[dt] = i;\n",
    "    \n",
    "torch.save(ind2datatype, \"../Data_cached/SequentialTraining/WSL/sequence_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = [\"glitch\", \"noise\", \"bbh\", \"sg\"];\n",
    "dataset_wsl_fft_collected = np.empty((0, dataset_wsl_fft[\"glitch\"].shape[1]))\n",
    "for dt in sequence:\n",
    "    dataset_wsl_fft_collected = np.vstack((dataset_wsl_fft_collected, dataset_wsl_fft[dt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_bkg = len(bkg_reserved_fft);\n",
    "# N_glitch = int(N_bkg/15);\n",
    "# N_bbh = int(N_bkg*2/15);\n",
    "# N_sg = int(N_bkg*2/15);\n",
    "\n",
    "# testset = np.empty((N_bkg+N_glitch+N_bbh+N_sg, len(bkg_reserved_fft[0])))\n",
    "\n",
    "# s = 0;\n",
    "# testset[s : s+N_glitch] = glitch_reserved_fft[np.random.permutation(len(glitch_reserved_fft))[:N_glitch]];\n",
    "# s += N_glitch;\n",
    "\n",
    "# testset[s : s+N_bkg] = bkg_reserved_fft[np.random.permutation(len(bkg_reserved_fft))[:N_bkg]];\n",
    "# s += N_bkg;\n",
    "\n",
    "# testset[s : s+N_bbh] = bbh_reserved_fft[np.random.permutation(len(bbh_reserved_fft))[:N_bbh]];\n",
    "# s += N_bbh;\n",
    "\n",
    "# testset[s : s+N_sg] = sg_reserved_fft[np.random.permutation(len(sg_reserved_fft))[:N_sg]];\n",
    "# s += N_sg;\n",
    "\n",
    "# correct_ans = np.concatenate(([0]*N_glitch, [1]*N_bkg, [2]*N_bbh, [3]*N_sg))\n",
    "\n",
    "# Nsample = {};\n",
    "# Nsample[\"glitch\"] = N_glitch;\n",
    "# Nsample[\"noise\"] = N_bkg;\n",
    "# Nsample[\"bbh\"] = N_bbh;\n",
    "# Nsample[\"sg\"] = N_sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ncut = 5;\n",
    "# cutList = {};\n",
    "\n",
    "# max_glitch = 0.0026;\n",
    "# min_glitch = 0.001;\n",
    "# cutList[\"glitch\"] = np.linspace(min_glitch, max_glitch, Ncut);\n",
    "\n",
    "# max_bkg = 0.0026;\n",
    "# min_bkg = 0.001;\n",
    "# cutList[\"noise\"] = np.linspace(min_bkg, max_bkg, Ncut);\n",
    "\n",
    "# max_bbh = 0.0024;\n",
    "# min_bbh = 0.0008;\n",
    "# cutList[\"bbh\"] = np.linspace(min_bbh, max_bbh, Ncut);\n",
    "\n",
    "# max_sg = 0.003;\n",
    "# min_sg = 0.0003;\n",
    "# cutList[\"sg\"] = np.linspace(min_sg, max_sg, Ncut);\n",
    "\n",
    "# torch.save(cutList, \"../Data_cached/SequentialTraining/WSL/cut_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ncut = 5;\n",
    "cutList = {};\n",
    "\n",
    "max_glitch = 0.001;\n",
    "min_glitch = 0.0024;\n",
    "cutList[\"glitch\"] = np.linspace(min_glitch, max_glitch, Ncut);\n",
    "\n",
    "max_bkg = 0.0008;\n",
    "min_bkg = 0.0018;\n",
    "cutList[\"noise\"] = np.linspace(min_bkg, max_bkg, Ncut);\n",
    "\n",
    "max_bbh = 0.0014;\n",
    "min_bbh = 0.0024;\n",
    "cutList[\"bbh\"] = np.linspace(min_bbh, max_bbh, Ncut);\n",
    "\n",
    "max_sg = 0.0008;\n",
    "min_sg = 0.0032;\n",
    "cutList[\"sg\"] = np.linspace(min_sg, max_sg, Ncut);\n",
    "\n",
    "torch.save(cutList, \"../Data_cached/SequentialTraining/WSL/cut_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ncut = 5;\n",
    "cutList = {};\n",
    "\n",
    "max_glitch = 0.001;\n",
    "min_glitch = 0.0024;\n",
    "cutList[\"glitch\"] = np.linspace(min_glitch, max_glitch, Ncut);\n",
    "\n",
    "max_bkg = 0.0008;\n",
    "min_bkg = 0.0018;\n",
    "cutList[\"noise\"] = np.linspace(min_bkg, max_bkg, Ncut);\n",
    "\n",
    "max_bbh = 0.0014;\n",
    "min_bbh = 0.0024;\n",
    "cutList[\"bbh\"] = np.linspace(min_bbh, max_bbh, Ncut);\n",
    "\n",
    "max_sg = 0.0008;\n",
    "min_sg = 0.0032;\n",
    "cutList[\"sg\"] = np.linspace(min_sg, max_sg, Ncut);\n",
    "\n",
    "# torch.save(cutList, \"../Data_cached/SequentialTraining/WSL/cut_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ncut = 5;\n",
    "cutList = {};\n",
    "\n",
    "max_glitch_L = 0.0008;\n",
    "min_glitch_L = 0.0024;\n",
    "cutList[\"glitch_L\"] = np.linspace(min_glitch_L, max_glitch_L, Ncut);\n",
    "\n",
    "max_glitch_H = 0.0004;\n",
    "min_glitch_H = 0.0024;\n",
    "cutList[\"glitch_H\"] = np.linspace(min_glitch_H, max_glitch_H, Ncut);\n",
    "\n",
    "max_bkg = 0.0008;\n",
    "min_bkg = 0.0018;\n",
    "cutList[\"noise\"] = np.linspace(min_bkg, max_bkg, Ncut);\n",
    "\n",
    "max_bbh = 0.0014;\n",
    "min_bbh = 0.0024;\n",
    "cutList[\"bbh\"] = np.linspace(min_bbh, max_bbh, Ncut);\n",
    "\n",
    "max_sg = 0.0008;\n",
    "min_sg = 0.0032;\n",
    "cutList[\"sg\"] = np.linspace(min_sg, max_sg, Ncut);\n",
    "\n",
    "torch.save(cutList, \"../Data_cached/SequentialTraining/WSL/cut_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {};\n",
    "# models[\"glitch\"] = torch.load(\"../Model_cached/4ae_3.pt\")\n",
    "# models[\"glitch\"].cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {};\n",
    "models[\"glitch\"] = torch.load('../Model_cached/2_det_oneglitchonenoise_Chia-Jui_glitch_trained_202-20-202.pt')\n",
    "models[\"glitch\"].cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAE(dataset, cutID, version, datatype):\n",
    "    \n",
    "    nTotal = len(dataset);\n",
    "    nTrain = int(rTrain * nTotal)\n",
    "    nTest = int(rTest * nTotal)\n",
    "\n",
    "    X_train = dataset[:nTrain]\n",
    "    X_test = dataset[-nTest:]\n",
    "    X_validation = dataset[nTrain:-nTest]\n",
    "\n",
    "    trainData = torch.FloatTensor(X_train)\n",
    "    testData = torch.FloatTensor(X_test)\n",
    "    validationData = torch.FloatTensor(X_validation)\n",
    "\n",
    "    train_dataset = TensorDataset(trainData)\n",
    "    test_dataset = TensorDataset(testData)\n",
    "    validation_dataset = TensorDataset(validationData)\n",
    "\n",
    "    trainDataLoader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validationDataLoader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "    autoencoder = AutoEncoder().cuda()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.00005)\n",
    "    loss_func = nn.MSELoss().cuda()\n",
    "    \n",
    "    loss_train = np.empty(epochs)\n",
    "    loss_validation = np.empty(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        autoencoder.train()\n",
    "        for batchidx, x in enumerate(trainDataLoader):\n",
    "            x = x[0].cuda()\n",
    "            encoded, decoded = autoencoder(x)\n",
    "            loss_overall = loss_func(decoded, x)\n",
    "            weighted_lossTrain = loss_overall\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            weighted_lossTrain.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batchidx, x in enumerate(validationDataLoader):\n",
    "                x = x[0].cuda()\n",
    "                encoded, decoded = autoencoder(x)\n",
    "                lossVal = loss_func(decoded, x)\n",
    "                val_loss += lossVal.item()\n",
    "\n",
    "            val_loss /= len(validationDataLoader)\n",
    "\n",
    "        loss_train[epoch] = weighted_lossTrain.item()\n",
    "        loss_validation[epoch] = val_loss\n",
    "    \n",
    "    autoencoder.cpu().eval()\n",
    "    _, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax[0].plot(loss_train)\n",
    "    ax[0].plot(loss_validation)\n",
    "    \n",
    "    dcd_train = autoencoder(torch.FloatTensor(X_train))[1].detach().numpy()\n",
    "    err_train = np.var(X_train-dcd_train, axis=1)\n",
    "    dcd_test = autoencoder(torch.FloatTensor(X_test))[1].detach().numpy()\n",
    "    err_test = np.var(X_test-dcd_test, axis=1)\n",
    "    foo = ax[1].hist(err_train, range=(0, max(err_train)), bins=50, density=True, histtype=\"step\")\n",
    "    foo = ax[1].hist(err_test, range=(0, max(err_train)), bins=50, density=True, histtype=\"step\")\n",
    "    \n",
    "    plt.savefig(\"../Pic_cached/SequentialTraining/WSL/training_AE_\"+cutID+\"_\" + version + \"_\" + datatype +\"_trained.jpg\")\n",
    "    plt.close()\n",
    "            \n",
    "    return autoencoder.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAE(dataset, cutID, version, datatype):\n",
    "    \n",
    "    nTotal = len(dataset);\n",
    "    nTrain = int(rTrain * nTotal)\n",
    "    nTest = int(rTest * nTotal)\n",
    "\n",
    "    X_train = dataset[:nTrain]\n",
    "    X_test = dataset[-nTest:]\n",
    "    X_validation = dataset[nTrain:-nTest]\n",
    "\n",
    "    trainData = torch.FloatTensor(X_train)\n",
    "    testData = torch.FloatTensor(X_test)\n",
    "    validationData = torch.FloatTensor(X_validation)\n",
    "\n",
    "    train_dataset = TensorDataset(trainData)\n",
    "    test_dataset = TensorDataset(testData)\n",
    "    validation_dataset = TensorDataset(validationData)\n",
    "\n",
    "    trainDataLoader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validationDataLoader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "    autoencoder = AutoEncoder().cuda()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.00005)\n",
    "    loss_func = nn.MSELoss().cuda()\n",
    "    \n",
    "    loss_train = np.empty(epochs)\n",
    "    loss_validation = np.empty(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        autoencoder.train()\n",
    "        for batchidx, x in enumerate(trainDataLoader):\n",
    "            x = x[0].cuda()\n",
    "            encoded, decoded = autoencoder(x)\n",
    "            loss_overall = loss_func(decoded, x)\n",
    "            weighted_lossTrain = loss_overall\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            weighted_lossTrain.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batchidx, x in enumerate(validationDataLoader):\n",
    "                x = x[0].cuda()\n",
    "                encoded, decoded = autoencoder(x)\n",
    "                lossVal = loss_func(decoded, x)\n",
    "                val_loss += lossVal.item()\n",
    "\n",
    "            val_loss /= len(validationDataLoader)\n",
    "\n",
    "        loss_train[epoch] = weighted_lossTrain.item()\n",
    "        loss_validation[epoch] = val_loss\n",
    "    \n",
    "    autoencoder.cpu().eval()\n",
    "    _, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax[0].plot(loss_train)\n",
    "    ax[0].plot(loss_validation)\n",
    "    \n",
    "    dcd_train = autoencoder(torch.FloatTensor(X_train))[1].detach().numpy()\n",
    "    err_train = np.var(X_train-dcd_train, axis=1)\n",
    "    dcd_test = autoencoder(torch.FloatTensor(X_test))[1].detach().numpy()\n",
    "    err_test = np.var(X_test-dcd_test, axis=1)\n",
    "    foo = ax[1].hist(err_train, range=(0, max(err_train)), bins=50, density=True, histtype=\"step\")\n",
    "    foo = ax[1].hist(err_test, range=(0, max(err_train)), bins=50, density=True, histtype=\"step\")\n",
    "    \n",
    "    plt.savefig(\"../Pic_cached/SequentialTraining/WSL/training_AE_\"+cutID+\"_\" + version + \"_\" + datatype +\"_trained.jpg\")\n",
    "    plt.close()\n",
    "            \n",
    "    return autoencoder.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWSC(dataset0, dataset1, cutID, version, datatype):\n",
    "# dataset0: bkg set from AE\n",
    "# dataset1: identified signal from AE\n",
    "    \n",
    "    nTotal0, nTotal1 = len(dataset0), len(dataset1);\n",
    "    nTrain0, nTrain1 = int(rTrain * nTotal0), int(rTrain * nTotal1)\n",
    "    nTest0 , nTest1  = int(rTest * nTotal0) , int(rTest * nTotal1)\n",
    "\n",
    "    X_train = np.concatenate((dataset0[:nTrain0], dataset1[:nTrain1]))\n",
    "    X_test = np.concatenate((dataset0[-nTest0:], dataset1[-nTest1:]))\n",
    "    X_validation = np.concatenate((dataset0[nTrain0:-nTest0], dataset1[nTrain1:-nTest1]))\n",
    "    \n",
    "    Y_train = np.concatenate((np.zeros((nTrain0, 1)), np.ones((nTrain1, 1))))\n",
    "    Y_test = np.concatenate((np.zeros((nTest0, 1)), np.ones((nTest1, 1))))\n",
    "    Y_validation = np.concatenate((np.zeros((dataset0[nTrain0:-nTest0].shape[0], 1)), np.ones((dataset1[nTrain1:-nTest1].shape[0], 1))))\n",
    "\n",
    "#     trainData = torch.FloatTensor(X_train)\n",
    "#     testData = torch.FloatTensor(X_test)\n",
    "#     validationData = torch.FloatTensor(X_validation)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(Y_train))\n",
    "    validation_dataset = TensorDataset(torch.FloatTensor(X_validation), torch.FloatTensor(Y_validation))\n",
    "#     train_dataset = TensorDataset(torch.FloatTensor(X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))), torch.FloatTensor(Y_train.reshape((Y_train.shape[0], 1, Y_train.shape[1]))))\n",
    "#     validation_dataset = TensorDataset(torch.FloatTensor(X_validation.reshape((X_validation.shape[0], 1, X_validation.shape[1]))), torch.FloatTensor(Y_validation.reshape((Y_validation.shape[0], 1, Y_validation.shape[1]))))\n",
    "\n",
    "    trainDataLoader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    validationDataLoader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle = True, drop_last=True)\n",
    "\n",
    "    wsc = WSClassifier().cuda()\n",
    "    optimizer = optim.Adam(wsc.parameters(), lr=0.00005)\n",
    "    loss_func = nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([nTrain0/nTrain1])).cuda()\n",
    "    \n",
    "    loss_train = np.empty(epochs)\n",
    "    loss_validation = np.empty(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "#         t0 = time.time()\n",
    "        wsc.train()\n",
    "        for batchidx, (x, y) in enumerate(trainDataLoader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            yprime = wsc(x)\n",
    "            loss = loss_func(yprime, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        wsc.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batchidx, (x, y) in enumerate(validationDataLoader):\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                yprime = wsc(x)\n",
    "                lossVal = loss_func(yprime, y)\n",
    "                val_loss += lossVal.item()\n",
    "\n",
    "            val_loss /= len(validationDataLoader)\n",
    "\n",
    "        loss_train[epoch] = loss.item()\n",
    "        loss_validation[epoch] = val_loss\n",
    "#         print(time.time() - t0)\n",
    "        \n",
    "    wsc.cpu().eval()\n",
    "    \n",
    "    _, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax[0].plot(loss_train)\n",
    "    ax[0].plot(loss_validation)\n",
    "    foo = ax[1].hist(nn.Sigmoid()(wsc(torch.FloatTensor(X_train))).detach().numpy().flatten(), range=(0, 1), bins=20, density=True, histtype=\"step\")\n",
    "    foo = ax[1].hist(nn.Sigmoid()(wsc(torch.FloatTensor(X_test ))).detach().numpy().flatten(), range=(0, 1), bins=20, density=True, histtype=\"step\")\n",
    "    \n",
    "    plt.savefig(\"../Pic_cached/SequentialTraining/WSL/training_WSC_\"+cutID+\"_\" + version + \"_\" + datatype +\"_trained.jpg\")\n",
    "    plt.close()\n",
    "    \n",
    "    return wsc.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = trainWSC(dataset0, dataset1, \"test\").cpu().eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0;\n",
    "\n",
    "ic = np.zeros(4, dtype=\"int\")\n",
    "\n",
    "# loop for only the cut in glitch, noise and bbh as it's not really meaningful to set cut in sg w/o new signals\n",
    "ic[2] = Ncut-1;\n",
    "ic[3] = Ncut-1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnt = 0;\n",
    "\n",
    "ic = np.zeros(4, dtype=\"int\")\n",
    "\n",
    "# loop for only the cut in glitch, noise and bbh as it's not really meaningful to set cut in sg w/o new signals\n",
    "ic[2] = Ncut-1;\n",
    "ic[3] = Ncut-1;\n",
    "\n",
    "# listResult = {};\n",
    "# listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)), dtype=\"int\");\n",
    "# listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype)-1), len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype)-1), 2))\n",
    "\n",
    "for ic[0], ic[1] in itertools.product(np.arange(Ncut), np.arange(Ncut)):\n",
    "# for ic[0], ic[1], ic[2], ic[3] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "    cnt += 1;\n",
    "    \n",
    "    if cnt < 11:\n",
    "        continue\n",
    "    # elif cnt > 85:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in sequence:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_filtered = dataset_wsl_fft_collected\n",
    "    \n",
    "    cutID = \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version\n",
    "        \n",
    "    for iPrev in range(2):\n",
    "        previousStep = ind2datatype[iPrev];\n",
    "        modelPrev = models[previousStep]; # previous step AE\n",
    "        \n",
    "        # train the WSC according to previous AE's cut\n",
    "        \n",
    "        dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "        \n",
    "        dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "        dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "        dcd = modelPrev(torch.FloatTensor(dataset_wsl_filtered))[1].detach().numpy();\n",
    "        dataset1 = dataset_wsl_filtered[np.var(dataset_wsl_filtered-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "        model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "        models[previousStep+\"_WSC\"] = model;\n",
    "        \n",
    "        # filter the data according to previous WSC\n",
    "        for j in range(iPrev, 4):\n",
    "            dt = ind2datatype[j];\n",
    "            dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "            data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "        \n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "        # train the current step AE\n",
    "        currentStep = ind2datatype[iPrev+1];\n",
    "        model = trainAE(data_filtered[currentStep], cutID, version, currentStep);\n",
    "        models[currentStep] = model;\n",
    "        \n",
    "    torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    print(models.keys())\n",
    "    \n",
    "#     dcd = {};\n",
    "#     err = {};\n",
    "#     ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "#     for datatype in list_datatype:\n",
    "#         dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "#         err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "        \n",
    "#     not_select = np.array([True]*len(testset));\n",
    "\n",
    "#     for iStep in range(len(list_datatype)):\n",
    "#         datatype = ind2datatype[iStep];\n",
    "#         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "#         ans[ind_pass] = iStep;\n",
    "#         not_select[ind_pass] = False;\n",
    "        \n",
    "#     ans[not_select] = -1;\n",
    "    \n",
    "#     listResult[\"cut\"][cnt] = ic;\n",
    "#     listResult[\"ans\"][cnt] = ans;\n",
    "    \n",
    "#     acc = np.zeros(len(ind2datatype));\n",
    "    \n",
    "#     for i in range(len(ind2datatype)):\n",
    "#         acc[i] = np.sum(np.logical_and(ans==i, correct_ans==i))/Nsample[ind2datatype[i]];\n",
    "        \n",
    "#     listResult[\"accuracy_4\"][cnt] = acc;\n",
    "    \n",
    "#     listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(Nsample[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "#                                      np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(Nsample[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "    # cnt += 1\n",
    "    print(cnt)\n",
    "    print(time.time() - t0)\n",
    "    \n",
    "# listResult[\"total_accuracy\"] = np.sum(listResult[\"ans\"]==correct_ans, axis=1)/len(testset);\n",
    "# torch.save(listResult, \"../data/SequentialTraining/training_performance_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = dataset_wsl_fft_collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = dataset_reserved_test_collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans = np.hstack(([0]*N_wsl['glitch'], [1]*N_wsl['noise'], [2]*N_wsl['bbh'], [3]*N_wsl['sg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans_withoutsignal = np.hstack(([0]*N_wsl['glitch'], [1]*N_wsl['noise'], [-1]*(N_wsl['bbh']+N_wsl['sg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans_withoutsignal = np.hstack(([0]*N_reserved_test['glitch'], [1]*N_reserved_test['noise'], [2]*N_reserved_test['bbh'], [3]*N_reserved_test['sg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans_withoutsignal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic[3] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datatype_withoutsignal = [\"noise\", \"glitch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_withoutsignal = np.array([0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult = {};\n",
    "listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(list_datatype_withoutsignal)), dtype=\"int\");\n",
    "listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(testset)), dtype=\"int\");\n",
    "listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(list_datatype_withoutsignal)))\n",
    "listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), 2))\n",
    "cnt = 0\n",
    "\n",
    "listResult[\"FPR\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)),1))\n",
    "\n",
    "\n",
    "for ic_withoutsignal[0], ic_withoutsignal[1] in itertools.product(np.arange(Ncut), np.arange(Ncut)):\n",
    "\n",
    "    \n",
    "    # if cnt < 86:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in sequence:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_filtered = dataset_wsl_fft_collected\n",
    "    \n",
    "    cutID = \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version\n",
    "        \n",
    "#     for iPrev in range(3):\n",
    "#         previousStep = ind2datatype[iPrev];\n",
    "#         modelPrev = models[previousStep]; # previous step AE\n",
    "        \n",
    "#         # train the WSC according to previous AE's cut\n",
    "        \n",
    "#         dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "#         dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset_wsl_filtered))[1].detach().numpy();\n",
    "#         dataset1 = dataset_wsl_filtered[np.var(dataset_wsl_filtered-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         model = trainWSC(dataset0, dataset1, cutID)\n",
    "#         models[previousStep+\"_WSC\"] = model;\n",
    "        \n",
    "#         # filter the data according to previous WSC\n",
    "#         for j in range(iPrev, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "#             data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "        \n",
    "# #         # filter the data\n",
    "# #         for j in range(iPrev+1, 4):\n",
    "# #             dt = ind2datatype[j];\n",
    "# #             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "# #             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "#         # train the current step AE\n",
    "#         currentStep = ind2datatype[iPrev+1];\n",
    "#         model = trainAE(data_filtered[currentStep], cutID);\n",
    "#         models[currentStep] = model;\n",
    "        \n",
    "#     torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version+\".json\")\n",
    "#     print(models.keys())\n",
    "    \n",
    "    models = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic_withoutsignal[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    # print(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    dcd = {};\n",
    "    err = {};\n",
    "    ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "    for datatype in list_datatype_withoutsignal:\n",
    "        if datatype == 'sg':\n",
    "            dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "            err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "        \n",
    "        # elif datatype == 'noise':\n",
    "        #     dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "        #     err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "            \n",
    "        else:\n",
    "            dcd[datatype] = nn.Sigmoid()(models[datatype + \"_WSC\"](torch.FloatTensor(testset))).detach().numpy().reshape(-1)\n",
    "        \n",
    "    not_select = np.array([True]*len(testset));\n",
    "\n",
    "    # for iStep in range(len(list_datatype_withoutsignal)):\n",
    "    #     datatype = ind2datatype[iStep];\n",
    "    #     if datatype == 'sg':\n",
    "    #         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "    #     else:\n",
    "    #         ind_pass = np.logical_and(not_select, dcd[datatype] <= 0.5);\n",
    "    #     ans[ind_pass] = iStep;\n",
    "    #     not_select[ind_pass] = False;\n",
    "        \n",
    "    # Pass glitch first\n",
    "    \n",
    "    datatype = 'glitch'\n",
    "    ind_pass = np.logical_and(not_select, dcd[datatype] <= 0.5)\n",
    "    ans[ind_pass] = 0;\n",
    "    not_select[ind_pass] = False;\n",
    "    # print(dcd['glitch'])\n",
    "    \n",
    "    # Leftover are noise and signals\n",
    "    \n",
    "    datatype = 'noise'\n",
    "    ind_pass = np.logical_and(not_select, dcd[datatype] <= np.sort(dcd[datatype][-N_wsl['bbh']-N_wsl['sg']:])[int(0.1 * (N_wsl['bbh']+N_wsl['sg']))])\n",
    "    # ind_pass = np.logical_and(not_select, err[datatype] <= np.sort(err[datatype][-N_wsl['bbh']-N_wsl['sg']:])[int(0.1 * (N_wsl['bbh']+N_wsl['sg']))])\n",
    "    noise_number = np.sum(np.logical_and(not_select, correct_ans_withoutsignal == 1))\n",
    "    passed_noise_number = noise_number - np.sum(np.logical_and(ind_pass, correct_ans_withoutsignal == 1))\n",
    "    ans[ind_pass] = 1;\n",
    "    not_select[ind_pass] = False;\n",
    "    \n",
    "    ans[not_select] = -1\n",
    "    \n",
    "    FPR = passed_noise_number / noise_number\n",
    "    \n",
    "    # print(dcd['noise'])\n",
    "    print('For cnt = {}, totally {} noise events passed the glitch WSL, and {} noise events within the threshold for TPR=0.9'.format(cnt, noise_number, passed_noise_number))\n",
    "    listResult['FPR'][cnt] = FPR\n",
    "    listResult['cut'][cnt] = ic_withoutsignal\n",
    "        \n",
    "    # ans[not_select] = -1;\n",
    "\n",
    "    # listResult[\"cut\"][cnt] = ic_withoutsignal;\n",
    "    # listResult[\"ans\"][cnt] = ans;\n",
    "\n",
    "    # acc = np.zeros(len(ind2datatype));\n",
    "\n",
    "    # for i in range(len(ind2datatype)):\n",
    "    #     acc[i] = np.sum(np.logical_and(ans==i, correct_ans_withoutsignal==i))/N_wsl[ind2datatype[i]];\n",
    "        \n",
    "    # listResult[\"accuracy_4\"][cnt] = acc;\n",
    "\n",
    "    # listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(N_wsl[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "    #                                     np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "\n",
    "    cnt += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['FPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the result for 0.05, 0.65, 0.15, 0.15\n",
    "\n",
    "print(listResult['cut'][np.argmin(listResult['FPR'])])\n",
    "print(listResult['FPR'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the result for 0.1, 0.3, 0.3, 0.3\n",
    "\n",
    "print(listResult['cut'][np.argmin(listResult['FPR'])])\n",
    "print(listResult['FPR'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(ans[-6000:] == 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_wsl['noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(ans[1500:-6000] == 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(ans[1500:-6000] == -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(ans[-6000:] == -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(ans[1500:-6000] == -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"04\" + \"_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['noise']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(bkg_filtered))[1].detach().numpy()\n",
    "dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "# foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "plt.axvline(np.sort(np.var(bbh_filtered-dcd_bbh, axis=1))[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['noise_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "# foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "# foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "plt.title(\"trained with noise\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "# plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['FPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['FPR'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['FPR'][np.argwhere(listResult['cut'] == [2,0]).flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['FPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "135 / 22500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = np.array([ [0.01*A, np.sum(np.all(listResult[\"accuracy_4\"] >= 0.01*A, axis=1))/len(listResult[\"accuracy_4\"])] for A in range(101)])\n",
    "plt.plot(foo[:, 0], foo[:, 1])\n",
    "plt.title('4-class accuracy')\n",
    "plt.xlabel('minimal accuracy larger than...')\n",
    "plt.ylabel('percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult[\"accuracy_4\"][np.argwhere(np.all(listResult[\"accuracy_4\"]>0.8, axis=1)).flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = np.array([ [0.01*A, np.sum(np.all(listResult[\"accuracy_2\"] >= 0.01*A, axis=1))/len(listResult[\"accuracy_2\"])] for A in range(101)])\n",
    "plt.plot(foo[:, 0], foo[:, 1])\n",
    "plt.title('2-class accuracy')\n",
    "plt.xlabel('minimal accuracy larger than...')\n",
    "plt.ylabel('percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult[\"total_accuracy\"] = np.sum(listResult[\"ans\"]==correct_ans, axis=1)/len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['total_accuracy'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult[\"accuracy_4\"][np.argmax(listResult[\"accuracy_4\"]>0.8).flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(np.all(np.array(list(itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)))) == np.array([3,1,3]), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result for Chia-Jui noise trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = dataset_wsl_fft_collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = dataset_reserved_test_collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans = np.hstack(([0]*N_wsl['glitch'], [1]*N_wsl['noise'], [2]*N_wsl['bbh'], [3]*N_wsl['sg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans_withoutsignal = np.hstack(([0]*N_wsl['glitch'], [1]*N_wsl['noise'], [-1]*(N_wsl['bbh']+N_wsl['sg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans_withoutsignal = np.hstack(([0]*N_reserved_test['glitch'], [1]*N_reserved_test['noise'], [2]*N_reserved_test['bbh'], [3]*N_reserved_test['sg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans_withoutsignal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic[3] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datatype_withoutsignal = [\"noise\", \"glitch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_withoutsignal = np.array([0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ncut = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult = {};\n",
    "listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(list_datatype_withoutsignal)), dtype=\"int\");\n",
    "listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(testset)), dtype=\"int\");\n",
    "listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(list_datatype_withoutsignal)))\n",
    "listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), 2))\n",
    "cnt = 0\n",
    "\n",
    "listResult[\"FPR\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)),1))\n",
    "\n",
    "\n",
    "for ic_withoutsignal[0], ic_withoutsignal[1] in itertools.product(np.arange(Ncut), np.arange(Ncut)):\n",
    "\n",
    "    \n",
    "    # if cnt < 86:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in sequence:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_filtered = dataset_wsl_fft_collected\n",
    "    \n",
    "    cutID = \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version\n",
    "        \n",
    "#     for iPrev in range(3):\n",
    "#         previousStep = ind2datatype[iPrev];\n",
    "#         modelPrev = models[previousStep]; # previous step AE\n",
    "        \n",
    "#         # train the WSC according to previous AE's cut\n",
    "        \n",
    "#         dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "#         dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset_wsl_filtered))[1].detach().numpy();\n",
    "#         dataset1 = dataset_wsl_filtered[np.var(dataset_wsl_filtered-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         model = trainWSC(dataset0, dataset1, cutID)\n",
    "#         models[previousStep+\"_WSC\"] = model;\n",
    "        \n",
    "#         # filter the data according to previous WSC\n",
    "#         for j in range(iPrev, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "#             data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "        \n",
    "# #         # filter the data\n",
    "# #         for j in range(iPrev+1, 4):\n",
    "# #             dt = ind2datatype[j];\n",
    "# #             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "# #             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "#         # train the current step AE\n",
    "#         currentStep = ind2datatype[iPrev+1];\n",
    "#         model = trainAE(data_filtered[currentStep], cutID);\n",
    "#         models[currentStep] = model;\n",
    "        \n",
    "#     torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version+\".json\")\n",
    "#     print(models.keys())\n",
    "    models = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic_withoutsignal[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    # print(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    dcd = {};\n",
    "    err = {};\n",
    "    ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "    for datatype in list_datatype_withoutsignal:\n",
    "        if datatype == 'sg':\n",
    "            dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "            err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "        \n",
    "        # elif datatype == 'noise':\n",
    "        #     dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "        #     err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "            \n",
    "        else:\n",
    "            dcd[datatype] = nn.Sigmoid()(models[datatype + \"_WSC\"](torch.FloatTensor(testset))).detach().numpy().reshape(-1)\n",
    "        \n",
    "    not_select = np.array([True]*len(testset));\n",
    "\n",
    "    # for iStep in range(len(list_datatype_withoutsignal)):\n",
    "    #     datatype = ind2datatype[iStep];\n",
    "    #     if datatype == 'sg':\n",
    "    #         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "    #     else:\n",
    "    #         ind_pass = np.logical_and(not_select, dcd[datatype] <= 0.5);\n",
    "    #     ans[ind_pass] = iStep;\n",
    "    #     not_select[ind_pass] = False;\n",
    "        \n",
    "    # Pass glitch first\n",
    "    \n",
    "    datatype = 'glitch'\n",
    "    ind_pass = np.logical_and(not_select, dcd[datatype] <= 0.5)\n",
    "    ans[ind_pass] = 0;\n",
    "    not_select[ind_pass] = False;\n",
    "    # print(dcd['glitch'])\n",
    "    \n",
    "    # Leftover are noise and signals\n",
    "    \n",
    "    datatype = 'noise'\n",
    "    ind_pass = np.logical_and(not_select, dcd[datatype] <= np.sort(dcd[datatype][-N_wsl['bbh']-N_wsl['sg']:])[int(0.1 * (N_wsl['bbh']+N_wsl['sg']))])\n",
    "    # ind_pass = np.logical_and(not_select, err[datatype] <= np.sort(err[datatype][-N_wsl['bbh']-N_wsl['sg']:])[int(0.1 * (N_wsl['bbh']+N_wsl['sg']))])\n",
    "    noise_number = np.sum(np.logical_and(not_select, correct_ans_withoutsignal == 1))\n",
    "    passed_noise_number = noise_number - np.sum(np.logical_and(ind_pass, correct_ans_withoutsignal == 1))\n",
    "    ans[ind_pass] = 1;\n",
    "    not_select[ind_pass] = False;\n",
    "    \n",
    "    ans[not_select] = -1\n",
    "    \n",
    "    FPR = passed_noise_number / noise_number\n",
    "    \n",
    "    # print(dcd['noise'])\n",
    "    print('For cnt = {}, totally {} noise events passed the glitch WSL, and {} noise events within the threshold for TPR=0.9'.format(cnt, noise_number, passed_noise_number))\n",
    "    listResult['FPR'][cnt] = FPR\n",
    "    listResult['cut'][cnt] = ic_withoutsignal\n",
    "        \n",
    "    # ans[not_select] = -1;\n",
    "\n",
    "    # listResult[\"cut\"][cnt] = ic_withoutsignal;\n",
    "    # listResult[\"ans\"][cnt] = ans;\n",
    "\n",
    "    # acc = np.zeros(len(ind2datatype));\n",
    "\n",
    "    # for i in range(len(ind2datatype)):\n",
    "    #     acc[i] = np.sum(np.logical_and(ans==i, correct_ans_withoutsignal==i))/N_wsl[ind2datatype[i]];\n",
    "        \n",
    "    # listResult[\"accuracy_4\"][cnt] = acc;\n",
    "\n",
    "    # listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(N_wsl[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "    #                                     np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "\n",
    "    cnt += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult = {};\n",
    "listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(list_datatype_withoutsignal)), dtype=\"int\");\n",
    "listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(testset)), dtype=\"int\");\n",
    "listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(list_datatype_withoutsignal)))\n",
    "listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), 2))\n",
    "cnt = 0\n",
    "\n",
    "listResult[\"FPR\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)),1))\n",
    "\n",
    "\n",
    "for ic_withoutsignal[0], ic_withoutsignal[1] in itertools.product(np.arange(Ncut), np.arange(Ncut)):\n",
    "\n",
    "    \n",
    "    # if cnt < 86:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in sequence:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_filtered = dataset_wsl_fft_collected\n",
    "    \n",
    "    cutID = \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version\n",
    "        \n",
    "#     for iPrev in range(3):\n",
    "#         previousStep = ind2datatype[iPrev];\n",
    "#         modelPrev = models[previousStep]; # previous step AE\n",
    "        \n",
    "#         # train the WSC according to previous AE's cut\n",
    "        \n",
    "#         dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "#         dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset_wsl_filtered))[1].detach().numpy();\n",
    "#         dataset1 = dataset_wsl_filtered[np.var(dataset_wsl_filtered-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         model = trainWSC(dataset0, dataset1, cutID)\n",
    "#         models[previousStep+\"_WSC\"] = model;\n",
    "        \n",
    "#         # filter the data according to previous WSC\n",
    "#         for j in range(iPrev, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "#             data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "        \n",
    "# #         # filter the data\n",
    "# #         for j in range(iPrev+1, 4):\n",
    "# #             dt = ind2datatype[j];\n",
    "# #             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "# #             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "#         # train the current step AE\n",
    "#         currentStep = ind2datatype[iPrev+1];\n",
    "#         model = trainAE(data_filtered[currentStep], cutID);\n",
    "#         models[currentStep] = model;\n",
    "        \n",
    "#     torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version+\".json\")\n",
    "#     print(models.keys())\n",
    "    models = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic_withoutsignal[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    # print(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    dcd = {};\n",
    "    err = {};\n",
    "    ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "    for datatype in list_datatype_withoutsignal:\n",
    "        if datatype == 'sg':\n",
    "            dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "            err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "        \n",
    "        # elif datatype == 'noise':\n",
    "        #     dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "        #     err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "            \n",
    "        else:\n",
    "            dcd[datatype] = nn.Sigmoid()(models[datatype + \"_WSC\"](torch.FloatTensor(testset))).detach().numpy().reshape(-1)\n",
    "        \n",
    "    not_select = np.array([True]*len(testset));\n",
    "\n",
    "    # for iStep in range(len(list_datatype_withoutsignal)):\n",
    "    #     datatype = ind2datatype[iStep];\n",
    "    #     if datatype == 'sg':\n",
    "    #         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "    #     else:\n",
    "    #         ind_pass = np.logical_and(not_select, dcd[datatype] <= 0.5);\n",
    "    #     ans[ind_pass] = iStep;\n",
    "    #     not_select[ind_pass] = False;\n",
    "        \n",
    "    # Pass glitch first\n",
    "    \n",
    "    datatype = 'glitch'\n",
    "    ind_pass = np.logical_and(not_select, dcd[datatype] <= 0.5)\n",
    "    ans[ind_pass] = 0;\n",
    "    not_select[ind_pass] = False;\n",
    "    # print(dcd['glitch'])\n",
    "    \n",
    "    # Leftover are noise and signals\n",
    "    \n",
    "    datatype = 'noise'\n",
    "    ind_pass = np.logical_and(not_select, dcd[datatype] <= np.sort(dcd[datatype][-N_wsl['bbh']-N_wsl['sg']:])[int(0.1 * (N_wsl['bbh']+N_wsl['sg']))])\n",
    "    # ind_pass = np.logical_and(not_select, err[datatype] <= np.sort(err[datatype][-N_wsl['bbh']-N_wsl['sg']:])[int(0.1 * (N_wsl['bbh']+N_wsl['sg']))])\n",
    "    noise_number = np.sum(np.logical_and(not_select, correct_ans_withoutsignal == 1))\n",
    "    passed_noise_number = noise_number - np.sum(np.logical_and(ind_pass, correct_ans_withoutsignal == 1))\n",
    "    ans[ind_pass] = 1;\n",
    "    not_select[ind_pass] = False;\n",
    "    \n",
    "    ans[not_select] = -1\n",
    "    \n",
    "    FPR = passed_noise_number / noise_number\n",
    "    \n",
    "    # print(dcd['noise'])\n",
    "    print('For cnt = {}, totally {} noise events passed the glitch WSL, and {} noise events within the threshold for TPR=0.9'.format(cnt, noise_number, passed_noise_number))\n",
    "    listResult['FPR'][cnt] = FPR\n",
    "    listResult['cut'][cnt] = ic_withoutsignal\n",
    "        \n",
    "    # ans[not_select] = -1;\n",
    "\n",
    "    # listResult[\"cut\"][cnt] = ic_withoutsignal;\n",
    "    # listResult[\"ans\"][cnt] = ans;\n",
    "\n",
    "    # acc = np.zeros(len(ind2datatype));\n",
    "\n",
    "    # for i in range(len(ind2datatype)):\n",
    "    #     acc[i] = np.sum(np.logical_and(ans==i, correct_ans_withoutsignal==i))/N_wsl[ind2datatype[i]];\n",
    "        \n",
    "    # listResult[\"accuracy_4\"][cnt] = acc;\n",
    "\n",
    "    # listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(N_wsl[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "    #                                     np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "\n",
    "    cnt += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['FPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['FPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['FPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(listResult['cut'][np.argmin(listResult['FPR'])])\n",
    "print(listResult['FPR'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the result for 0.05, 0.65, 0.15, 0.15\n",
    "\n",
    "print(listResult['cut'][np.argmin(listResult['FPR'])])\n",
    "print(listResult['FPR'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the result for 0.1, 0.3, 0.3, 0.3\n",
    "\n",
    "print(listResult['cut'][np.argmin(listResult['FPR'])])\n",
    "print(listResult['FPR'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(ans[-6000:] == 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_wsl['noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(ans[1500:-6000] == 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(ans[1500:-6000] == -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(ans[-6000:] == -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(ans[1500:-6000] == -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"00\" + \"_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"00\" + \"_v6.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['glitch']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(bkg_filtered))[1].detach().numpy()\n",
    "dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with glitch\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "plt.axvline(0.0024, color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['glitch']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(bkg_filtered))[1].detach().numpy()\n",
    "dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with glitch\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "plt.axvline(np.sort(np.var(bbh_filtered-dcd_bbh, axis=1))[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['glitch']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(bkg_filtered))[1].detach().numpy()\n",
    "dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "# plt.axvline(np.sort(np.var(bbh_filtered-dcd_bbh, axis=1))[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GWAK data + Chia-Jui model\n",
    "\n",
    "model = models['noise']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(bkg_filtered))[1].detach().numpy()\n",
    "dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "# foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "# plt.axvline(np.sort(np.var(bbh_filtered-dcd_bbh, axis=1))[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "# plt.axvline(0.0018, color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia-Jui data + GWAK model\n",
    "\n",
    "model = models['glitch']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(bkg_filtered))[1].detach().numpy()\n",
    "dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with glitch\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "plt.axvline(np.sort(np.var(bbh_filtered-dcd_bbh, axis=1))[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia-Jui data + GWAK model\n",
    "\n",
    "model = models['noise']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(bkg_filtered))[1].detach().numpy()\n",
    "dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "plt.axvline(np.sort(np.var(bbh_filtered-dcd_bbh, axis=1))[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['glitch']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(bkg_filtered))[1].detach().numpy()\n",
    "dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with glitch\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "# plt.axvline(np.sort(np.var(bbh_filtered-dcd_bbh, axis=1))[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['glitch']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(bkg_filtered))[1].detach().numpy()\n",
    "dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with glitch\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "# plt.axvline(np.sort(np.var(bbh_filtered-dcd_bbh, axis=1))[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.axvline(0.0024, color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['glitch']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(bkg_filtered))[1].detach().numpy()\n",
    "dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with glitch\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "plt.axvline(np.sort(np.var(bbh_filtered-dcd_bbh, axis=1))[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['noise']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(bkg_filtered))[1].detach().numpy()\n",
    "dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "# foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "# plt.axvline(np.sort(np.var(bbh_filtered-dcd_bbh, axis=1))[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.axvline(0.0018, color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['noise']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(bkg_filtered))[1].detach().numpy()\n",
    "dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "# foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "plt.axvline(np.sort(np.var(bbh_filtered-dcd_bbh, axis=1))[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GWAK data + Chia-Jui model\n",
    "\n",
    "model = models['glitch_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = nn.Sigmoid()(model(torch.FloatTensor(glitch_fft))).detach().numpy()\n",
    "\n",
    "# foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "# foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with glitch\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "# plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.axvline(0.5, color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "# plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GWAK data + Chia-Jui model\n",
    "\n",
    "model = models['noise_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = nn.Sigmoid()(model(torch.FloatTensor(glitch_fft))).detach().numpy()\n",
    "\n",
    "# foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "# foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "# foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with glitch\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "# plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia-Jui data + GWAK model\n",
    "\n",
    "model = models['glitch_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = nn.Sigmoid()(model(torch.FloatTensor(glitch_fft))).detach().numpy()\n",
    "\n",
    "# foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "# foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with glitch\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "# plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia-Jui data + GWAK model\n",
    "\n",
    "model = models['glitch_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = nn.Sigmoid()(model(torch.FloatTensor(glitch_fft))).detach().numpy()\n",
    "\n",
    "# foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "# foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "# plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia-Jui data + GWAK model\n",
    "\n",
    "model = models['noise_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = nn.Sigmoid()(model(torch.FloatTensor(glitch_fft))).detach().numpy()\n",
    "\n",
    "# foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "# foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "# plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia-Jui data + Chia-Jui model\n",
    "\n",
    "model = models['glitch_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = nn.Sigmoid()(model(torch.FloatTensor(glitch_fft))).detach().numpy()\n",
    "\n",
    "# foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "# foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "# plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.axvline(0.5, color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "# plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia-Jui data + Chia-Jui model\n",
    "\n",
    "model = models['glitch_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = nn.Sigmoid()(model(torch.FloatTensor(glitch_fft))).detach().numpy()\n",
    "\n",
    "# foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "# foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "# plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['noise_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "# foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "# foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "plt.title(\"trained with noise\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "# plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['FPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['FPR'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['FPR'][np.argwhere(listResult['cut'] == [2,0]).flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['FPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "135 / 22500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = np.array([ [0.01*A, np.sum(np.all(listResult[\"accuracy_4\"] >= 0.01*A, axis=1))/len(listResult[\"accuracy_4\"])] for A in range(101)])\n",
    "plt.plot(foo[:, 0], foo[:, 1])\n",
    "plt.title('4-class accuracy')\n",
    "plt.xlabel('minimal accuracy larger than...')\n",
    "plt.ylabel('percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult[\"accuracy_4\"][np.argwhere(np.all(listResult[\"accuracy_4\"]>0.8, axis=1)).flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = np.array([ [0.01*A, np.sum(np.all(listResult[\"accuracy_2\"] >= 0.01*A, axis=1))/len(listResult[\"accuracy_2\"])] for A in range(101)])\n",
    "plt.plot(foo[:, 0], foo[:, 1])\n",
    "plt.title('2-class accuracy')\n",
    "plt.xlabel('minimal accuracy larger than...')\n",
    "plt.ylabel('percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult[\"total_accuracy\"] = np.sum(listResult[\"ans\"]==correct_ans, axis=1)/len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['total_accuracy'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult[\"accuracy_4\"][np.argmax(listResult[\"accuracy_4\"]>0.8).flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(np.all(np.array(list(itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)))) == np.array([3,1,3]), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's think about using 2 det for the glitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {};\n",
    "models[\"glitch_L\"] = torch.load('../Model_cached/1_det_L_glitch_trained_with_permutation_1.pt')\n",
    "models[\"glitch_H\"] = torch.load('../Model_cached/1_det_H_glitch_trained_with_permutation_1.pt')\n",
    "models[\"glitch_L\"].cpu().eval()\n",
    "models[\"glitch_H\"].cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0;\n",
    "\n",
    "ic = np.zeros(5, dtype=\"int\")\n",
    "\n",
    "# loop for only the cut in glitch, noise and bbh as it's not really meaningful to set cut in sg w/o new signals\n",
    "ic[2] = Ncut-1;\n",
    "ic[3] = Ncut-1;\n",
    "\n",
    "# listResult = {};\n",
    "# listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)), dtype=\"int\");\n",
    "# listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype)-1), len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype)-1), 2))\n",
    "\n",
    "for ic[0], ic[1] in itertools.product(np.arange(Ncut), np.arange(Ncut)):\n",
    "# for ic[0], ic[1], ic[2], ic[3] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "    cnt += 1;\n",
    "    \n",
    "    # if cnt < 11:\n",
    "    #     continue\n",
    "    # elif cnt > 85:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in sequence:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_filtered = dataset_wsl_fft_collected\n",
    "    \n",
    "    cutID = \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version\n",
    "    \n",
    "    iPrev = 0\n",
    "    \n",
    "    previousStep = ind2datatype[iPrev];\n",
    "    modelPrev_L = models['glitch_L']; # previous step AE\n",
    "    modelPrev_H = models['glitch_H']; # previous step AE\n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    dcd_L = modelPrev_L(torch.FloatTensor(dataset_wsl_filtered[:,:101]))[1].detach().numpy();\n",
    "    dcd_H = modelPrev_H(torch.FloatTensor(dataset_wsl_filtered[:,101:]))[1].detach().numpy();\n",
    "    dataset1 = dataset_wsl_filtered[np.logical_and(np.var(dataset_wsl_filtered[:,:101]-dcd_L, axis=1) >= cutList['glitch_L'][ic[0]],\n",
    "                                                   np.var(dataset_wsl_filtered[:,101:]-dcd_H, axis=1) >= cutList['glitch_H'][ic[0]])]\n",
    "    \n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    models[previousStep+\"_WSC\"] = model;\n",
    "    \n",
    "    # filter the data according to previous WSC\n",
    "    for j in range(iPrev, 4):\n",
    "        dt = ind2datatype[j];\n",
    "        dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "        data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "    \n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "    \n",
    "    # train the current step AE\n",
    "    currentStep = ind2datatype[iPrev+1];\n",
    "    model = trainAE(data_filtered[currentStep], cutID, version, currentStep);\n",
    "    models[currentStep] = model;   \n",
    "       \n",
    "     \n",
    "    for iPrev in range(1,2):\n",
    "        previousStep = ind2datatype[iPrev];\n",
    "        modelPrev = models[previousStep]; # previous step AE\n",
    "        \n",
    "        # train the WSC according to previous AE's cut\n",
    "        \n",
    "        dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "        \n",
    "        dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "        dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "        dcd = modelPrev(torch.FloatTensor(dataset_wsl_filtered))[1].detach().numpy();\n",
    "        dataset1 = dataset_wsl_filtered[np.var(dataset_wsl_filtered-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "        model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "        models[previousStep+\"_WSC\"] = model;\n",
    "        \n",
    "        # filter the data according to previous WSC\n",
    "        for j in range(iPrev, 4):\n",
    "            dt = ind2datatype[j];\n",
    "            dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "            data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "        \n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "        # train the current step AE\n",
    "        currentStep = ind2datatype[iPrev+1];\n",
    "        model = trainAE(data_filtered[currentStep], cutID, version, currentStep);\n",
    "        models[currentStep] = model;\n",
    "        \n",
    "    torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    print(models.keys())\n",
    "    \n",
    "#     dcd = {};\n",
    "#     err = {};\n",
    "#     ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "#     for datatype in list_datatype:\n",
    "#         dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "#         err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "        \n",
    "#     not_select = np.array([True]*len(testset));\n",
    "\n",
    "#     for iStep in range(len(list_datatype)):\n",
    "#         datatype = ind2datatype[iStep];\n",
    "#         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "#         ans[ind_pass] = iStep;\n",
    "#         not_select[ind_pass] = False;\n",
    "        \n",
    "#     ans[not_select] = -1;\n",
    "    \n",
    "#     listResult[\"cut\"][cnt] = ic;\n",
    "#     listResult[\"ans\"][cnt] = ans;\n",
    "    \n",
    "#     acc = np.zeros(len(ind2datatype));\n",
    "    \n",
    "#     for i in range(len(ind2datatype)):\n",
    "#         acc[i] = np.sum(np.logical_and(ans==i, correct_ans==i))/Nsample[ind2datatype[i]];\n",
    "        \n",
    "#     listResult[\"accuracy_4\"][cnt] = acc;\n",
    "    \n",
    "#     listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(Nsample[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "#                                      np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(Nsample[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "    # cnt += 1\n",
    "    print(cnt)\n",
    "    print(time.time() - t0)\n",
    "    \n",
    "# listResult[\"total_accuracy\"] = np.sum(listResult[\"ans\"]==correct_ans, axis=1)/len(testset);\n",
    "# torch.save(listResult, \"../data/SequentialTraining/training_performance_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0;\n",
    "\n",
    "ic = np.zeros(5, dtype=\"int\")\n",
    "\n",
    "# loop for only the cut in glitch, noise and bbh as it's not really meaningful to set cut in sg w/o new signals\n",
    "ic[3] = Ncut-1;\n",
    "ic[4] = Ncut-1;\n",
    "\n",
    "# listResult = {};\n",
    "# listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)), dtype=\"int\");\n",
    "# listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype)-1), len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype)-1), 2))\n",
    "\n",
    "for ic[0], ic[1], ic[2] in itertools.product(np.arange(Ncut), np.arange(Ncut),  np.arange(Ncut)):\n",
    "# for ic[0], ic[1], ic[2], ic[3] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "    cnt += 1;\n",
    "    \n",
    "    # if cnt < 11:\n",
    "    #     continue\n",
    "    # elif cnt > 85:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in sequence:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_filtered = dataset_wsl_fft_collected\n",
    "    \n",
    "    cutID = \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version\n",
    "    \n",
    "    iPrev = 0\n",
    "    \n",
    "    previousStep = ind2datatype[iPrev];\n",
    "    modelPrev_L = models['glitch_L']; # previous step AE\n",
    "    modelPrev_H = models['glitch_H']; # previous step AE\n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    dcd_L = modelPrev_L(torch.FloatTensor(dataset_wsl_filtered[:,:101]))[1].detach().numpy();\n",
    "    dcd_H = modelPrev_H(torch.FloatTensor(dataset_wsl_filtered[:,101:]))[1].detach().numpy();\n",
    "    dataset1 = dataset_wsl_filtered[np.logical_and(np.var(dataset_wsl_filtered[:,:101]-dcd_L, axis=1) >= cutList['glitch_L'][ic[0]],\n",
    "                                                   np.var(dataset_wsl_filtered[:,101:]-dcd_H, axis=1) >= cutList['glitch_H'][ic[1]])]\n",
    "    \n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    models[previousStep+\"_WSC\"] = model;\n",
    "    \n",
    "    # filter the data according to previous WSC\n",
    "    for j in range(iPrev, 4):\n",
    "        dt = ind2datatype[j];\n",
    "        dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "        data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "    \n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "    \n",
    "    # train the current step AE\n",
    "    currentStep = ind2datatype[iPrev+1];\n",
    "    model = trainAE(data_filtered[currentStep], cutID, version, currentStep);\n",
    "    models[currentStep] = model;   \n",
    "       \n",
    "     \n",
    "    for iPrev in range(1,2):\n",
    "        previousStep = ind2datatype[iPrev];\n",
    "        modelPrev = models[previousStep]; # previous step AE\n",
    "        \n",
    "        # train the WSC according to previous AE's cut\n",
    "        \n",
    "        dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "        \n",
    "        dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "        dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[iPrev+1]]]\n",
    "        \n",
    "        dcd = modelPrev(torch.FloatTensor(dataset_wsl_filtered))[1].detach().numpy();\n",
    "        dataset1 = dataset_wsl_filtered[np.var(dataset_wsl_filtered-dcd, axis=1) >= cutList[previousStep][ic[iPrev+1]]]\n",
    "        \n",
    "        model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "        models[previousStep+\"_WSC\"] = model;\n",
    "        \n",
    "        # filter the data according to previous WSC\n",
    "        for j in range(iPrev, 4):\n",
    "            dt = ind2datatype[j];\n",
    "            dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "            data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "        \n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "        # train the current step AE\n",
    "        currentStep = ind2datatype[iPrev+1];\n",
    "        model = trainAE(data_filtered[currentStep], cutID, version, currentStep);\n",
    "        models[currentStep] = model;\n",
    "        \n",
    "    torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version+\".json\")\n",
    "    print(models.keys())\n",
    "    \n",
    "#     dcd = {};\n",
    "#     err = {};\n",
    "#     ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "#     for datatype in list_datatype:\n",
    "#         dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "#         err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "        \n",
    "#     not_select = np.array([True]*len(testset));\n",
    "\n",
    "#     for iStep in range(len(list_datatype)):\n",
    "#         datatype = ind2datatype[iStep];\n",
    "#         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "#         ans[ind_pass] = iStep;\n",
    "#         not_select[ind_pass] = False;\n",
    "        \n",
    "#     ans[not_select] = -1;\n",
    "    \n",
    "#     listResult[\"cut\"][cnt] = ic;\n",
    "#     listResult[\"ans\"][cnt] = ans;\n",
    "    \n",
    "#     acc = np.zeros(len(ind2datatype));\n",
    "    \n",
    "#     for i in range(len(ind2datatype)):\n",
    "#         acc[i] = np.sum(np.logical_and(ans==i, correct_ans==i))/Nsample[ind2datatype[i]];\n",
    "        \n",
    "#     listResult[\"accuracy_4\"][cnt] = acc;\n",
    "    \n",
    "#     listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(Nsample[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "#                                      np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(Nsample[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "    # cnt += 1\n",
    "    print(cnt)\n",
    "    print(time.time() - t0)\n",
    "    \n",
    "# listResult[\"total_accuracy\"] = np.sum(listResult[\"ans\"]==correct_ans, axis=1)/len(testset);\n",
    "# torch.save(listResult, \"../data/SequentialTraining/training_performance_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ae_fft['glitch'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ae_fft['noise'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two detector for glitch, but WSL for each detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The WSL model is already pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSClassifier_Onedetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WSClassifier_Onedetector, self).__init__()\n",
    "        self.fc1 = nn.Linear(101, 32)  # 第一层全连接层，输入维度为4，输出维度为64\n",
    "        self.norm1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()  # 激活函数\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        # self.norm2 = nn.BatchNorm1d(8)\n",
    "        # self.fc4 = nn.Linear(8, 1)  # 第三层全连接层，输入维度为32，输出维度为类别数目\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        # nn.init.kaiming_normal_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(self.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "        # x = self.relu(x)\n",
    "#         x = self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ae_fft['bbh'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft['bbh'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ncut = 5;\n",
    "cutList = {};\n",
    "\n",
    "max_glitch_L = 0.0024;\n",
    "min_glitch_L = 0.0008;\n",
    "cutList[\"glitch_L\"] = np.linspace(min_glitch_L, max_glitch_L, Ncut);\n",
    "\n",
    "max_glitch_H = 0.0024;\n",
    "min_glitch_H = 0.0004;\n",
    "cutList[\"glitch_H\"] = np.linspace(min_glitch_H, max_glitch_H, Ncut);\n",
    "\n",
    "max_bkg = 0.0008;\n",
    "min_bkg = 0.0018;\n",
    "cutList[\"noise\"] = np.linspace(min_bkg, max_bkg, Ncut);\n",
    "\n",
    "# max_bbh = 0.0014;\n",
    "# min_bbh = 0.0024;\n",
    "# cutList[\"bbh\"] = np.linspace(min_bbh, max_bbh, Ncut);\n",
    "\n",
    "# max_sg = 0.0008;\n",
    "# min_sg = 0.0032;\n",
    "# cutList[\"sg\"] = np.linspace(min_sg, max_sg, Ncut);\n",
    "\n",
    "# torch.save(cutList, \"../Data_cached/SequentialTraining/WSL/cut_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0;\n",
    "\n",
    "ic = np.zeros(5, dtype=\"int\")\n",
    "\n",
    "# loop for only the cut in glitch, noise and bbh as it's not really meaningful to set cut in sg w/o new signals\n",
    "ic[3] = Ncut-1;\n",
    "ic[4] = Ncut-1;\n",
    "\n",
    "# listResult = {};\n",
    "# listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)), dtype=\"int\");\n",
    "# listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype)-1), len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype)-1), 2))\n",
    "\n",
    "for ic[0], ic[1], ic[2] in itertools.product(np.arange(Ncut), np.arange(Ncut),  np.arange(Ncut)):\n",
    "# for ic[0], ic[1], ic[2], ic[3] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "    cnt += 1;\n",
    "    \n",
    "    if cnt < 102:\n",
    "        continue\n",
    "    # elif cnt > 85:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in sequence:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_filtered = dataset_wsl_fft_collected\n",
    "    \n",
    "    cutID = \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version\n",
    "    \n",
    "    iPrev = 0\n",
    "    \n",
    "    previousStep = ind2datatype[iPrev];\n",
    "    # modelPrev_L = models['glitch_L']; # previous step AE\n",
    "    # modelPrev_H = models['glitch_H']; # previous step AE\n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    # dcd_L = modelPrev_L(torch.FloatTensor(dataset_wsl_filtered[:,:101]))[1].detach().numpy();\n",
    "    # dcd_H = modelPrev_H(torch.FloatTensor(dataset_wsl_filtered[:,101:]))[1].detach().numpy();\n",
    "    # dataset1 = dataset_wsl_filtered[np.logical_and(np.var(dataset_wsl_filtered[:,:101]-dcd_L, axis=1) >= cutList['glitch_L'][ic[0]],\n",
    "    #                                                np.var(dataset_wsl_filtered[:,101:]-dcd_H, axis=1) >= cutList['glitch_H'][ic[1]])]\n",
    "    \n",
    "    # model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    # models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "    model_L = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_L']\n",
    "    model_H = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_H']\n",
    "\n",
    "    models[previousStep+\"_WSC_L\"] = model_L\n",
    "    models[previousStep+\"_WSC_H\"] = model_H\n",
    "\n",
    "    \n",
    "    # filter the data according to previous WSC\n",
    "    for j in range(iPrev, 4):\n",
    "        dt = ind2datatype[j];\n",
    "        dcd_L = nn.Sigmoid()(model_L(torch.FloatTensor(data_filtered[dt][:,:101]))).detach().numpy().flatten();\n",
    "        dcd_H = nn.Sigmoid()(model_H(torch.FloatTensor(data_filtered[dt][:,101:]))).detach().numpy().flatten();\n",
    "        data_filtered[dt] = data_filtered[dt][np.logical_and(dcd_L > 0.5, dcd_H > 0.5)]\n",
    "    \n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "    \n",
    "    # train the current step AE\n",
    "    currentStep = ind2datatype[iPrev+1];\n",
    "    model = trainAE(data_filtered[currentStep], cutID, version, currentStep);\n",
    "    models[currentStep] = model;   \n",
    "       \n",
    "     \n",
    "    for iPrev in range(1,2):\n",
    "        previousStep = ind2datatype[iPrev];\n",
    "        modelPrev = models[previousStep]; # previous step AE\n",
    "        \n",
    "        # train the WSC according to previous AE's cut\n",
    "        \n",
    "        dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "        \n",
    "        dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "        dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[iPrev+1]]]\n",
    "        \n",
    "        dcd = modelPrev(torch.FloatTensor(dataset_wsl_filtered))[1].detach().numpy();\n",
    "        dataset1 = dataset_wsl_filtered[np.var(dataset_wsl_filtered-dcd, axis=1) >= cutList[previousStep][ic[iPrev+1]]]\n",
    "        \n",
    "        model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "        models[previousStep+\"_WSC\"] = model;\n",
    "        \n",
    "        # filter the data according to previous WSC\n",
    "        for j in range(iPrev, 4):\n",
    "            dt = ind2datatype[j];\n",
    "            dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "            data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "        \n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "        # train the current step AE\n",
    "        currentStep = ind2datatype[iPrev+1];\n",
    "        model = trainAE(data_filtered[currentStep], cutID, version, currentStep);\n",
    "        models[currentStep] = model;\n",
    "        \n",
    "    torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version+\".json\")\n",
    "    print(models.keys())\n",
    "    \n",
    "#     dcd = {};\n",
    "#     err = {};\n",
    "#     ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "#     for datatype in list_datatype:\n",
    "#         dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "#         err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "        \n",
    "#     not_select = np.array([True]*len(testset));\n",
    "\n",
    "#     for iStep in range(len(list_datatype)):\n",
    "#         datatype = ind2datatype[iStep];\n",
    "#         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "#         ans[ind_pass] = iStep;\n",
    "#         not_select[ind_pass] = False;\n",
    "        \n",
    "#     ans[not_select] = -1;\n",
    "    \n",
    "#     listResult[\"cut\"][cnt] = ic;\n",
    "#     listResult[\"ans\"][cnt] = ans;\n",
    "    \n",
    "#     acc = np.zeros(len(ind2datatype));\n",
    "    \n",
    "#     for i in range(len(ind2datatype)):\n",
    "#         acc[i] = np.sum(np.logical_and(ans==i, correct_ans==i))/Nsample[ind2datatype[i]];\n",
    "        \n",
    "#     listResult[\"accuracy_4\"][cnt] = acc;\n",
    "    \n",
    "#     listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(Nsample[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "#                                      np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(Nsample[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "    # cnt += 1\n",
    "    print(cnt)\n",
    "    print(time.time() - t0)\n",
    "    \n",
    "# listResult[\"total_accuracy\"] = np.sum(listResult[\"ans\"]==correct_ans, axis=1)/len(testset);\n",
    "# torch.save(listResult, \"../data/SequentialTraining/training_performance_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datatype_withoutsignal = [\"noise\", \"glitch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = [\"glitch\", \"noise\", \"bbh\", \"sg\"];\n",
    "dataset_wsl_fft_collected = np.empty((0, dataset_wsl_fft[\"glitch\"].shape[1]))\n",
    "for dt in sequence:\n",
    "    dataset_wsl_fft_collected = np.vstack((dataset_wsl_fft_collected, dataset_wsl_fft[dt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = dataset_wsl_fft_collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_withoutsignal = np.zeros(3, dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datatype_withoutsignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans_withoutsignal = np.hstack(([0]*N_wsl['glitch'], [1]*N_wsl['noise'], [-1]*(N_wsl['bbh']+N_wsl['sg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans_withoutsignal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult = {};\n",
    "listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)+1), len(list_datatype_withoutsignal)+1), dtype=\"int\");\n",
    "listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)+1), len(testset)), dtype=\"int\");\n",
    "listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)+1), len(list_datatype_withoutsignal)))\n",
    "listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)+1), 2))\n",
    "cnt = 0\n",
    "\n",
    "listResult[\"FPR\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)+1),1))\n",
    "\n",
    "\n",
    "for ic_withoutsignal[0], ic_withoutsignal[1], ic_withoutsignal[2] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "\n",
    "    \n",
    "    # if cnt < 86:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in sequence:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_filtered = dataset_wsl_fft_collected\n",
    "        \n",
    "#     for iPrev in range(3):\n",
    "#         previousStep = ind2datatype[iPrev];\n",
    "#         modelPrev = models[previousStep]; # previous step AE\n",
    "        \n",
    "#         # train the WSC according to previous AE's cut\n",
    "        \n",
    "#         dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "#         dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset_wsl_filtered))[1].detach().numpy();\n",
    "#         dataset1 = dataset_wsl_filtered[np.var(dataset_wsl_filtered-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         model = trainWSC(dataset0, dataset1, cutID)\n",
    "#         models[previousStep+\"_WSC\"] = model;\n",
    "        \n",
    "#         # filter the data according to previous WSC\n",
    "#         for j in range(iPrev, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "#             data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "        \n",
    "# #         # filter the data\n",
    "# #         for j in range(iPrev+1, 4):\n",
    "# #             dt = ind2datatype[j];\n",
    "# #             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "# #             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "#         # train the current step AE\n",
    "#         currentStep = ind2datatype[iPrev+1];\n",
    "#         model = trainAE(data_filtered[currentStep], cutID);\n",
    "#         models[currentStep] = model;\n",
    "        \n",
    "#     torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version+\".json\")\n",
    "#     print(models.keys())\n",
    "    models = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic_withoutsignal[j]) for j in range(3)) + \"_\"+version+\".json\")\n",
    "    # print(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    dcd = {};\n",
    "    err = {};\n",
    "    ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "    dcd['glitch_L'] = nn.Sigmoid()(models['glitch_WSC_L'](torch.FloatTensor(testset[:,:101]))).detach().numpy().reshape(-1)\n",
    "    dcd['glitch_H'] = nn.Sigmoid()(models['glitch_WSC_H'](torch.FloatTensor(testset[:,101:]))).detach().numpy().reshape(-1)\n",
    "    dcd['noise'] = nn.Sigmoid()(models[\"noise_WSC\"](torch.FloatTensor(testset))).detach().numpy().reshape(-1)\n",
    "    \n",
    "    not_select = np.array([True]*len(testset));\n",
    "\n",
    "    # for iStep in range(len(list_datatype_withoutsignal)):\n",
    "    #     datatype = ind2datatype[iStep];\n",
    "    #     if datatype == 'sg':\n",
    "    #         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "    #     else:\n",
    "    #         ind_pass = np.logical_and(not_select, dcd[datatype] <= 0.5);\n",
    "    #     ans[ind_pass] = iStep;\n",
    "    #     not_select[ind_pass] = False;\n",
    "        \n",
    "    # Pass glitch first\n",
    "    \n",
    "    datatype = 'glitch'\n",
    "    ind_pass = np.logical_and(not_select, np.logical_or(dcd['glitch_L'] <= 0.5, dcd['glitch_H'] <= 0.5))\n",
    "    ans[ind_pass] = 0;\n",
    "    not_select[ind_pass] = False;\n",
    "    # print(dcd['glitch'])\n",
    "    \n",
    "    # Leftover are noise and signals\n",
    "    \n",
    "    datatype = 'noise'\n",
    "    ind_pass = np.logical_and(not_select, dcd[datatype] <= np.sort(dcd[datatype][-N_wsl['bbh']-N_wsl['sg']:])[int(0.1 * (N_wsl['bbh']+N_wsl['sg']))])\n",
    "    # ind_pass = np.logical_and(not_select, err[datatype] <= np.sort(err[datatype][-N_wsl['bbh']-N_wsl['sg']:])[int(0.1 * (N_wsl['bbh']+N_wsl['sg']))])\n",
    "    noise_number = np.sum(np.logical_and(not_select, correct_ans_withoutsignal == 1))\n",
    "    passed_noise_number = noise_number - np.sum(np.logical_and(ind_pass, correct_ans_withoutsignal == 1))\n",
    "    ans[ind_pass] = 1;\n",
    "    not_select[ind_pass] = False;\n",
    "    \n",
    "    ans[not_select] = -1\n",
    "    \n",
    "    FPR = passed_noise_number / noise_number\n",
    "    \n",
    "    # print(dcd['noise'])\n",
    "    print('For cnt = {}, totally {} noise events passed the glitch WSL, and {} noise events within the threshold for TPR=0.9'.format(cnt, noise_number, passed_noise_number))\n",
    "    listResult['FPR'][cnt] = FPR\n",
    "    listResult['cut'][cnt] = ic_withoutsignal\n",
    "        \n",
    "    # ans[not_select] = -1;\n",
    "\n",
    "    # listResult[\"cut\"][cnt] = ic_withoutsignal;\n",
    "    # listResult[\"ans\"][cnt] = ans;\n",
    "\n",
    "    # acc = np.zeros(len(ind2datatype));\n",
    "\n",
    "    # for i in range(len(ind2datatype)):\n",
    "    #     acc[i] = np.sum(np.logical_and(ans==i, correct_ans_withoutsignal==i))/N_wsl[ind2datatype[i]];\n",
    "        \n",
    "    # listResult[\"accuracy_4\"][cnt] = acc;\n",
    "\n",
    "    # listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(N_wsl[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "    #                                     np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "\n",
    "    cnt += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['cut'][np.argwhere(listResult['FPR'] < 0.7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(listResult['FPR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['FPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['cut'][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['cut'][:,[0]] == [idx_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_1 = 1\n",
    "idx_2 = 0\n",
    "\n",
    "np.argwhere(np.all(listResult['cut'][:,:2] == np.array([idx_1, idx_2]), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_1 = 1\n",
    "idx_2 = 3\n",
    "\n",
    "np.argwhere(np.all(listResult['cut'][:,:2] == np.array([idx_1, idx_2]), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join([]) + \"_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['noise_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "bbh_filtered = dataset_wsl_fft['bbh']\n",
    "sg_filtered = dataset_wsl_fft['sg']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "\n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "# foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "# foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "plt.title(\"trained with noise\")\n",
    "# plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "plt.legend()\n",
    "# plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including the CNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick check for roburtness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_wsl_total = 30000;\n",
    "N_wsl = {}\n",
    "N_wsl[\"noise\"] = int(0.65*N_wsl_total)\n",
    "N_wsl[\"bbh\"] = int(0.1*N_wsl_total)\n",
    "N_wsl[\"sglf\"] = int(0.1*N_wsl_total)\n",
    "N_wsl[\"sghf\"] = int(0.1*N_wsl_total)\n",
    "N_wsl[\"glitch\"] = int(0.05*N_wsl_total)\n",
    "\n",
    "\n",
    "snr_range = ['5-12','12-24','24-48','48-96']\n",
    "ratio = [0.25, 0.25, 0.25, 0.25]\n",
    "list_datatype = [\"noise\", \"bbh\", \"sglf\", \"sghf\", \"glitch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datatype_full = ['noise']\n",
    "\n",
    "for snr in snr_range:\n",
    "    list_datatype_full.append('bbh' + snr)\n",
    "    list_datatype_full.append('sglf' + snr)\n",
    "    list_datatype_full.append('sghf' + snr)\n",
    "    \n",
    "list_datatype_full.append('glitch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = ['glitch','noise', \n",
    "            'bbh5-12', 'bbh12-24', 'bbh24-48', 'bbh48-96',\n",
    "            'sglf5-12', 'sglf12-24', 'sglf24-48', 'sglf48-96',\n",
    "            'sghf5-12', 'sghf12-24', 'sghf24-48', 'sghf48-96',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(snr_range) == len(ratio)\n",
    "\n",
    "for i in range(len(ratio)):\n",
    "\n",
    "    N_wsl['bbh' + snr_range[i]] = int(ratio[i] * N_wsl['bbh'])\n",
    "    \n",
    "    N_wsl['sglf' + snr_range[i]] = int(ratio[i] * N_wsl['sglf'])\n",
    "    \n",
    "    N_wsl['sghf' + snr_range[i]] = int(ratio[i] * N_wsl['sghf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Introducing Chia-Jui's data\n",
    "\n",
    "renorm_factor_0 = 20;\n",
    "renorm_factor_1 = 20;\n",
    "\n",
    "realbkg_L = np.load('../Data_cached/real_bkg_2202000_63917s_4000Hz_50ms.npy')[:1000000].reshape(-1,1,200) / renorm_factor_0;\n",
    "realbkg_H = np.load('../Data_cached/real_bkg_H_1466640_58803s_4000Hz_50ms.npy')[:1000000].reshape(-1,1,200) / renorm_factor_0;\n",
    "\n",
    "realbkg = np.concatenate((realbkg_L, realbkg_H), axis = 1).reshape(-1,200)\n",
    "\n",
    "# realbbh = np.load('../Data_cached/injected_BBH_1823_around_merger_time_63917_58803.npz')['strain'].reshape(-1,200) / renorm_factor_0;\n",
    "\n",
    "# realsg = np.load('../Data_cached/injected_lfsg_1835_around_merger_time_63917_58803.npz')['strain'].reshape(-1,200) / renorm_factor_0;\n",
    "# realglitch = np.load(\"../data/real_glitches_9998_4000Hz_25ms.npz\")[\"strain_time_data\"]\n",
    "\n",
    "realbbh_list = {}\n",
    "realsglf_list = {}\n",
    "realsghf_list = {}\n",
    "\n",
    "\n",
    "for snr in snr_range:\n",
    "    realbbh_list[snr] = np.load('../Data_cached/injected_BBH_55k_snr{}_0th_events_before_merger_time_windowlength_200.npz'.format(snr))['strain'][:25000].reshape(-1,200)\n",
    "    realsglf_list[snr] = np.load('../Data_cached/injected_SGLF_55k_snr{}_0th_events_before_merger_time_windowlength_200.npz'.format(snr))['strain'][:25000].reshape(-1,200)\n",
    "    realsghf_list[snr] = np.load('../Data_cached/injected_SGHF_55k_snr{}_0th_events_before_merger_time_windowlength_200.npz'.format(snr))['strain'][:25000].reshape(-1,200)\n",
    "\n",
    "    \n",
    "\n",
    "realglitch_L = np.load(\"../Data_cached/real_glitches_snrlt5_60132_4000Hz_25ms.npz\")[\"strain_time_data\"][:50000].reshape(-1,1,200) / renorm_factor_1\n",
    "realglitch_H = np.load('../Data_cached/real_glitches_H_snrlt5_59732_4000Hz_25ms.npz')[\"strain_time_data\"][:50000].reshape(-1,1,200) / renorm_factor_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realglitch_L = np.load(\"../Data_cached/real_glitches_snrlt5_60132_4000Hz_25ms.npz\")[\"strain_time_data\"][:30000].reshape(-1,1,200) / renorm_factor_1\n",
    "realglitch_H = np.load('../Data_cached/real_glitches_H_snrlt5_59732_4000Hz_25ms.npz')[\"strain_time_data\"][:30000].reshape(-1,1,200) / renorm_factor_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novel procedure. For training sample, the model is purely glitch trained. For testing and WSL sample, the model is one glitch + one noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_glitch_for_AE = int(len(realglitch_L) * 0.8) - N_wsl['glitch'] // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_glitch_one_noise = int(len(realglitch_L) * 0.2) + N_wsl['glitch'] // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(realglitch_L)\n",
    "np.random.shuffle(realglitch_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realglitch_L.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realbkg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glitch_for_ae = np.concatenate((realglitch_L[:num_glitch_for_AE], realglitch_H[:num_glitch_for_AE]), axis=1).reshape(-1,200)\n",
    "glitch_for_ae_fft = abs(np.fft.rfft(glitch_for_ae))\n",
    "glitch_for_ae_fft = glitch_for_ae_fft/np.linalg.norm([glitch_for_ae_fft], axis=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_for_glitch_building = realbkg[:2 * num_one_glitch_one_noise].reshape(-1,2,200)\n",
    "\n",
    "glitch_L_noise_H = np.concatenate((realglitch_L[-num_one_glitch_one_noise:], noise_for_glitch_building[:,[1],:]), axis = 1)\n",
    "noise_L_glitch_H = np.concatenate((noise_for_glitch_building[:,[0],:], realglitch_H[-num_one_glitch_one_noise:]), axis = 1)\n",
    "one_glitch_one_noise = np.vstack((glitch_L_noise_H, noise_L_glitch_H))\n",
    "np.random.shuffle(one_glitch_one_noise)\n",
    "one_glitch_one_noise = one_glitch_one_noise.reshape(-1,200)\n",
    "\n",
    "realbkg = realbkg[2 * num_one_glitch_one_noise:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_glitch_one_noise_fft = abs(np.fft.rfft(one_glitch_one_noise))\n",
    "one_glitch_one_noise_fft = one_glitch_one_noise_fft / np.linalg.norm([one_glitch_one_noise_fft], axis=2).T\n",
    "\n",
    "one_glitch_one_noise = one_glitch_one_noise / np.linalg.norm([one_glitch_one_noise], axis = 2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realbkg_reserved = realbkg[-int(0.2 * len(realbkg)):]\n",
    "# realbbh_reserved = realbbh[-int(0.2 * len(realbbh)):]\n",
    "# realsg_reserved = realsg[-int(0.2 * len(realsg)):]\n",
    "# realglitch_reserved = realglitch[-int(0.2 * len(realglitch)):]\n",
    "\n",
    "realbkg = realbkg[:-int(0.2 * len(realbkg))]\n",
    "# realbbh = realbbh[:-int(0.2 * len(realbbh))-int(0.2 * len(realbbh))%2]\n",
    "# realsg = realsg[:-int(0.2 * len(realsg))-int(0.2 * len(realsg))%2]\n",
    "# realglitch = realglitch[:-int(0.2 * len(realglitch))]\n",
    "\n",
    "realbkg = realbkg / np.linalg.norm([realbkg], axis = 2).T\n",
    "\n",
    "bkg_fft = abs(np.fft.rfft(realbkg))\n",
    "bkg_fft = bkg_fft/np.linalg.norm([bkg_fft], axis=2).T\n",
    "\n",
    "# bbh_fft = abs(np.fft.rfft(realbbh))\n",
    "# bbh_fft = bbh_fft/np.linalg.norm([bbh_fft], axis=2).T\n",
    "\n",
    "# sg_fft = abs(np.fft.rfft(realsg))\n",
    "# sg_fft = sg_fft/np.linalg.norm([sg_fft], axis=2).T\n",
    "\n",
    "# glitch_fft = abs(np.fft.rfft(realglitch))\n",
    "# glitch_fft = glitch_fft/np.linalg.norm([glitch_fft], axis=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realbbh_reserved_list = {}\n",
    "realsglf_reserved_list = {}\n",
    "realsghf_reserved_list = {}\n",
    "\n",
    "realbbh_fft_list = {}\n",
    "realsglf_fft_list = {}\n",
    "realsghf_fft_list = {}\n",
    "\n",
    "\n",
    "for snr in snr_range:\n",
    "    realbbh_reserved_list[snr] = realbbh_list[snr][-int(0.2 * len(realbbh_list[snr]))-int(0.2 * len(realbbh_list[snr]))%2:]\n",
    "    realbbh_list[snr] = realbbh_list[snr][:-int(0.2 * len(realbbh_list[snr]))-int(0.2 * len(realbbh_list[snr]))%2]\n",
    "\n",
    "    realbbh_fft_list[snr] = abs(np.fft.rfft(realbbh_list[snr]))\n",
    "    realbbh_fft_list[snr] = realbbh_fft_list[snr]/np.linalg.norm([realbbh_fft_list[snr]], axis=2).T\n",
    "    \n",
    "    realbbh_list[snr] = realbbh_list[snr]/np.linalg.norm([realbbh_list[snr]], axis=2).T\n",
    "    \n",
    "    \n",
    "    realsglf_reserved_list[snr] = realsglf_list[snr][-int(0.2 * len(realsglf_list[snr]))-int(0.2 * len(realsglf_list[snr]))%2:]\n",
    "    realsglf_list[snr] = realsglf_list[snr][:-int(0.2 * len(realsglf_list[snr]))-int(0.2 * len(realsglf_list[snr]))%2]\n",
    "\n",
    "    realsglf_fft_list[snr] = abs(np.fft.rfft(realsglf_list[snr]))\n",
    "    realsglf_fft_list[snr] = realsglf_fft_list[snr]/np.linalg.norm([realsglf_fft_list[snr]], axis=2).T\n",
    "    \n",
    "    realsglf_list[snr] = realsglf_list[snr]/np.linalg.norm([realsglf_list[snr]], axis=2).T\n",
    "    \n",
    "    \n",
    "    realsghf_reserved_list[snr] = realsghf_list[snr][-int(0.2 * len(realsghf_list[snr]))-int(0.2 * len(realsghf_list[snr]))%2:]\n",
    "    realsghf_list[snr] = realsghf_list[snr][:-int(0.2 * len(realsghf_list[snr]))-int(0.2 * len(realsghf_list[snr]))%2]\n",
    "\n",
    "    realsghf_fft_list[snr] = abs(np.fft.rfft(realsghf_list[snr]))\n",
    "    realsghf_fft_list[snr] = realsghf_fft_list[snr]/np.linalg.norm([realsghf_fft_list[snr]], axis=2).T\n",
    "    \n",
    "    realsghf_list[snr] = realsghf_list[snr]/np.linalg.norm([realsghf_list[snr]], axis=2).T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_fft = bkg_fft.reshape(-1,202)\n",
    "\n",
    "# bbh_fft = bbh_fft.reshape(-1,202)\n",
    "\n",
    "# sg_fft = sg_fft.reshape(-1,202)\n",
    "\n",
    "# glitch_fft = glitch_fft.reshape(-1,202)\n",
    "\n",
    "glitch_for_ae_fft = glitch_for_ae_fft.reshape(-1,202)\n",
    "\n",
    "one_glitch_one_noise_fft = one_glitch_one_noise_fft.reshape(-1,202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = {}\n",
    "dataset_raw_fft = {}\n",
    "\n",
    "dataset_raw[\"noise\"] = realbkg.reshape(-1,400)\n",
    "# dataset_raw_fft[\"bbh\"] = bbh_fft\n",
    "# dataset_raw_fft[\"sg\"] = sg_fft\n",
    "for snr in snr_range:\n",
    "    dataset_raw['bbh' + snr] = realbbh_list[snr].reshape(-1,400)\n",
    "    dataset_raw['sglf' + snr] = realsglf_list[snr].reshape(-1,400)\n",
    "    dataset_raw['sghf' + snr] = realsghf_list[snr].reshape(-1,400)\n",
    "# dataset_raw[\"glitch\"] = glitch.reshape(-1,400)\n",
    "\n",
    "\n",
    "dataset_raw_fft[\"noise\"] = bkg_fft\n",
    "# dataset_raw_fft[\"bbh\"] = bbh_fft\n",
    "# dataset_raw_fft[\"sg\"] = sg_fft\n",
    "for snr in snr_range:\n",
    "    dataset_raw_fft['bbh' + snr] = realbbh_fft_list[snr].reshape(-1,202)\n",
    "    dataset_raw_fft['sglf' + snr] = realsglf_fft_list[snr].reshape(-1,202)\n",
    "    dataset_raw_fft['sghf' + snr] = realsghf_fft_list[snr].reshape(-1,202)\n",
    "# dataset_raw_fft[\"glitch\"] = glitch_fft\n",
    "\n",
    "dataset_wsl = {};\n",
    "dataset_ae = {};\n",
    "dataset_wsl_fft = {};\n",
    "dataset_ae_fft = {};\n",
    "\n",
    "for dt in list_datatype_full[:-1]:\n",
    "    perm = np.random.permutation(len(dataset_raw_fft[dt]))\n",
    "    # perm = np.loadtxt(\"../Data_Cached/SequentialTraining/WSL/perm_\"+dt+\"_2det_Chia-Jui_v7_GWAK.dat\").astype(int)\n",
    "    \n",
    "    nwsl = N_wsl[dt]\n",
    "    dataset_wsl[dt] = dataset_raw[dt][perm[:nwsl]]\n",
    "    dataset_wsl_fft[dt] = dataset_raw_fft[dt][perm[:nwsl]]\n",
    "    # dataset_wsl[dt] = dataset_wsl[dt] / np.linalg.norm([dataset_wsl[dt]], axis=2).T\n",
    "    # dataset_wsl_fft[dt] = abs(np.fft.rfft(dataset_wsl[dt]))\n",
    "    # dataset_wsl_fft[dt] = dataset_wsl_fft[dt]/np.linalg.norm([dataset_wsl_fft[dt]], axis=2).T\n",
    "    \n",
    "    dataset_ae[dt] = dataset_raw[dt][perm[nwsl:]]\n",
    "    dataset_ae_fft[dt]  = dataset_raw_fft[dt][perm[nwsl:]]\n",
    "    # dataset_ae[dt] = dataset_ae[dt] / np.linalg.norm([dataset_ae[dt]], axis=2).T\n",
    "    # dataset_ae_fft[dt] = abs(np.fft.rfft(dataset_ae[dt]))\n",
    "    # dataset_ae_fft[dt] = dataset_ae_fft[dt]/np.linalg.norm([dataset_ae_fft[dt]], axis=2).T\n",
    "    \n",
    "    # np.savetxt(\"../Data_Cached/SequentialTraining/WSL/perm_\"+dt+\"_2det_Chia-Jui_\"+version+\"_2.dat\", perm)\n",
    "    \n",
    "dataset_ae['bbh'] = np.zeros((0,400))\n",
    "dataset_ae['sglf'] = np.zeros((0,400))\n",
    "dataset_ae['sghf'] = np.zeros((0,400))\n",
    "\n",
    "dataset_ae_fft['bbh'] = np.zeros((0,202))\n",
    "dataset_ae_fft['sglf'] = np.zeros((0,202))\n",
    "dataset_ae_fft['sghf'] = np.zeros((0,202))\n",
    "\n",
    "for snr in snr_range:\n",
    "    dataset_ae['bbh'] = np.append(dataset_ae['bbh'], dataset_ae['bbh'+snr])\n",
    "    dataset_ae_fft['bbh'] = np.append(dataset_ae_fft['bbh'], dataset_ae_fft['bbh'+snr])\n",
    "    \n",
    "    dataset_ae['sglf'] = np.append(dataset_ae['sglf'], dataset_ae['sglf'+snr])\n",
    "    dataset_ae_fft['sglf'] = np.append(dataset_ae_fft['sglf'], dataset_ae_fft['sglf'+snr])\n",
    "    \n",
    "    dataset_ae['sghf'] = np.append(dataset_ae['sghf'], dataset_ae['sghf'+snr])\n",
    "    dataset_ae_fft['sghf'] = np.append(dataset_ae_fft['sghf'], dataset_ae_fft['sghf'+snr])\n",
    "\n",
    "dataset_ae['bbh'] = dataset_ae['bbh'].reshape(-1,400)\n",
    "dataset_ae['sglf'] = dataset_ae['sglf'].reshape(-1,400)\n",
    "dataset_ae['sghf'] = dataset_ae['sghf'].reshape(-1,400)\n",
    "\n",
    "dataset_ae_fft['bbh'] = dataset_ae_fft['bbh'].reshape(-1,202)\n",
    "dataset_ae_fft['sglf'] = dataset_ae_fft['sglf'].reshape(-1,202)\n",
    "dataset_ae_fft['sghf'] = dataset_ae_fft['sghf'].reshape(-1,202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw[dt].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl[dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_raw_fft[dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft['glitch'] = one_glitch_one_noise_fft[:N_wsl['glitch']]\n",
    "dataset_wsl['glitch'] = one_glitch_one_noise[:2 * N_wsl['glitch']].reshape(-1,400)\n",
    "\n",
    "dataset_ae_fft['glitch'] = glitch_for_ae_fft\n",
    "\n",
    "# Missing dataset ae ['glitch'] here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_wsl.keys():\n",
    "\n",
    "    print(np.linalg.norm(dataset_wsl[key][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_ae.keys():\n",
    "\n",
    "    print(np.linalg.norm(dataset_wsl[key][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_wsl_fft.keys():\n",
    "\n",
    "    print(np.linalg.norm(dataset_wsl[key][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ae_fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_ae.keys():\n",
    "    print(key)\n",
    "    print(dataset_ae[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_ae_fft.keys():\n",
    "    print(key)\n",
    "    print(dataset_ae_fft[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_ae_fft.keys():\n",
    "    print(key)\n",
    "    print(dataset_ae_fft[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_wsl.keys():\n",
    "    print(key)\n",
    "    print(dataset_wsl[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_wsl_fft.keys():\n",
    "    print(key)\n",
    "    print(dataset_wsl_fft[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft_collected = np.empty((0, 202))\n",
    "for dt in sequence:\n",
    "    dataset_wsl_fft_collected = np.vstack((dataset_wsl_fft_collected, dataset_wsl_fft[dt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_collected = np.empty((0, 400))\n",
    "for dt in sequence:\n",
    "    dataset_wsl_collected = np.vstack((dataset_wsl_collected, dataset_wsl[dt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft_collected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_collected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(202, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(10, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 202),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def count_trainable_params(model):\n",
    "    \"\"\"\n",
    "    计算给定PyTorch模型的可训练参数数量。\n",
    "    \n",
    "    参数:\n",
    "    model (nn.Module) - 要计算参数的PyTorch模型\n",
    "    \n",
    "    返回:\n",
    "    int - 模型的可训练参数数量\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    return trainable_params\n",
    "\n",
    "\n",
    "model = AutoEncoder()\n",
    "trainable_params = count_trainable_params(model)\n",
    "print(f\"Model has {trainable_params} trainable parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params_AE = count_trainable_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder_1det(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder_1det, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(101, 20),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(20, 101),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSClassifier_Onedetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WSClassifier_Onedetector, self).__init__()\n",
    "        self.fc1 = nn.Linear(101, 32)  # 第一层全连接层，输入维度为4，输出维度为64\n",
    "        self.norm1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()  # 激活函数\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        # self.norm2 = nn.BatchNorm1d(8)\n",
    "        # self.fc4 = nn.Linear(8, 1)  # 第三层全连接层，输入维度为32，输出维度为类别数目\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        # nn.init.kaiming_normal_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(self.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "        # x = self.relu(x)\n",
    "#         x = self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WSClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(202, 32)  # 第一层全连接层，输入维度为4，输出维度为64\n",
    "        self.norm1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()  # 激活函数\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        # self.norm2 = nn.BatchNorm1d(8)\n",
    "        # self.fc4 = nn.Linear(8, 1)  # 第三层全连接层，输入维度为32，输出维度为类别数目\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        # nn.init.kaiming_normal_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(self.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "        # x = self.relu(x)\n",
    "#         x = self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def count_trainable_params(model):\n",
    "    \"\"\"\n",
    "    计算给定PyTorch模型的可训练参数数量。\n",
    "    \n",
    "    参数:\n",
    "    model (nn.Module) - 要计算参数的PyTorch模型\n",
    "    \n",
    "    返回:\n",
    "    int - 模型的可训练参数数量\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    return trainable_params\n",
    "\n",
    "\n",
    "model = WSClassifier()\n",
    "trainable_params = count_trainable_params(model)\n",
    "print(f\"Model has {trainable_params} trainable parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params_WSC = count_trainable_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveletCNNAE_xc(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        num_ifos: int,\n",
    "        c_depth: int=8, \n",
    "        n_chann: int=64, \n",
    "        l1: int=1024\n",
    "        # lx: int=200\n",
    "    ):\n",
    "        \n",
    "        super(WaveletCNNAE_xc, self).__init__()\n",
    "        \n",
    "        self.c_depth = c_depth\n",
    "        self.n_chann = n_chann\n",
    "        \n",
    "        self.cap_norm = nn.GroupNorm(num_ifos, num_ifos)\n",
    "        \n",
    "        self.Conv_In_encode = nn.Conv1d(\n",
    "                in_channels=num_ifos, \n",
    "                out_channels=self.n_chann, \n",
    "                kernel_size=1\n",
    "            )\n",
    "        \n",
    "        self.Conv_Out_encode = nn.Conv1d(\n",
    "                in_channels=self.n_chann, \n",
    "                out_channels=1, \n",
    "                kernel_size=1\n",
    "            )\n",
    "        \n",
    "        self.Conv_In_decode = nn.Conv1d(\n",
    "                in_channels=1, \n",
    "                out_channels=self.n_chann, \n",
    "                kernel_size=1\n",
    "            )\n",
    "        \n",
    "        self.Conv_Out_decode = nn.Conv1d(\n",
    "                in_channels=self.n_chann, \n",
    "                out_channels=num_ifos, \n",
    "                kernel_size=1\n",
    "            )\n",
    "        \n",
    "        self.body_norm_encode = nn.GroupNorm(4 ,n_chann)\n",
    "        self.body_norm_decode = nn.GroupNorm(4 ,n_chann)\n",
    "        self.end_norm_encode = nn.BatchNorm1d(1)\n",
    "        self.end_norm_decode = nn.BatchNorm1d(1)\n",
    "        \n",
    "        self.WaveNet_layers_encode = nn.ModuleList()\n",
    "        self.WaveNet_layers_decode = nn.ModuleList()\n",
    "        self.WaveNet_layers_dp = nn.ModuleList()\n",
    "        \n",
    "        \n",
    "        for i in range(self.c_depth):\n",
    "\n",
    "            conv_layer = nn.Conv1d(\n",
    "                in_channels=self.n_chann, \n",
    "                out_channels=self.n_chann,\n",
    "                kernel_size=2,\n",
    "                dilation=2**i\n",
    "            )\n",
    "            \n",
    "            self.WaveNet_layers_encode.append(conv_layer)\n",
    "            \n",
    "        for i in range(self.c_depth-1, -1, -1):\n",
    "\n",
    "            conv_layer = nn.Conv1d(\n",
    "                in_channels=self.n_chann, \n",
    "                out_channels=self.n_chann,\n",
    "                kernel_size=2,\n",
    "                dilation=2**i\n",
    "            )\n",
    "            \n",
    "            self.WaveNet_layers_decode.append(conv_layer)\n",
    "            self.WaveNet_layers_dp.append(nn.ZeroPad1d(2**i))\n",
    "        \n",
    "        \n",
    "#         self.Padding_layer = nn.ZeroPad1d(2**c_depth - 1)\n",
    "                \n",
    "        # self.L1 = nn.Linear(8192-2**c_depth, l1)\n",
    "        \n",
    "        # Consider replacing other batch normalizatoin layers with other nor method\n",
    "        # Because batch norm are baised by the population of the CCSN rate in one batch \n",
    "        # This may produce overfitting model and will not be able to found at test phase\n",
    "        # Question: Will we be able to figure out the side effect at infereceing phase?\n",
    "                \n",
    "#         self.conv_norm = nn.BatchNorm1d(200-2**c_depth + 1)\n",
    "        self.L1 = nn.Linear(200-2**c_depth + 1, l1)\n",
    "        self.L1_norm = nn.BatchNorm1d(l1)\n",
    "        self.L2 = nn.Linear(l1, 200-2**c_depth + 1)\n",
    "        self.L2_norm = nn.BatchNorm1d(200-2**c_depth + 1)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.Conv_In_encode.weight)\n",
    "        nn.init.kaiming_normal_(self.Conv_Out_encode.weight)\n",
    "        nn.init.constant_(self.Conv_In_encode.bias, 0.001)\n",
    "        nn.init.constant_(self.Conv_Out_encode.bias, 0.001)\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.Conv_In_decode.weight)\n",
    "        nn.init.kaiming_normal_(self.Conv_Out_decode.weight)\n",
    "        nn.init.constant_(self.Conv_In_decode.bias, 0.001)\n",
    "        nn.init.constant_(self.Conv_Out_decode.bias, 0.001)\n",
    "\n",
    "        # Initialize all the convolutional layer in between\n",
    "        for conv_layer in self.WaveNet_layers_encode:\n",
    "            nn.init.kaiming_normal_(conv_layer.weight)\n",
    "            nn.init.constant_(conv_layer.bias, 0.001)\n",
    "            \n",
    "        for conv_layer in self.WaveNet_layers_decode:\n",
    "            nn.init.kaiming_normal_(conv_layer.weight)\n",
    "            nn.init.constant_(conv_layer.bias, 0.001)    \n",
    "\n",
    "        nn.init.kaiming_uniform_(self.L1.weight)\n",
    "        nn.init.kaiming_uniform_(self.L2.weight)\n",
    "        nn.init.constant_(self.L1.bias, 0.001)\n",
    "        nn.init.constant_(self.L2.bias, 0.001)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \n",
    "        x = self.cap_norm(x)\n",
    "        x = self.Conv_In_encode(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # x = self.norm(x)\n",
    "        \n",
    "        for what_are_u_wavin_at in self.WaveNet_layers_encode:\n",
    "            x = self.body_norm_encode(x)\n",
    "            x = what_are_u_wavin_at(x)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "        x = self.Conv_Out_encode(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.end_norm_encode(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.L1_norm(F.relu(self.L1(x)))\n",
    "        \n",
    "        # print('Encoder done')\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = self.L2_norm(F.relu(self.L2(x)))\n",
    "        \n",
    "#         x = self.Padding_layer(x)\n",
    "        \n",
    "        x = torch.unsqueeze(x,1)\n",
    "\n",
    "        # print(x.shape)\n",
    "        \n",
    "        # x = self.cap_norm(x)\n",
    "        x = self.Conv_In_decode(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # x = self.norm(x)\n",
    "        \n",
    "        for (pad, dcd) in zip(self.WaveNet_layers_dp, self.WaveNet_layers_decode):\n",
    "            # print(x.shape)\n",
    "            x = self.body_norm_decode(x)\n",
    "            x = pad(x)\n",
    "            x = torch.flip(dcd(torch.flip(x, [-1])), [-1])\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        # print('CNN done')\n",
    "        \n",
    "        x = self.Conv_Out_decode(x)\n",
    "        # print(x.shape)\n",
    "        x = F.tanh(x)\n",
    "        # print(x.shape)\n",
    "        # x = self.end_norm_decode(x)\n",
    "        \n",
    "        # x = torch.flatten(x, 1)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.decode(self.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAE(dataset0_to_be_examined, cutID, version, datatype):\n",
    "    \n",
    "    if len(dataset0_to_be_examined) > 10 * trainable_params_AE:\n",
    "        dataset = dataset0_to_be_examined[np.random.choice(len(dataset0_to_be_examined), 10 * trainable_params_AE, replace = False)]\n",
    "    else:\n",
    "        dataset = dataset0_to_be_examined\n",
    "\n",
    "    print('{} events passed to AE for training. '.format(len(dataset)))\n",
    "\n",
    "    nTotal = len(dataset);\n",
    "    nTrain = int(rTrain * nTotal)\n",
    "    nTest = int(rTest * nTotal)\n",
    "\n",
    "    X_train = dataset[:nTrain]\n",
    "    X_test = dataset[-nTest:]\n",
    "    X_validation = dataset[nTrain:-nTest]\n",
    "\n",
    "    trainData = torch.FloatTensor(X_train)\n",
    "    testData = torch.FloatTensor(X_test)\n",
    "    validationData = torch.FloatTensor(X_validation)\n",
    "\n",
    "    train_dataset = TensorDataset(trainData)\n",
    "    test_dataset = TensorDataset(testData)\n",
    "    validation_dataset = TensorDataset(validationData)\n",
    "\n",
    "    trainDataLoader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validationDataLoader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "    autoencoder = AutoEncoder().cuda()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.00005)\n",
    "    loss_func = nn.MSELoss().cuda()\n",
    "    \n",
    "    loss_train = np.empty(epochs)\n",
    "    loss_validation = np.empty(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        autoencoder.train()\n",
    "        for batchidx, x in enumerate(trainDataLoader):\n",
    "            x = x[0].cuda()\n",
    "            encoded, decoded = autoencoder(x)\n",
    "            loss_overall = loss_func(decoded, x)\n",
    "            weighted_lossTrain = loss_overall\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            weighted_lossTrain.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batchidx, x in enumerate(validationDataLoader):\n",
    "                x = x[0].cuda()\n",
    "                encoded, decoded = autoencoder(x)\n",
    "                lossVal = loss_func(decoded, x)\n",
    "                val_loss += lossVal.item()\n",
    "\n",
    "            val_loss /= len(validationDataLoader)\n",
    "\n",
    "        loss_train[epoch] = weighted_lossTrain.item()\n",
    "        loss_validation[epoch] = val_loss\n",
    "    \n",
    "    autoencoder.cpu().eval()\n",
    "    _, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax[0].plot(loss_train)\n",
    "    ax[0].plot(loss_validation)\n",
    "    \n",
    "    dcd_train = autoencoder(torch.FloatTensor(X_train))[1].detach().numpy()\n",
    "    err_train = np.var(X_train-dcd_train, axis=1)\n",
    "    dcd_test = autoencoder(torch.FloatTensor(X_test))[1].detach().numpy()\n",
    "    err_test = np.var(X_test-dcd_test, axis=1)\n",
    "    foo = ax[1].hist(err_train, range=(0, max(err_train)), bins=50, density=True, histtype=\"step\")\n",
    "    foo = ax[1].hist(err_test, range=(0, max(err_train)), bins=50, density=True, histtype=\"step\")\n",
    "    \n",
    "    plt.savefig(\"../Pic_cached/SequentialTraining/WSL/training_AE_\"+cutID+\"_\" + version + \"_\" + datatype +\"_trained.jpg\")\n",
    "    plt.close()\n",
    "            \n",
    "    return autoencoder.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAE_Onedetector(dataset0_to_be_examined, cutID, version, datatype):\n",
    "    \n",
    "    if len(dataset0_to_be_examined) > 10 * trainable_params_AE:\n",
    "        dataset = dataset0_to_be_examined[np.random.choice(len(dataset0_to_be_examined), 10 * trainable_params_AE, replace = False)]\n",
    "    else:\n",
    "        dataset = dataset0_to_be_examined\n",
    "\n",
    "    print('{} events passed to AE for training. '.format(len(dataset)))\n",
    "\n",
    "    nTotal = len(dataset);\n",
    "    nTrain = int(rTrain * nTotal)\n",
    "    nTest = int(rTest * nTotal)\n",
    "\n",
    "    X_train = dataset[:nTrain]\n",
    "    X_test = dataset[-nTest:]\n",
    "    X_validation = dataset[nTrain:-nTest]\n",
    "\n",
    "    trainData = torch.FloatTensor(X_train)\n",
    "    testData = torch.FloatTensor(X_test)\n",
    "    validationData = torch.FloatTensor(X_validation)\n",
    "\n",
    "    train_dataset = TensorDataset(trainData)\n",
    "    test_dataset = TensorDataset(testData)\n",
    "    validation_dataset = TensorDataset(validationData)\n",
    "\n",
    "    trainDataLoader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validationDataLoader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "    autoencoder = AutoEncoder_1det().cuda()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.00005)\n",
    "    loss_func = nn.MSELoss().cuda()\n",
    "    \n",
    "    loss_train = np.empty(epochs)\n",
    "    loss_validation = np.empty(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        autoencoder.train()\n",
    "        for batchidx, x in enumerate(trainDataLoader):\n",
    "            x = x[0].cuda()\n",
    "            encoded, decoded = autoencoder(x)\n",
    "            loss_overall = loss_func(decoded, x)\n",
    "            weighted_lossTrain = loss_overall\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            weighted_lossTrain.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batchidx, x in enumerate(validationDataLoader):\n",
    "                x = x[0].cuda()\n",
    "                encoded, decoded = autoencoder(x)\n",
    "                lossVal = loss_func(decoded, x)\n",
    "                val_loss += lossVal.item()\n",
    "\n",
    "            val_loss /= len(validationDataLoader)\n",
    "\n",
    "        loss_train[epoch] = weighted_lossTrain.item()\n",
    "        loss_validation[epoch] = val_loss\n",
    "    \n",
    "    autoencoder.cpu().eval()\n",
    "    _, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax[0].plot(loss_train)\n",
    "    ax[0].plot(loss_validation)\n",
    "    \n",
    "    dcd_train = autoencoder(torch.FloatTensor(X_train))[1].detach().numpy()\n",
    "    err_train = np.var(X_train-dcd_train, axis=1)\n",
    "    dcd_test = autoencoder(torch.FloatTensor(X_test))[1].detach().numpy()\n",
    "    err_test = np.var(X_test-dcd_test, axis=1)\n",
    "    foo = ax[1].hist(err_train, range=(0, max(err_train)), bins=50, density=True, histtype=\"step\")\n",
    "    foo = ax[1].hist(err_test, range=(0, max(err_train)), bins=50, density=True, histtype=\"step\")\n",
    "    \n",
    "    plt.savefig(\"../Pic_cached/SequentialTraining/WSL/training_AE_\"+cutID+\"_\" + version + \"_\" + datatype +\"_trained.jpg\")\n",
    "    plt.close()\n",
    "            \n",
    "    return autoencoder.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWSC(dataset0_to_be_examined, dataset1, cutID, version, datatype):\n",
    "# dataset0: bkg set from AE\n",
    "# dataset1: identified signal from AE\n",
    "\n",
    "    if len(dataset0_to_be_examined) > 10 * trainable_params_WSC:\n",
    "        dataset0 = dataset0_to_be_examined[np.random.choice(len(dataset0_to_be_examined), 10 * trainable_params_WSC, replace = False)]\n",
    "    else:\n",
    "        dataset0 = dataset0_to_be_examined\n",
    "    \n",
    "    print('{} noise events and {} signal events passed to WSC for training. '.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    nTotal0, nTotal1 = len(dataset0), len(dataset1);\n",
    "    nTrain0, nTrain1 = int(rTrain * nTotal0), int(rTrain * nTotal1)\n",
    "    nTest0 , nTest1  = int(rTest * nTotal0) , int(rTest * nTotal1)\n",
    "\n",
    "    X_train = np.concatenate((dataset0[:nTrain0], dataset1[:nTrain1]))\n",
    "    X_test = np.concatenate((dataset0[-nTest0:], dataset1[-nTest1:]))\n",
    "    X_validation = np.concatenate((dataset0[nTrain0:-nTest0], dataset1[nTrain1:-nTest1]))\n",
    "    \n",
    "    Y_train = np.concatenate((np.zeros((nTrain0, 1)), np.ones((nTrain1, 1))))\n",
    "    Y_test = np.concatenate((np.zeros((nTest0, 1)), np.ones((nTest1, 1))))\n",
    "    Y_validation = np.concatenate((np.zeros((dataset0[nTrain0:-nTest0].shape[0], 1)), np.ones((dataset1[nTrain1:-nTest1].shape[0], 1))))\n",
    "\n",
    "#     trainData = torch.FloatTensor(X_train)\n",
    "#     testData = torch.FloatTensor(X_test)\n",
    "#     validationData = torch.FloatTensor(X_validation)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(Y_train))\n",
    "    validation_dataset = TensorDataset(torch.FloatTensor(X_validation), torch.FloatTensor(Y_validation))\n",
    "#     train_dataset = TensorDataset(torch.FloatTensor(X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))), torch.FloatTensor(Y_train.reshape((Y_train.shape[0], 1, Y_train.shape[1]))))\n",
    "#     validation_dataset = TensorDataset(torch.FloatTensor(X_validation.reshape((X_validation.shape[0], 1, X_validation.shape[1]))), torch.FloatTensor(Y_validation.reshape((Y_validation.shape[0], 1, Y_validation.shape[1]))))\n",
    "\n",
    "    trainDataLoader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    validationDataLoader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle = True, drop_last=True)\n",
    "\n",
    "    wsc = WSClassifier().cuda()\n",
    "    optimizer = optim.Adam(wsc.parameters(), lr=0.00005)\n",
    "    loss_func = nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([nTrain0/nTrain1])).cuda()\n",
    "    \n",
    "    loss_train = np.empty(epochs)\n",
    "    loss_validation = np.empty(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "#         t0 = time.time()\n",
    "        wsc.train()\n",
    "        for batchidx, (x, y) in enumerate(trainDataLoader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            yprime = wsc(x)\n",
    "            loss = loss_func(yprime, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        wsc.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batchidx, (x, y) in enumerate(validationDataLoader):\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                yprime = wsc(x)\n",
    "                lossVal = loss_func(yprime, y)\n",
    "                val_loss += lossVal.item()\n",
    "\n",
    "            val_loss /= len(validationDataLoader)\n",
    "\n",
    "        loss_train[epoch] = loss.item()\n",
    "        loss_validation[epoch] = val_loss\n",
    "#         print(time.time() - t0)\n",
    "        \n",
    "    wsc.cpu().eval()\n",
    "    \n",
    "    _, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax[0].plot(loss_train)\n",
    "    ax[0].plot(loss_validation)\n",
    "    foo = ax[1].hist(nn.Sigmoid()(wsc(torch.FloatTensor(X_train))).detach().numpy().flatten(), range=(0, 1), bins=20, density=True, histtype=\"step\")\n",
    "    foo = ax[1].hist(nn.Sigmoid()(wsc(torch.FloatTensor(X_test ))).detach().numpy().flatten(), range=(0, 1), bins=20, density=True, histtype=\"step\")\n",
    "    \n",
    "    plt.savefig(\"../Pic_cached/SequentialTraining/WSL/training_WSC_\"+cutID+\"_\" + version + \"_\" + datatype +\"_trained.jpg\")\n",
    "    plt.close()\n",
    "    \n",
    "    return wsc.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWSC_Onedetector(dataset0_to_be_examined, dataset1, cutID, version, datatype):\n",
    "# dataset0: bkg set from AE\n",
    "# dataset1: identified signal from AE\n",
    "\n",
    "    if len(dataset0_to_be_examined) > 10 * trainable_params_WSC:\n",
    "        dataset0 = dataset0_to_be_examined[np.random.choice(len(dataset0_to_be_examined), 10 * trainable_params_WSC, replace = False)]\n",
    "    else:\n",
    "        dataset0 = dataset0_to_be_examined\n",
    "    \n",
    "    print('{} noise events and {} signal events passed to WSC for training. '.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    nTotal0, nTotal1 = len(dataset0), len(dataset1);\n",
    "    nTrain0, nTrain1 = int(rTrain * nTotal0), int(rTrain * nTotal1)\n",
    "    nTest0 , nTest1  = int(rTest * nTotal0) , int(rTest * nTotal1)\n",
    "\n",
    "    X_train = np.concatenate((dataset0[:nTrain0], dataset1[:nTrain1]))\n",
    "    X_test = np.concatenate((dataset0[-nTest0:], dataset1[-nTest1:]))\n",
    "    X_validation = np.concatenate((dataset0[nTrain0:-nTest0], dataset1[nTrain1:-nTest1]))\n",
    "    \n",
    "    Y_train = np.concatenate((np.zeros((nTrain0, 1)), np.ones((nTrain1, 1))))\n",
    "    Y_test = np.concatenate((np.zeros((nTest0, 1)), np.ones((nTest1, 1))))\n",
    "    Y_validation = np.concatenate((np.zeros((dataset0[nTrain0:-nTest0].shape[0], 1)), np.ones((dataset1[nTrain1:-nTest1].shape[0], 1))))\n",
    "\n",
    "#     trainData = torch.FloatTensor(X_train)\n",
    "#     testData = torch.FloatTensor(X_test)\n",
    "#     validationData = torch.FloatTensor(X_validation)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(Y_train))\n",
    "    validation_dataset = TensorDataset(torch.FloatTensor(X_validation), torch.FloatTensor(Y_validation))\n",
    "#     train_dataset = TensorDataset(torch.FloatTensor(X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))), torch.FloatTensor(Y_train.reshape((Y_train.shape[0], 1, Y_train.shape[1]))))\n",
    "#     validation_dataset = TensorDataset(torch.FloatTensor(X_validation.reshape((X_validation.shape[0], 1, X_validation.shape[1]))), torch.FloatTensor(Y_validation.reshape((Y_validation.shape[0], 1, Y_validation.shape[1]))))\n",
    "\n",
    "    trainDataLoader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    validationDataLoader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle = True, drop_last=True)\n",
    "\n",
    "    wsc = WSClassifier_Onedetector().cuda()\n",
    "    optimizer = optim.Adam(wsc.parameters(), lr=0.00005)\n",
    "    loss_func = nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([nTrain0/nTrain1])).cuda()\n",
    "    \n",
    "    loss_train = np.empty(epochs)\n",
    "    loss_validation = np.empty(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "#         t0 = time.time()\n",
    "        wsc.train()\n",
    "        for batchidx, (x, y) in enumerate(trainDataLoader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            yprime = wsc(x)\n",
    "            loss = loss_func(yprime, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        wsc.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batchidx, (x, y) in enumerate(validationDataLoader):\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                yprime = wsc(x)\n",
    "                lossVal = loss_func(yprime, y)\n",
    "                val_loss += lossVal.item()\n",
    "\n",
    "            val_loss /= len(validationDataLoader)\n",
    "\n",
    "        loss_train[epoch] = loss.item()\n",
    "        loss_validation[epoch] = val_loss\n",
    "#         print(time.time() - t0)\n",
    "        \n",
    "    wsc.cpu().eval()\n",
    "    \n",
    "    _, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax[0].plot(loss_train)\n",
    "    ax[0].plot(loss_validation)\n",
    "    foo = ax[1].hist(nn.Sigmoid()(wsc(torch.FloatTensor(X_train))).detach().numpy().flatten(), range=(0, 1), bins=20, density=True, histtype=\"step\")\n",
    "    foo = ax[1].hist(nn.Sigmoid()(wsc(torch.FloatTensor(X_test ))).detach().numpy().flatten(), range=(0, 1), bins=20, density=True, histtype=\"step\")\n",
    "    \n",
    "    plt.savefig(\"../Pic_cached/SequentialTraining/WSL/training_WSC_\"+cutID+\"_\" + version + \"_\" + datatype +\"_trained.jpg\")\n",
    "    plt.close()\n",
    "    \n",
    "    return wsc.cpu().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass the first glith double WSL (pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the noise AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the noise WSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the BBH CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the BBH CNN WSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, let the test sample (WSL training sample) pass the procedure to get the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final label WSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v12'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First assume the CNN AE and the WSL is pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ncut = 5;\n",
    "cutList = {};\n",
    "\n",
    "max_glitch_L = 0.0024;\n",
    "min_glitch_L = 0.0008;\n",
    "cutList[\"glitch_L\"] = np.linspace(min_glitch_L, max_glitch_L, Ncut);\n",
    "\n",
    "max_glitch_H = 0.0024;\n",
    "min_glitch_H = 0.0004;\n",
    "cutList[\"glitch_H\"] = np.linspace(min_glitch_H, max_glitch_H, Ncut);\n",
    "\n",
    "max_bkg = 0.0018;\n",
    "min_bkg = 0.0008;\n",
    "cutList[\"noise\"] = np.linspace(min_bkg, max_bkg, Ncut);\n",
    "\n",
    "\n",
    "max_bbh_cnn = 0.0056\n",
    "min_bbh_cnn = 0.0045\n",
    "cutList['bbh_CNN'] = np.linspace(min_bbh_cnn, max_bbh_cnn, Ncut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0;\n",
    "\n",
    "ic = np.zeros(2, dtype=\"int\")\n",
    "\n",
    "# loop for only the cut in glitch, noise and bbh as it's not really meaningful to set cut in sg w/o new signals\n",
    "# ic[3] = Ncut-1;\n",
    "# ic[4] = Ncut-1;\n",
    "\n",
    "# listResult = {};\n",
    "# listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)), dtype=\"int\");\n",
    "# listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype)-1), len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype)-1), 2))\n",
    "\n",
    "for ic[0], ic[1] in itertools.product(np.arange(Ncut), np.arange(Ncut)):\n",
    "# for ic[0], ic[1], ic[2], ic[3] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "    cnt += 1;\n",
    "    \n",
    "    if cnt < 23:\n",
    "        continue\n",
    "    # elif cnt > 85:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in ['glitch', 'noise', 'bbh', 'sglf', 'sghf']:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_fft_filtered = dataset_wsl_fft_collected\n",
    "    dataset_wsl_filtered = dataset_wsl_collected\n",
    "    \n",
    "    cutID = \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version\n",
    "    \n",
    "    \n",
    "\n",
    "        # --- Glitch AE+WSL ---\n",
    "\n",
    "    iPrev = 0\n",
    "    \n",
    "    previousStep = 'glitch';\n",
    "    # modelPrev_L = models['glitch_L']; # previous step AE\n",
    "    # modelPrev_H = models['glitch_H']; # previous step AE\n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    # dcd_L = modelPrev_L(torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))[1].detach().numpy();\n",
    "    # dcd_H = modelPrev_H(torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))[1].detach().numpy();\n",
    "    # dataset1 = dataset_wsl_fft_filtered[np.logical_and(np.var(dataset_wsl_fft_filtered[:,:101]-dcd_L, axis=1) >= cutList['glitch_L'][ic[0]],\n",
    "    #                                                np.var(dataset_wsl_fft_filtered[:,101:]-dcd_H, axis=1) >= cutList['glitch_H'][ic[1]])]\n",
    "    \n",
    "    # model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    # models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "    # model_L = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_L']\n",
    "    # model_H = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_H']\n",
    "\n",
    "    # model_L = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(['1','0']) + '_v2.json')['glitch_L']\n",
    "    # model_H = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(['1','0']) + '_v2.json')['glitch_H']\n",
    "    \n",
    "    # --- Train the glitch AE for L\n",
    "\n",
    "    models[previousStep+\"_WSC_L\"] = model_L\n",
    "    models[previousStep+\"_WSC_H\"] = model_H\n",
    "\n",
    "    \n",
    "    # filter the data according to previous WSC\n",
    "    for dt in ['glitch', 'noise', 'bbh', 'sglf', 'sghf']:\n",
    "        dcd_L = nn.Sigmoid()(model_L(torch.FloatTensor(data_filtered[dt][:,:101]))).detach().numpy().flatten();\n",
    "        dcd_H = nn.Sigmoid()(model_H(torch.FloatTensor(data_filtered[dt][:,101:]))).detach().numpy().flatten();\n",
    "        data_filtered[dt] = data_filtered[dt][np.logical_and(dcd_L > 0.5, dcd_H > 0.5)]\n",
    "    \n",
    "    dcd_L = nn.Sigmoid()(model_L(torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))).detach().numpy().flatten();\n",
    "    dcd_H = nn.Sigmoid()(model_H(torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))).detach().numpy().flatten();\n",
    "\n",
    "    dataset_wsl_fft_filtered = dataset_wsl_fft_filtered[np.logical_and(dcd_L > 0.5, dcd_H > 0.5)]\n",
    "    dataset_wsl_filtered = dataset_wsl_filtered[np.logical_and(dcd_L > 0.5, dcd_H > 0.5)]\n",
    "\n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "    \n",
    "    # train the current step AE\n",
    "\n",
    "    print('Start noise AE')\n",
    "\n",
    "    print('Start training the noise WSL, dataset 0 size {}'.format(len(data_filtered['noise'])))\n",
    "\n",
    "    currentStep = 'noise';\n",
    "    model = trainAE(data_filtered[currentStep], cutID, version, currentStep);\n",
    "    models[currentStep] = model;   \n",
    "    \n",
    "    print('Noise AE trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "    \n",
    "    # --- Noise AE+WSL ---\n",
    "    # Here's a cut scan for the noise AE\n",
    "\n",
    "    print('Start noise WSL')\n",
    "\n",
    "    iPrev = 1\n",
    "\n",
    "    previousStep = 'noise';\n",
    "    modelPrev = models[previousStep]; # previous step AE\n",
    "    \n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    # dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "    # dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[0]]]\n",
    "    \n",
    "    dcd = modelPrev(torch.FloatTensor(dataset_wsl_fft_filtered))[1].detach().numpy();\n",
    "    dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered-dcd, axis=1) >= cutList[previousStep][ic[0]]]\n",
    "    \n",
    "    print('Start training the noise WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "    # --- Here I just filter the noise sample ---\n",
    "\n",
    "    data_noise_noiselike = data_filtered['noise'][nn.Sigmoid()(model(torch.FloatTensor(data_filtered['noise']))).detach().numpy().flatten() <= 0.5]\n",
    "    signal_noise_like_args = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft_filtered))).detach().numpy().flatten() <= 0.5\n",
    "    \n",
    "    print('Noise WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "\n",
    "    # data_wsl_noiselike = {}\n",
    "\n",
    "    # filter the data according to previous WSC\n",
    "    # for j in range(iPrev, 4):\n",
    "    #     dt = ind2datatype[j];\n",
    "    #     dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "    #     data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "    #     data_noiselike[dt] = data_filtered[dt][dcd<=0.5]\n",
    "\n",
    "    # dcd = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_filtered))).detach().numpy().flatten()\n",
    "    # data_wsl_noiselike = dataset_wsl_collected[dcd <= 0.5].reshape(-1,2,200)\n",
    "    \n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "    # --- BBH CNN+WSL ---\n",
    "    # Here's a cut scan for BBH CNN\n",
    "\n",
    "    print('Start BBHCNN WSL')\n",
    "\n",
    "    models['BBH_CNN'] = torch.load('../Model_cached/CNN_BBH/model_dep_3_chnl_4_btn_20_v4.pt')\n",
    "\n",
    "    # Note that for CNN, we have to use the initial input data (2 * 200 in timeseries)\n",
    "\n",
    "    dataset0 = data_noise_noiselike\n",
    "\n",
    "    dcd = models['BBH_CNN'](torch.FloatTensor(dataset_wsl_filtered[signal_noise_like_args].reshape(-1,2,200))).detach().numpy()\n",
    "    dataset1 = dataset_wsl_fft_filtered[signal_noise_like_args][np.mean(np.var(dataset_wsl_filtered[signal_noise_like_args].reshape(-1,2,200)-dcd, axis = 2), axis = 1) <= cutList['bbh_CNN'][ic[1]]]\n",
    "\n",
    "    print('Start training the BBHCNN WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "    \n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, 'BBH_CNN')\n",
    "    models['BBH_CNN_WSC'] = model\n",
    "\n",
    "    print('BBHCNN WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "    \n",
    "    # --- Finalized WSL ---\n",
    "\n",
    "    print('Start final WSL for cnt = {}'.format(cnt))\n",
    "\n",
    "    dcd = {}\n",
    "\n",
    "    dcd['glitch_WSC_L'] = nn.Sigmoid()(models['glitch_WSC_L'](torch.FloatTensor(dataset_wsl_fft_collected[:,:101]))).detach().numpy().flatten()\n",
    "    dcd['glitch_WSC_H'] = nn.Sigmoid()(models['glitch_WSC_H'](torch.FloatTensor(dataset_wsl_fft_collected[:,101:]))).detach().numpy().flatten()\n",
    "\n",
    "\n",
    "    for dt in ['noise_WSC', 'BBH_CNN_WSC']:\n",
    "        dcd[dt] = nn.Sigmoid()(models[dt](torch.FloatTensor(dataset_wsl_fft_collected))).detach().numpy().flatten()\n",
    "\n",
    "    glitch_pass = np.logical_and(dcd['glitch_WSC_L'] > 0.5, dcd['glitch_WSC_H'] > 0.5)\n",
    "\n",
    "    noise_pass = np.logical_or(dcd['noise_WSC'] > 0.5, np.logical_and(dcd['noise_WSC'] <= 0.5, dcd['BBH_CNN_WSC'] > 0.5))\n",
    "\n",
    "    passed_args = np.logical_and(glitch_pass, noise_pass)\n",
    "\n",
    "    # dataset0 = np.concatenate((dataset_ae_fft['glitch'], dataset_ae_fft['noise']), axis = 0)\n",
    "\n",
    "    dataset0 = data_filtered['noise']\n",
    "\n",
    "    dataset1 = dataset_wsl_fft_collected[passed_args]\n",
    "\n",
    "    print('Start training the final WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, 'Final')\n",
    "\n",
    "    models['Final_WSC'] = model\n",
    "\n",
    "\n",
    "    print('Final WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "\n",
    "    torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    print(models.keys())\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     dcd = {};\n",
    "#     err = {};\n",
    "#     ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "#     for datatype in list_datatype:\n",
    "#         dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "#         err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "        \n",
    "#     not_select = np.array([True]*len(testset));\n",
    "\n",
    "#     for iStep in range(len(list_datatype)):\n",
    "#         datatype = ind2datatype[iStep];\n",
    "#         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "#         ans[ind_pass] = iStep;\n",
    "#         not_select[ind_pass] = False;\n",
    "        \n",
    "#     ans[not_select] = -1;\n",
    "    \n",
    "#     listResult[\"cut\"][cnt] = ic;\n",
    "#     listResult[\"ans\"][cnt] = ans;\n",
    "    \n",
    "#     acc = np.zeros(len(ind2datatype));\n",
    "    \n",
    "#     for i in range(len(ind2datatype)):\n",
    "#         acc[i] = np.sum(np.logical_and(ans==i, correct_ans==i))/Nsample[ind2datatype[i]];\n",
    "        \n",
    "#     listResult[\"accuracy_4\"][cnt] = acc;\n",
    "    \n",
    "#     listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(Nsample[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "#                                      np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(Nsample[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "    # cnt += 1\n",
    "    print(cnt)\n",
    "    print(time.time() - t0)\n",
    "    \n",
    "# listResult[\"total_accuracy\"] = np.sum(listResult[\"ans\"]==correct_ans, axis=1)/len(testset);\n",
    "# torch.save(listResult, \"../data/SequentialTraining/training_performance_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0;\n",
    "\n",
    "ic = np.zeros(2, dtype=\"int\")\n",
    "\n",
    "# loop for only the cut in glitch, noise and bbh as it's not really meaningful to set cut in sg w/o new signals\n",
    "# ic[3] = Ncut-1;\n",
    "# ic[4] = Ncut-1;\n",
    "\n",
    "# listResult = {};\n",
    "# listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)), dtype=\"int\");\n",
    "# listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype)-1), len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype)-1), 2))\n",
    "\n",
    "for ic[0], ic[1] in itertools.product(np.arange(Ncut), np.arange(Ncut)):\n",
    "# for ic[0], ic[1], ic[2], ic[3] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "    cnt += 1;\n",
    "    \n",
    "    if not np.all(ic == np.array([4,0])):\n",
    "        continue\n",
    "    # elif cnt > 85:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in ['glitch', 'noise', 'bbh', 'sglf', 'sghf']:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_fft_filtered = dataset_wsl_fft_collected\n",
    "    dataset_wsl_filtered = dataset_wsl_collected\n",
    "    \n",
    "    cutID = \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version\n",
    "    \n",
    "    \n",
    "\n",
    "        # --- Glitch AE+WSL ---\n",
    "\n",
    "    iPrev = 0\n",
    "    \n",
    "    previousStep = 'glitch';\n",
    "    # modelPrev_L = models['glitch_L']; # previous step AE\n",
    "    # modelPrev_H = models['glitch_H']; # previous step AE\n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    # dcd_L = modelPrev_L(torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))[1].detach().numpy();\n",
    "    # dcd_H = modelPrev_H(torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))[1].detach().numpy();\n",
    "    # dataset1 = dataset_wsl_fft_filtered[np.logical_and(np.var(dataset_wsl_fft_filtered[:,:101]-dcd_L, axis=1) >= cutList['glitch_L'][ic[0]],\n",
    "    #                                                np.var(dataset_wsl_fft_filtered[:,101:]-dcd_H, axis=1) >= cutList['glitch_H'][ic[1]])]\n",
    "    \n",
    "    # model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    # models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "    # model_L = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_L']\n",
    "    # model_H = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_H']\n",
    "\n",
    "    model_L = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(['1','0']) + '_v2.json')['glitch_L']\n",
    "    model_H = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(['1','0']) + '_v2.json')['glitch_H']\n",
    "    \n",
    "    # --- Train the glitch AE for L\n",
    "\n",
    "    models[previousStep+\"_WSC_L\"] = model_L\n",
    "    models[previousStep+\"_WSC_H\"] = model_H\n",
    "\n",
    "    \n",
    "    # filter the data according to previous WSC\n",
    "    for dt in ['glitch', 'noise', 'bbh', 'sglf', 'sghf']:\n",
    "        dcd_L = nn.Sigmoid()(model_L(torch.FloatTensor(data_filtered[dt][:,:101]))).detach().numpy().flatten();\n",
    "        dcd_H = nn.Sigmoid()(model_H(torch.FloatTensor(data_filtered[dt][:,101:]))).detach().numpy().flatten();\n",
    "        data_filtered[dt] = data_filtered[dt][np.logical_and(dcd_L >= 0, dcd_H > 0.5)]\n",
    "    \n",
    "    dcd_L = nn.Sigmoid()(model_L(torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))).detach().numpy().flatten();\n",
    "    dcd_H = nn.Sigmoid()(model_H(torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))).detach().numpy().flatten();\n",
    "\n",
    "    dataset_wsl_fft_filtered = dataset_wsl_fft_filtered[np.logical_and(dcd_L >= 0, dcd_H > 0.5)]\n",
    "    dataset_wsl_filtered = dataset_wsl_filtered[np.logical_and(dcd_L >= 0, dcd_H > 0.5)]\n",
    "\n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "    \n",
    "    # train the current step AE\n",
    "\n",
    "    print('Start noise AE')\n",
    "\n",
    "    print('Start training the noise WSL, dataset 0 size {}'.format(len(data_filtered['noise'])))\n",
    "\n",
    "    currentStep = 'noise';\n",
    "    model = trainAE(data_filtered[currentStep], cutID, version, currentStep);\n",
    "    models[currentStep] = model;   \n",
    "    \n",
    "    print('Noise AE trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "    \n",
    "    # --- Noise AE+WSL ---\n",
    "    # Here's a cut scan for the noise AE\n",
    "\n",
    "    print('Start noise WSL')\n",
    "\n",
    "    iPrev = 1\n",
    "\n",
    "    previousStep = 'noise';\n",
    "    modelPrev = models[previousStep]; # previous step AE\n",
    "    \n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    # dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "    # dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[0]]]\n",
    "    \n",
    "    dcd = modelPrev(torch.FloatTensor(dataset_wsl_fft_filtered))[1].detach().numpy();\n",
    "    dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered-dcd, axis=1) >= cutList[previousStep][ic[0]]]\n",
    "    \n",
    "    print('Start training the noise WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "    # --- Here I just filter the noise sample ---\n",
    "\n",
    "    data_noise_noiselike = data_filtered['noise'][nn.Sigmoid()(model(torch.FloatTensor(data_filtered['noise']))).detach().numpy().flatten() <= 0.5]\n",
    "    signal_noise_like_args = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft_filtered))).detach().numpy().flatten() <= 0.5\n",
    "    \n",
    "    print('Noise WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "\n",
    "    # data_wsl_noiselike = {}\n",
    "\n",
    "    # filter the data according to previous WSC\n",
    "    # for j in range(iPrev, 4):\n",
    "    #     dt = ind2datatype[j];\n",
    "    #     dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "    #     data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "    #     data_noiselike[dt] = data_filtered[dt][dcd<=0.5]\n",
    "\n",
    "    # dcd = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_filtered))).detach().numpy().flatten()\n",
    "    # data_wsl_noiselike = dataset_wsl_collected[dcd <= 0.5].reshape(-1,2,200)\n",
    "    \n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "    # --- BBH CNN+WSL ---\n",
    "    # Here's a cut scan for BBH CNN\n",
    "\n",
    "    print('Start BBHCNN WSL')\n",
    "\n",
    "    models['BBH_CNN'] = torch.load('../Model_cached/CNN_BBH/model_dep_3_chnl_4_btn_20_v4.pt')\n",
    "\n",
    "    # Note that for CNN, we have to use the initial input data (2 * 200 in timeseries)\n",
    "\n",
    "    dataset0 = data_noise_noiselike\n",
    "\n",
    "    dcd = models['BBH_CNN'](torch.FloatTensor(dataset_wsl_filtered[signal_noise_like_args].reshape(-1,2,200))).detach().numpy()\n",
    "    dataset1 = dataset_wsl_fft_filtered[signal_noise_like_args][np.mean(np.var(dataset_wsl_filtered[signal_noise_like_args].reshape(-1,2,200)-dcd, axis = 2), axis = 1) <= cutList['bbh_CNN'][ic[1]]]\n",
    "\n",
    "    print('Start training the BBHCNN WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "    \n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, 'BBH_CNN')\n",
    "    models['BBH_CNN_WSC'] = model\n",
    "\n",
    "    print('BBHCNN WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "    \n",
    "    # --- Finalized WSL ---\n",
    "\n",
    "    print('Start final WSL for cnt = {}'.format(cnt))\n",
    "\n",
    "    dcd = {}\n",
    "\n",
    "    dcd['glitch_WSC_L'] = nn.Sigmoid()(models['glitch_WSC_L'](torch.FloatTensor(dataset_wsl_fft_collected[:,:101]))).detach().numpy().flatten()\n",
    "    dcd['glitch_WSC_H'] = nn.Sigmoid()(models['glitch_WSC_H'](torch.FloatTensor(dataset_wsl_fft_collected[:,101:]))).detach().numpy().flatten()\n",
    "\n",
    "\n",
    "    for dt in ['noise_WSC', 'BBH_CNN_WSC']:\n",
    "        dcd[dt] = nn.Sigmoid()(models[dt](torch.FloatTensor(dataset_wsl_fft_collected))).detach().numpy().flatten()\n",
    "\n",
    "    glitch_pass = np.logical_and(dcd['glitch_WSC_L'] > 0.5, dcd['glitch_WSC_H'] > 0.5)\n",
    "\n",
    "    noise_pass = np.logical_or(dcd['noise_WSC'] > 0.5, np.logical_and(dcd['noise_WSC'] <= 0.5, dcd['BBH_CNN_WSC'] > 0.5))\n",
    "\n",
    "    passed_args = np.logical_and(glitch_pass, noise_pass)\n",
    "\n",
    "    # dataset0 = np.concatenate((dataset_ae_fft['glitch'], dataset_ae_fft['noise']), axis = 0)\n",
    "\n",
    "    dataset0 = data_filtered['noise']\n",
    "\n",
    "    dataset1 = dataset_wsl_fft_collected[passed_args]\n",
    "\n",
    "    print('Start training the final WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, 'Final')\n",
    "\n",
    "    models['Final_WSC'] = model\n",
    "\n",
    "\n",
    "    print('Final WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "\n",
    "    torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    print(models.keys())\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     dcd = {};\n",
    "#     err = {};\n",
    "#     ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "#     for datatype in list_datatype:\n",
    "#         dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "#         err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "        \n",
    "#     not_select = np.array([True]*len(testset));\n",
    "\n",
    "#     for iStep in range(len(list_datatype)):\n",
    "#         datatype = ind2datatype[iStep];\n",
    "#         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "#         ans[ind_pass] = iStep;\n",
    "#         not_select[ind_pass] = False;\n",
    "        \n",
    "#     ans[not_select] = -1;\n",
    "    \n",
    "#     listResult[\"cut\"][cnt] = ic;\n",
    "#     listResult[\"ans\"][cnt] = ans;\n",
    "    \n",
    "#     acc = np.zeros(len(ind2datatype));\n",
    "    \n",
    "#     for i in range(len(ind2datatype)):\n",
    "#         acc[i] = np.sum(np.logical_and(ans==i, correct_ans==i))/Nsample[ind2datatype[i]];\n",
    "        \n",
    "#     listResult[\"accuracy_4\"][cnt] = acc;\n",
    "    \n",
    "#     listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(Nsample[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "#                                      np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(Nsample[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "    # cnt += 1\n",
    "    print(cnt)\n",
    "    print(time.time() - t0)\n",
    "    \n",
    "# listResult[\"total_accuracy\"] = np.sum(listResult[\"ans\"]==correct_ans, axis=1)/len(testset);\n",
    "# torch.save(listResult, \"../data/SequentialTraining/training_performance_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not np.all(ic == np.array([4,0,1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0;\n",
    "\n",
    "ic = np.zeros(4, dtype=\"int\")\n",
    "\n",
    "# loop for only the cut in glitch, noise and bbh as it's not really meaningful to set cut in sg w/o new signals\n",
    "# ic[3] = Ncut-1;\n",
    "# ic[4] = Ncut-1;\n",
    "\n",
    "# listResult = {};\n",
    "# listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)), dtype=\"int\");\n",
    "# listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype)-1), len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype)-1), 2))\n",
    "\n",
    "for ic[0], ic[1], ic[2], ic[3] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "# for ic[0], ic[1], ic[2], ic[3] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "    cnt += 1;\n",
    "    \n",
    "    print(ic)\n",
    "    \n",
    "    if not np.all(ic == np.array([4,0,1,0])):\n",
    "        continue\n",
    "    # elif cnt > 85:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in ['glitch', 'noise', 'bbh', 'sglf', 'sghf']:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_fft_filtered = dataset_wsl_fft_collected\n",
    "    dataset_wsl_filtered = dataset_wsl_collected\n",
    "    \n",
    "    cutID = \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version\n",
    "    \n",
    "    \n",
    "    print('Starting training on ic = {}'.format(''.join(str(ic[j] for j in range(4)))))\n",
    "    \n",
    "        # --- Glitch AE+WSL ---\n",
    "\n",
    "    iPrev = 0\n",
    "    \n",
    "    previousStep = 'glitch';\n",
    "    # modelPrev_L = models['glitch_L']; # previous step AE\n",
    "    # modelPrev_H = models['glitch_H']; # previous step AE\n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    # dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    # dcd_L = modelPrev_L(torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))[1].detach().numpy();\n",
    "    # dcd_H = modelPrev_H(torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))[1].detach().numpy();\n",
    "    # dataset1 = dataset_wsl_fft_filtered[np.logical_and(np.var(dataset_wsl_fft_filtered[:,:101]-dcd_L, axis=1) >= cutList['glitch_L'][ic[0]],\n",
    "    #                                                np.var(dataset_wsl_fft_filtered[:,101:]-dcd_H, axis=1) >= cutList['glitch_H'][ic[1]])]\n",
    "    \n",
    "    # model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    # models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "    # model_L = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_L']\n",
    "    # model_H = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_H']\n",
    "    \n",
    "    model = trainAE_Onedetector(dataset_ae_fft['glitch'][:,:101], cutID, version, 'glitch')\n",
    "    models['glitch_L'] = model\n",
    "    \n",
    "    model = trainAE_Onedetector(dataset_ae_fft['glitch'][:,101:], cutID, version, 'glitch')\n",
    "    models['glitch_H'] = model\n",
    "    \n",
    "    dcd = models['glitch_L'](torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))[1].detach().numpy()\n",
    "    \n",
    "    dataset0 = data_filtered['glitch'][:,:101]\n",
    "    dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered[:,:101]-dcd, axis=1) > cutList['glitch_L'][ic[2]]][:,:101]\n",
    "    \n",
    "    model_L = trainWSC_Onedetector(dataset0, dataset1, cutID, version, 'glitch')\n",
    "    models[previousStep+\"_WSC_L\"] = model_L\n",
    "    \n",
    "    \n",
    "    dcd = models['glitch_H'](torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))[1].detach().numpy()\n",
    "    \n",
    "    dataset0 = data_filtered['glitch'][:,101:]\n",
    "    dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered[:,101:]-dcd, axis=1) > cutList['glitch_H'][ic[3]]][:,101:]\n",
    "    \n",
    "    model_H = trainWSC_Onedetector(dataset0, dataset1, cutID, version, 'glitch')\n",
    "    models[previousStep+\"_WSC_H\"] = model_H\n",
    "\n",
    "    \n",
    "    # filter the data according to previous WSC\n",
    "    for dt in ['glitch', 'noise', 'bbh', 'sglf', 'sghf']:\n",
    "        dcd_L = nn.Sigmoid()(model_L(torch.FloatTensor(data_filtered[dt][:,:101]))).detach().numpy().flatten();\n",
    "        dcd_H = nn.Sigmoid()(model_H(torch.FloatTensor(data_filtered[dt][:,101:]))).detach().numpy().flatten();\n",
    "        data_filtered[dt] = data_filtered[dt][np.logical_and(dcd_L > 0.5, dcd_H > 0.5)]\n",
    "    \n",
    "    dcd_L = nn.Sigmoid()(model_L(torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))).detach().numpy().flatten();\n",
    "    dcd_H = nn.Sigmoid()(model_H(torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))).detach().numpy().flatten();\n",
    "\n",
    "    dataset_wsl_fft_filtered = dataset_wsl_fft_filtered[np.logical_and(dcd_L > 0.5, dcd_H > 0.5)]\n",
    "    dataset_wsl_filtered = dataset_wsl_filtered[np.logical_and(dcd_L > 0.5, dcd_H > 0.5)]\n",
    "\n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "    \n",
    "    # train the current step AE\n",
    "\n",
    "    print('Start noise AE')\n",
    "\n",
    "    print('Start training the noise WSL, dataset 0 size {}'.format(len(data_filtered['noise'])))\n",
    "\n",
    "    currentStep = 'noise';\n",
    "    model = trainAE(data_filtered[currentStep], cutID, version, currentStep);\n",
    "    models[currentStep] = model;   \n",
    "    \n",
    "    print('Noise AE trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "    \n",
    "    # --- Noise AE+WSL ---\n",
    "    # Here's a cut scan for the noise AE\n",
    "\n",
    "    print('Start noise WSL')\n",
    "\n",
    "    iPrev = 1\n",
    "\n",
    "    previousStep = 'noise';\n",
    "    modelPrev = models[previousStep]; # previous step AE\n",
    "    \n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    # dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "    # dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[0]]]\n",
    "    \n",
    "    dcd = modelPrev(torch.FloatTensor(dataset_wsl_fft_filtered))[1].detach().numpy();\n",
    "    dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered-dcd, axis=1) >= cutList[previousStep][ic[0]]]\n",
    "    \n",
    "    print('Start training the noise WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "    # --- Here I just filter the noise sample ---\n",
    "\n",
    "    data_noise_noiselike = data_filtered['noise'][nn.Sigmoid()(model(torch.FloatTensor(data_filtered['noise']))).detach().numpy().flatten() <= 0.5]\n",
    "    signal_noise_like_args = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft_filtered))).detach().numpy().flatten() <= 0.5\n",
    "    \n",
    "    print('Noise WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "\n",
    "    # data_wsl_noiselike = {}\n",
    "\n",
    "    # filter the data according to previous WSC\n",
    "    # for j in range(iPrev, 4):\n",
    "    #     dt = ind2datatype[j];\n",
    "    #     dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "    #     data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "    #     data_noiselike[dt] = data_filtered[dt][dcd<=0.5]\n",
    "\n",
    "    # dcd = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_filtered))).detach().numpy().flatten()\n",
    "    # data_wsl_noiselike = dataset_wsl_collected[dcd <= 0.5].reshape(-1,2,200)\n",
    "    \n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "    # --- BBH CNN+WSL ---\n",
    "    # Here's a cut scan for BBH CNN\n",
    "\n",
    "    print('Start BBHCNN WSL')\n",
    "\n",
    "    models['BBH_CNN'] = torch.load('../Model_cached/CNN_BBH/model_dep_3_chnl_4_btn_20_v4.pt')\n",
    "\n",
    "    # Note that for CNN, we have to use the initial input data (2 * 200 in timeseries)\n",
    "\n",
    "    dataset0 = data_noise_noiselike\n",
    "\n",
    "    dcd = models['BBH_CNN'](torch.FloatTensor(dataset_wsl_filtered[signal_noise_like_args].reshape(-1,2,200))).detach().numpy()\n",
    "    dataset1 = dataset_wsl_fft_filtered[signal_noise_like_args][np.mean(np.var(dataset_wsl_filtered[signal_noise_like_args].reshape(-1,2,200)-dcd, axis = 2), axis = 1) <= cutList['bbh_CNN'][ic[1]]]\n",
    "\n",
    "    print('Start training the BBHCNN WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "    \n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, 'BBH_CNN')\n",
    "    models['BBH_CNN_WSC'] = model\n",
    "\n",
    "    print('BBHCNN WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "    \n",
    "    # --- Finalized WSL ---\n",
    "\n",
    "    print('Start final WSL for cnt = {}'.format(cnt))\n",
    "\n",
    "    dcd = {}\n",
    "\n",
    "    dcd['glitch_WSC_L'] = nn.Sigmoid()(models['glitch_WSC_L'](torch.FloatTensor(dataset_wsl_fft_collected[:,:101]))).detach().numpy().flatten()\n",
    "    dcd['glitch_WSC_H'] = nn.Sigmoid()(models['glitch_WSC_H'](torch.FloatTensor(dataset_wsl_fft_collected[:,101:]))).detach().numpy().flatten()\n",
    "\n",
    "\n",
    "    for dt in ['noise_WSC', 'BBH_CNN_WSC']:\n",
    "        dcd[dt] = nn.Sigmoid()(models[dt](torch.FloatTensor(dataset_wsl_fft_collected))).detach().numpy().flatten()\n",
    "\n",
    "    glitch_pass = np.logical_and(dcd['glitch_WSC_L'] > 0.5, dcd['glitch_WSC_H'] > 0.5)\n",
    "\n",
    "    noise_pass = np.logical_or(dcd['noise_WSC'] > 0.5, np.logical_and(dcd['noise_WSC'] <= 0.5, dcd['BBH_CNN_WSC'] > 0.5))\n",
    "\n",
    "    passed_args = np.logical_and(glitch_pass, noise_pass)\n",
    "\n",
    "    # dataset0 = np.concatenate((dataset_ae_fft['glitch'], dataset_ae_fft['noise']), axis = 0)\n",
    "\n",
    "    dataset0 = data_filtered['noise']\n",
    "\n",
    "    dataset1 = dataset_wsl_fft_collected[passed_args]\n",
    "\n",
    "    print('Start training the final WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, 'Final')\n",
    "\n",
    "    models['Final_WSC'] = model\n",
    "\n",
    "\n",
    "    print('Final WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "\n",
    "    torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    print(models.keys())\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     dcd = {};\n",
    "#     err = {};\n",
    "#     ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "#     for datatype in list_datatype:\n",
    "#         dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "#         err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "        \n",
    "#     not_select = np.array([True]*len(testset));\n",
    "\n",
    "#     for iStep in range(len(list_datatype)):\n",
    "#         datatype = ind2datatype[iStep];\n",
    "#         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "#         ans[ind_pass] = iStep;\n",
    "#         not_select[ind_pass] = False;\n",
    "        \n",
    "#     ans[not_select] = -1;\n",
    "    \n",
    "#     listResult[\"cut\"][cnt] = ic;\n",
    "#     listResult[\"ans\"][cnt] = ans;\n",
    "    \n",
    "#     acc = np.zeros(len(ind2datatype));\n",
    "    \n",
    "#     for i in range(len(ind2datatype)):\n",
    "#         acc[i] = np.sum(np.logical_and(ans==i, correct_ans==i))/Nsample[ind2datatype[i]];\n",
    "        \n",
    "#     listResult[\"accuracy_4\"][cnt] = acc;\n",
    "    \n",
    "#     listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(Nsample[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "#                                      np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(Nsample[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "    # cnt += 1\n",
    "    print(cnt)\n",
    "    print(time.time() - t0)\n",
    "    \n",
    "# listResult[\"total_accuracy\"] = np.sum(listResult[\"ans\"]==correct_ans, axis=1)/len(testset);\n",
    "# torch.save(listResult, \"../data/SequentialTraining/training_performance_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.mean(np.var(dataset_wsl_collected[signal_noise_like_args].reshape(-1,2,200)-dcd, axis = 2), axis = 1) <= cutList['bbh_CNN'][ic[1]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_noise_like_args = np.argwhere(nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_filtered))).detach().numpy().flatten() <= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_noise_like_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_collected[signal_noise_like_args].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the pipeline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft_collected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_wsl.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans = np.hstack(([0]*N_wsl['glitch'], [1]*N_wsl['noise'], [2]*N_wsl['bbh'], [3]*N_wsl['sglf'], [4]*N_wsl['sghf']))\n",
    "correct_ans_withoutsignal = np.hstack(([0]*N_wsl['glitch'], [1]*N_wsl['noise'], [-1]*(N_wsl['bbh']+N_wsl['sglf']+N_wsl['sghf'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans_withoutsignal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datatype_withoutsignal = ['glitch','noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = dataset_wsl_fft_collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult = {};\n",
    "listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(list_datatype_withoutsignal)), dtype=\"int\");\n",
    "listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(list_datatype_withoutsignal)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), 2))\n",
    "cnt = 0\n",
    "ic_withoutsignal = np.zeros(2, dtype = int)\n",
    "\n",
    "listResult[\"FPR\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)),1))\n",
    "\n",
    "\n",
    "for ic_withoutsignal[0], ic_withoutsignal[1] in itertools.product(np.arange(Ncut), np.arange(Ncut)):\n",
    "\n",
    "    \n",
    "    # if cnt < 86:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in sequence:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_filtered = dataset_wsl_fft_collected\n",
    "        \n",
    "#     for iPrev in range(3):\n",
    "#         previousStep = ind2datatype[iPrev];\n",
    "#         modelPrev = models[previousStep]; # previous step AE\n",
    "        \n",
    "#         # train the WSC according to previous AE's cut\n",
    "        \n",
    "#         dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "#         dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset_wsl_filtered))[1].detach().numpy();\n",
    "#         dataset1 = dataset_wsl_filtered[np.var(dataset_wsl_filtered-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         model = trainWSC(dataset0, dataset1, cutID)\n",
    "#         models[previousStep+\"_WSC\"] = model;\n",
    "        \n",
    "#         # filter the data according to previous WSC\n",
    "#         for j in range(iPrev, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "#             data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "        \n",
    "# #         # filter the data\n",
    "# #         for j in range(iPrev+1, 4):\n",
    "# #             dt = ind2datatype[j];\n",
    "# #             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "# #             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "#         # train the current step AE\n",
    "#         currentStep = ind2datatype[iPrev+1];\n",
    "#         model = trainAE(data_filtered[currentStep], cutID);\n",
    "#         models[currentStep] = model;\n",
    "        \n",
    "#     torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version+\".json\")\n",
    "#     print(models.keys())\n",
    "    models = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic_withoutsignal[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    # print(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    dcd = {};\n",
    "    err = {};\n",
    "    ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "    dcd['glitch_L'] = nn.Sigmoid()(models['glitch_WSC_L'](torch.FloatTensor(testset[:,:101]))).detach().numpy().reshape(-1)\n",
    "    dcd['glitch_H'] = nn.Sigmoid()(models['glitch_WSC_H'](torch.FloatTensor(testset[:,101:]))).detach().numpy().reshape(-1)\n",
    "    dcd['noise_WSC'] = nn.Sigmoid()(models[\"noise_WSC\"](torch.FloatTensor(testset))).detach().numpy().reshape(-1)\n",
    "    dcd['Final_WSC'] = nn.Sigmoid()(models[\"Final_WSC\"](torch.FloatTensor(testset))).detach().numpy().reshape(-1)\n",
    "    \n",
    "    not_select = np.array([True]*len(testset));\n",
    "\n",
    "    # for iStep in range(len(list_datatype_withoutsignal)):\n",
    "    #     datatype = ind2datatype[iStep];\n",
    "    #     if datatype == 'sg':\n",
    "    #         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "    #     else:\n",
    "    #         ind_pass = np.logical_and(not_select, dcd[datatype] <= 0.5);\n",
    "    #     ans[ind_pass] = iStep;\n",
    "    #     not_select[ind_pass] = False;\n",
    "        \n",
    "    # Pass glitch first\n",
    "    \n",
    "    datatype = 'glitch'\n",
    "    ind_pass = np.logical_and(not_select, np.logical_or(dcd['glitch_L'] <= 0.5, dcd['glitch_H'] <= 0.5))\n",
    "    ans[ind_pass] = 0;\n",
    "    not_select[ind_pass] = False;\n",
    "    # print(dcd['glitch'])\n",
    "    \n",
    "    # Leftover are noise and signals\n",
    "    \n",
    "    # datatype = 'noise_WSC'\n",
    "    datatype = 'Final_WSC'\n",
    "    ind_pass = np.logical_and(not_select, dcd[datatype] <= np.sort(dcd[datatype][-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:][not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:]])[int(0.1 * (np.sum(not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:])))])\n",
    "    # ind_pass = np.logical_and(not_select, err[datatype] <= np.sort(err[datatype][-N_wsl['bbh']-N_wsl['sg']:])[int(0.1 * (N_wsl['bbh']+N_wsl['sg']))])\n",
    "    noise_number = np.sum(np.logical_and(not_select, correct_ans_withoutsignal == 1))\n",
    "    passed_noise_number = noise_number - np.sum(np.logical_and(ind_pass, correct_ans_withoutsignal == 1))\n",
    "    ans[ind_pass] = 1;\n",
    "    not_select[ind_pass] = False;\n",
    "    \n",
    "    ans[not_select] = -1\n",
    "    \n",
    "    FPR = passed_noise_number / noise_number\n",
    "    \n",
    "    # print(dcd['noise'])\n",
    "    print('For cnt = {}, totally {} noise events passed the glitch WSL, and {} noise events within the threshold for TPR=0.9'.format(cnt, noise_number, passed_noise_number))\n",
    "    listResult['FPR'][cnt] = FPR\n",
    "    listResult['cut'][cnt] = ic_withoutsignal\n",
    "    listResult['ans'][cnt] = ans\n",
    "        \n",
    "    # ans[not_select] = -1;\n",
    "\n",
    "    # listResult[\"cut\"][cnt] = ic_withoutsignal;\n",
    "    # listResult[\"ans\"][cnt] = ans;\n",
    "\n",
    "    # acc = np.zeros(len(ind2datatype));\n",
    "\n",
    "    # for i in range(len(ind2datatype)):\n",
    "    #     acc[i] = np.sum(np.logical_and(ans==i, correct_ans_withoutsignal==i))/N_wsl[ind2datatype[i]];\n",
    "        \n",
    "    # listResult[\"accuracy_4\"][cnt] = acc;\n",
    "\n",
    "    # listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(N_wsl[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "    #                                     np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "\n",
    "    cnt += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult = {};\n",
    "listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(list_datatype_withoutsignal)), dtype=\"int\");\n",
    "listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(list_datatype_withoutsignal)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), 2))\n",
    "cnt = 0\n",
    "ic_withoutsignal = np.zeros(2, dtype = int)\n",
    "\n",
    "listResult[\"FPR\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)),1))\n",
    "\n",
    "\n",
    "for ic_withoutsignal[0], ic_withoutsignal[1] in itertools.product(np.arange(Ncut), np.arange(Ncut)):\n",
    "\n",
    "    \n",
    "    # if cnt < 86:\n",
    "    #     continue\n",
    "    \n",
    "    if not np.all(ic_withoutsignal == np.array([4,0])):\n",
    "        continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in sequence:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_filtered = dataset_wsl_fft_collected\n",
    "        \n",
    "#     for iPrev in range(3):\n",
    "#         previousStep = ind2datatype[iPrev];\n",
    "#         modelPrev = models[previousStep]; # previous step AE\n",
    "        \n",
    "#         # train the WSC according to previous AE's cut\n",
    "        \n",
    "#         dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "#         dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset_wsl_filtered))[1].detach().numpy();\n",
    "#         dataset1 = dataset_wsl_filtered[np.var(dataset_wsl_filtered-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         model = trainWSC(dataset0, dataset1, cutID)\n",
    "#         models[previousStep+\"_WSC\"] = model;\n",
    "        \n",
    "#         # filter the data according to previous WSC\n",
    "#         for j in range(iPrev, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "#             data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "        \n",
    "# #         # filter the data\n",
    "# #         for j in range(iPrev+1, 4):\n",
    "# #             dt = ind2datatype[j];\n",
    "# #             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "# #             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "#         # train the current step AE\n",
    "#         currentStep = ind2datatype[iPrev+1];\n",
    "#         model = trainAE(data_filtered[currentStep], cutID);\n",
    "#         models[currentStep] = model;\n",
    "        \n",
    "#     torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version+\".json\")\n",
    "#     print(models.keys())\n",
    "    models = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic_withoutsignal[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    # print(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    dcd = {};\n",
    "    err = {};\n",
    "    ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "    dcd['glitch_L'] = nn.Sigmoid()(models['glitch_WSC_L'](torch.FloatTensor(testset[:,:101]))).detach().numpy().reshape(-1)\n",
    "    dcd['glitch_H'] = nn.Sigmoid()(models['glitch_WSC_H'](torch.FloatTensor(testset[:,101:]))).detach().numpy().reshape(-1)\n",
    "    dcd['noise_WSC'] = nn.Sigmoid()(models[\"noise_WSC\"](torch.FloatTensor(testset))).detach().numpy().reshape(-1)\n",
    "    dcd['Final_WSC'] = nn.Sigmoid()(models[\"Final_WSC\"](torch.FloatTensor(testset))).detach().numpy().reshape(-1)\n",
    "    \n",
    "    not_select = np.array([True]*len(testset));\n",
    "\n",
    "    # for iStep in range(len(list_datatype_withoutsignal)):\n",
    "    #     datatype = ind2datatype[iStep];\n",
    "    #     if datatype == 'sg':\n",
    "    #         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "    #     else:\n",
    "    #         ind_pass = np.logical_and(not_select, dcd[datatype] <= 0.5);\n",
    "    #     ans[ind_pass] = iStep;\n",
    "    #     not_select[ind_pass] = False;\n",
    "        \n",
    "    # Pass glitch first\n",
    "    \n",
    "    datatype = 'glitch'\n",
    "    ind_pass = np.logical_and(not_select, np.logical_or(dcd['glitch_L'] < 0, dcd['glitch_H'] <= 0.5))\n",
    "    ans[ind_pass] = 0;\n",
    "    not_select[ind_pass] = False;\n",
    "    # print(dcd['glitch'])\n",
    "    \n",
    "    # Leftover are noise and signals\n",
    "    \n",
    "    # datatype = 'noise_WSC'\n",
    "    datatype = 'Final_WSC'\n",
    "    ind_pass = np.logical_and(not_select, dcd[datatype] <= np.sort(dcd[datatype][-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:][not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:]])[int(0.1 * (np.sum(not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:])))])\n",
    "    # ind_pass = np.logical_and(not_select, err[datatype] <= np.sort(err[datatype][-N_wsl['bbh']-N_wsl['sg']:])[int(0.1 * (N_wsl['bbh']+N_wsl['sg']))])\n",
    "    noise_number = np.sum(np.logical_and(not_select, correct_ans_withoutsignal == 1))\n",
    "    passed_noise_number = noise_number - np.sum(np.logical_and(ind_pass, correct_ans_withoutsignal == 1))\n",
    "    ans[ind_pass] = 1;\n",
    "    not_select[ind_pass] = False;\n",
    "    \n",
    "    ans[not_select] = -1\n",
    "    \n",
    "    FPR = passed_noise_number / noise_number\n",
    "    \n",
    "    # print(dcd['noise'])\n",
    "    print('For cnt = {}, totally {} noise events passed the glitch WSL, and {} noise events within the threshold for TPR=0.9'.format(cnt, noise_number, passed_noise_number))\n",
    "    listResult['FPR'][cnt] = FPR\n",
    "    listResult['cut'][cnt] = ic_withoutsignal\n",
    "    listResult['ans'][cnt] = ans\n",
    "        \n",
    "    # ans[not_select] = -1;\n",
    "\n",
    "    # listResult[\"cut\"][cnt] = ic_withoutsignal;\n",
    "    # listResult[\"ans\"][cnt] = ans;\n",
    "\n",
    "    # acc = np.zeros(len(ind2datatype));\n",
    "\n",
    "    # for i in range(len(ind2datatype)):\n",
    "    #     acc[i] = np.sum(np.logical_and(ans==i, correct_ans_withoutsignal==i))/N_wsl[ind2datatype[i]];\n",
    "        \n",
    "    # listResult[\"accuracy_4\"][cnt] = acc;\n",
    "\n",
    "    # listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(N_wsl[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "    #                                     np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "\n",
    "    cnt += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_wsl['noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['ans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "\n",
    "np.sum(listResult['ans'][cnt][-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 20\n",
    "\n",
    "np.sum(listResult['ans'][cnt][-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "\n",
    "np.sum(listResult['ans'][cnt][N_wsl['glitch']:N_wsl['glitch']+N_wsl['noise']] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcd[datatype][-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:][not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(['4','0']) + \"_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['noise_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "# glitch_filtered = dataset_wsl_fft['glitch']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    plt.title(\"Performance of Final WSC\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['BBH_CNN_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "# glitch_filtered = dataset_wsl_fft['glitch']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    plt.title(\"Performance of Final WSC\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v12\n",
    "\n",
    "model = models['Final_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "# glitch_filtered = dataset_wsl_fft['glitch']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    plt.title(\"Performance of Final WSC\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v15\n",
    "\n",
    "model = models['Final_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "# glitch_filtered = dataset_wsl_fft['glitch']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    plt.title(\"Performance of Final WSC\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['glitch_WSC_L']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['glitch_WSC_H']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,101:]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,101:]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC H\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,101:]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14\n",
    "\n",
    "model = models['Final_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "# glitch_filtered = dataset_wsl_fft['glitch']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    plt.title(\"Performance of Final WSC\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14\n",
    "\n",
    "model = models['glitch_WSC_L']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14\n",
    "\n",
    "model = models['glitch_WSC_H']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,101:]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,101:]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC H\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,101:]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutList['glitch_L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14\n",
    "\n",
    "model = trainAE_Onedetector(dataset_ae_fft['glitch'][:,:101], 'test', 'test', 'test')\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,:101]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,:101]))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,:101]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,:101]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "plt.axvline(cutList['glitch_L'][ic[2]], color=\"k\", linestyle=\"--\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14\n",
    "\n",
    "model = models['glitch_L']\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,:101]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,:101]))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,:101]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,:101]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "plt.axvline(cutList['glitch_L'][ic[2]], color=\"k\", linestyle=\"--\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14\n",
    "\n",
    "model = models['glitch_H']\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,101:]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,101:]))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,101:]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,101:]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "plt.axvline(cutList['glitch_H'][ic[3]], color=\"k\", linestyle=\"--\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test training the glitch AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft_filtered = dataset_wsl_fft_collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = trainAE_Onedetector(dataset_ae_fft['glitch'][:,:101], 'test', 'test', 'glitch')\n",
    "# models['glitch_L'] = model\n",
    "\n",
    "# model = trainAE_Onedetector(dataset_ae_fft['glitch'][:,101:], 'test', 'test', 'glitch')\n",
    "# models['glitch_H'] = model\n",
    "\n",
    "dcd = models['glitch_L'](torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))[1].detach().numpy()\n",
    "\n",
    "dataset0 = dataset_ae_fft['glitch'][:,:101]\n",
    "dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered[:,:101]-dcd, axis=1) > cutList['glitch_L'][1]][:,:101]\n",
    "\n",
    "model_L = trainWSC_Onedetector(dataset0, dataset1, 'test', 'test', 'glitch')\n",
    "models[\"glitch_WSC_L\"] = model_L\n",
    "\n",
    "\n",
    "dcd = models['glitch_H'](torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))[1].detach().numpy()\n",
    "\n",
    "dataset0 = dataset_ae_fft['glitch'][:,101:]\n",
    "dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered[:,101:]-dcd, axis=1) > cutList['glitch_H'][0]][:,101:]\n",
    "\n",
    "model_H = trainWSC_Onedetector(dataset0, dataset1, 'test', 'test', 'glitch')\n",
    "models[\"glitch_WSC_H\"] = model_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcd = models['glitch_L'](torch.FloatTensor(dataset_wsl_fft['noise'][:,:101]))[1].detach().numpy()\n",
    "\n",
    "dataset0 = dataset_ae_fft['glitch'][:,:101]\n",
    "dataset1 = dataset_wsl_fft['noise'][np.var(dataset_wsl_fft['noise'][:,:101]-dcd, axis=1) > cutList['glitch_L'][3]][:,:101]\n",
    "\n",
    "model_L = trainWSC_Onedetector(dataset0, dataset1, 'test', 'test', 'glitch')\n",
    "models[\"glitch_WSC_L\"] = model_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_L']\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,:101]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,:101]))[1].detach().numpy()\n",
    "\n",
    "for key in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,:101]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,:101]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "\n",
    "    for snr in snr_range:\n",
    "        key_to_plot = key+snr\n",
    "        dcd_signal = model(torch.FloatTensor(dataset_wsl_fft[key_to_plot][:,:101]))[1].detach().numpy()\n",
    "        plt.hist(np.var(dataset_wsl_fft[key_to_plot][:,:101] - dcd_signal, axis = 1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=key_to_plot)\n",
    "\n",
    "\n",
    "\n",
    "    plt.title(\"trained with Livinston noise\")\n",
    "    plt.axvline(cutList['glitch_L'][0], color=\"k\", linestyle=\"--\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_WSC_L']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_WSC_L']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_WSC_L']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_H']\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,101:]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,101:]))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,101:]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,101:]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "plt.axvline(cutList['glitch_H'][0], color=\"k\", linestyle=\"--\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_H']\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,101:]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,101:]))[1].detach().numpy()\n",
    "\n",
    "for key in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,101:]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,101:]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "\n",
    "    for snr in snr_range:\n",
    "        key_to_plot = key+snr\n",
    "        dcd_signal = model(torch.FloatTensor(dataset_wsl_fft[key_to_plot][:,101:]))[1].detach().numpy()\n",
    "        plt.hist(np.var(dataset_wsl_fft[key_to_plot][:,101:] - dcd_signal, axis = 1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=key_to_plot)\n",
    "\n",
    "\n",
    "\n",
    "    plt.title(\"trained with Hanford noise\")\n",
    "    plt.axvline(cutList['glitch_H'][1], color=\"k\", linestyle=\"--\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "model = models['glitch_WSC_H']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,101:]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,101:]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC H\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,101:]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the Livinston Glitch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick check for roburtness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_wsl_total = 30000;\n",
    "N_wsl = {}\n",
    "N_wsl[\"noise\"] = int(0.65*N_wsl_total)\n",
    "N_wsl[\"bbh\"] = int(0.1*N_wsl_total)\n",
    "N_wsl[\"sglf\"] = int(0.1*N_wsl_total)\n",
    "N_wsl[\"sghf\"] = int(0.1*N_wsl_total)\n",
    "N_wsl[\"glitch\"] = int(0.05*N_wsl_total)\n",
    "\n",
    "\n",
    "snr_range = ['5-12','12-24','24-48','48-96']\n",
    "ratio = [0.25, 0.25, 0.25, 0.25]\n",
    "list_datatype = [\"noise\", \"bbh\", \"sglf\", \"sghf\", \"glitch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datatype_full = ['noise']\n",
    "\n",
    "for snr in snr_range:\n",
    "    list_datatype_full.append('bbh' + snr)\n",
    "    list_datatype_full.append('sglf' + snr)\n",
    "    list_datatype_full.append('sghf' + snr)\n",
    "    \n",
    "list_datatype_full.append('glitch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = ['glitch','noise', \n",
    "            'bbh5-12', 'bbh12-24', 'bbh24-48', 'bbh48-96',\n",
    "            'sglf5-12', 'sglf12-24', 'sglf24-48', 'sglf48-96',\n",
    "            'sghf5-12', 'sghf12-24', 'sghf24-48', 'sghf48-96',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(snr_range) == len(ratio)\n",
    "\n",
    "for i in range(len(ratio)):\n",
    "\n",
    "    N_wsl['bbh' + snr_range[i]] = int(ratio[i] * N_wsl['bbh'])\n",
    "    \n",
    "    N_wsl['sglf' + snr_range[i]] = int(ratio[i] * N_wsl['sglf'])\n",
    "    \n",
    "    N_wsl['sghf' + snr_range[i]] = int(ratio[i] * N_wsl['sghf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Introducing Chia-Jui's data\n",
    "\n",
    "renorm_factor_0 = 20;\n",
    "renorm_factor_1 = 20;\n",
    "\n",
    "realbkg_L = np.load('../Data_cached/real_bkg_2202000_63917s_4000Hz_50ms.npy')[:1000000].reshape(-1,1,200) / renorm_factor_0;\n",
    "realbkg_H = np.load('../Data_cached/real_bkg_H_1466640_58803s_4000Hz_50ms.npy')[:1000000].reshape(-1,1,200) / renorm_factor_0;\n",
    "\n",
    "realbkg = np.concatenate((realbkg_L, realbkg_H), axis = 1).reshape(-1,200)\n",
    "\n",
    "# realbbh = np.load('../Data_cached/injected_BBH_1823_around_merger_time_63917_58803.npz')['strain'].reshape(-1,200) / renorm_factor_0;\n",
    "\n",
    "# realsg = np.load('../Data_cached/injected_lfsg_1835_around_merger_time_63917_58803.npz')['strain'].reshape(-1,200) / renorm_factor_0;\n",
    "# realglitch = np.load(\"../data/real_glitches_9998_4000Hz_25ms.npz\")[\"strain_time_data\"]\n",
    "\n",
    "realbbh_list = {}\n",
    "realsglf_list = {}\n",
    "realsghf_list = {}\n",
    "\n",
    "\n",
    "for snr in snr_range:\n",
    "    realbbh_list[snr] = np.load('../Data_cached/injected_BBH_55k_snr{}_0th_events_before_merger_time_windowlength_200.npz'.format(snr))['strain'][:25000].reshape(-1,200)\n",
    "    realsglf_list[snr] = np.load('../Data_cached/injected_SGLF_55k_snr{}_0th_events_before_merger_time_windowlength_200.npz'.format(snr))['strain'][:25000].reshape(-1,200)\n",
    "    realsghf_list[snr] = np.load('../Data_cached/injected_SGHF_55k_snr{}_0th_events_before_merger_time_windowlength_200.npz'.format(snr))['strain'][:25000].reshape(-1,200)\n",
    "\n",
    "    \n",
    "\n",
    "realglitch_L = np.load(\"../Data_cached/real_glitches_L_snrlt5_60132_4000Hz_50ms_centered.npz\")[\"strain_time_data\"][:50000].reshape(-1,1,200) / renorm_factor_1\n",
    "realglitch_H = np.load('../Data_cached/real_glitches_H_snrlt5_59732_4000Hz_25ms.npz')[\"strain_time_data\"][:50000].reshape(-1,1,200) / renorm_factor_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novel procedure. For training sample, the model is purely glitch trained. For testing and WSL sample, the model is one glitch + one noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_glitch_for_AE = int(len(realglitch_L) * 0.8) - N_wsl['glitch'] // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_glitch_one_noise = int(len(realglitch_L) * 0.2) + N_wsl['glitch'] // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(realglitch_L)\n",
    "np.random.shuffle(realglitch_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realglitch_L.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realbkg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glitch_for_ae = np.concatenate((realglitch_L[:num_glitch_for_AE], realglitch_H[:num_glitch_for_AE]), axis=1).reshape(-1,200)\n",
    "glitch_for_ae_fft = abs(np.fft.rfft(glitch_for_ae))\n",
    "glitch_for_ae_fft = glitch_for_ae_fft/np.linalg.norm([glitch_for_ae_fft], axis=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glitch_for_ae = glitch_for_ae / np.linalg.norm([glitch_for_ae], axis = 2).T\n",
    "\n",
    "glitch_for_ae = glitch_for_ae.reshape(-1,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_for_glitch_building = realbkg[:2 * num_one_glitch_one_noise].reshape(-1,2,200)\n",
    "\n",
    "# glitch_L_noise_H = np.concatenate((realglitch_L[-num_one_glitch_one_noise:], noise_for_glitch_building[:,[1],:]), axis = 1)\n",
    "noise_L_glitch_H = np.concatenate((noise_for_glitch_building[:,[0],:], realglitch_H[-num_one_glitch_one_noise:]), axis = 1)\n",
    "one_glitch_one_noise = noise_L_glitch_H\n",
    "np.random.shuffle(one_glitch_one_noise)\n",
    "one_glitch_one_noise = one_glitch_one_noise.reshape(-1,200)\n",
    "\n",
    "realbkg = realbkg[2 * num_one_glitch_one_noise:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_glitch_one_noise_fft = abs(np.fft.rfft(one_glitch_one_noise))\n",
    "one_glitch_one_noise_fft = one_glitch_one_noise_fft / np.linalg.norm([one_glitch_one_noise_fft], axis=2).T\n",
    "\n",
    "one_glitch_one_noise = one_glitch_one_noise / np.linalg.norm([one_glitch_one_noise], axis = 2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_glitch_one_noise_fft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realbkg_reserved = realbkg[-int(0.2 * len(realbkg)):]\n",
    "# realbbh_reserved = realbbh[-int(0.2 * len(realbbh)):]\n",
    "# realsg_reserved = realsg[-int(0.2 * len(realsg)):]\n",
    "# realglitch_reserved = realglitch[-int(0.2 * len(realglitch)):]\n",
    "\n",
    "realbkg = realbkg[:-int(0.2 * len(realbkg))]\n",
    "# realbbh = realbbh[:-int(0.2 * len(realbbh))-int(0.2 * len(realbbh))%2]\n",
    "# realsg = realsg[:-int(0.2 * len(realsg))-int(0.2 * len(realsg))%2]\n",
    "# realglitch = realglitch[:-int(0.2 * len(realglitch))]\n",
    "\n",
    "realbkg = realbkg / np.linalg.norm([realbkg], axis = 2).T\n",
    "\n",
    "bkg_fft = abs(np.fft.rfft(realbkg))\n",
    "bkg_fft = bkg_fft/np.linalg.norm([bkg_fft], axis=2).T\n",
    "\n",
    "# bbh_fft = abs(np.fft.rfft(realbbh))\n",
    "# bbh_fft = bbh_fft/np.linalg.norm([bbh_fft], axis=2).T\n",
    "\n",
    "# sg_fft = abs(np.fft.rfft(realsg))\n",
    "# sg_fft = sg_fft/np.linalg.norm([sg_fft], axis=2).T\n",
    "\n",
    "# glitch_fft = abs(np.fft.rfft(realglitch))\n",
    "# glitch_fft = glitch_fft/np.linalg.norm([glitch_fft], axis=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realbbh_reserved_list = {}\n",
    "realsglf_reserved_list = {}\n",
    "realsghf_reserved_list = {}\n",
    "\n",
    "realbbh_fft_list = {}\n",
    "realsglf_fft_list = {}\n",
    "realsghf_fft_list = {}\n",
    "\n",
    "\n",
    "for snr in snr_range:\n",
    "    realbbh_reserved_list[snr] = realbbh_list[snr][-int(0.2 * len(realbbh_list[snr]))-int(0.2 * len(realbbh_list[snr]))%2:]\n",
    "    realbbh_list[snr] = realbbh_list[snr][:-int(0.2 * len(realbbh_list[snr]))-int(0.2 * len(realbbh_list[snr]))%2]\n",
    "\n",
    "    realbbh_fft_list[snr] = abs(np.fft.rfft(realbbh_list[snr]))\n",
    "    realbbh_fft_list[snr] = realbbh_fft_list[snr]/np.linalg.norm([realbbh_fft_list[snr]], axis=2).T\n",
    "    \n",
    "    realbbh_list[snr] = realbbh_list[snr]/np.linalg.norm([realbbh_list[snr]], axis=2).T\n",
    "    \n",
    "    \n",
    "    realsglf_reserved_list[snr] = realsglf_list[snr][-int(0.2 * len(realsglf_list[snr]))-int(0.2 * len(realsglf_list[snr]))%2:]\n",
    "    realsglf_list[snr] = realsglf_list[snr][:-int(0.2 * len(realsglf_list[snr]))-int(0.2 * len(realsglf_list[snr]))%2]\n",
    "\n",
    "    realsglf_fft_list[snr] = abs(np.fft.rfft(realsglf_list[snr]))\n",
    "    realsglf_fft_list[snr] = realsglf_fft_list[snr]/np.linalg.norm([realsglf_fft_list[snr]], axis=2).T\n",
    "    \n",
    "    realsglf_list[snr] = realsglf_list[snr]/np.linalg.norm([realsglf_list[snr]], axis=2).T\n",
    "    \n",
    "    \n",
    "    realsghf_reserved_list[snr] = realsghf_list[snr][-int(0.2 * len(realsghf_list[snr]))-int(0.2 * len(realsghf_list[snr]))%2:]\n",
    "    realsghf_list[snr] = realsghf_list[snr][:-int(0.2 * len(realsghf_list[snr]))-int(0.2 * len(realsghf_list[snr]))%2]\n",
    "\n",
    "    realsghf_fft_list[snr] = abs(np.fft.rfft(realsghf_list[snr]))\n",
    "    realsghf_fft_list[snr] = realsghf_fft_list[snr]/np.linalg.norm([realsghf_fft_list[snr]], axis=2).T\n",
    "    \n",
    "    realsghf_list[snr] = realsghf_list[snr]/np.linalg.norm([realsghf_list[snr]], axis=2).T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_fft = bkg_fft.reshape(-1,202)\n",
    "\n",
    "# bbh_fft = bbh_fft.reshape(-1,202)\n",
    "\n",
    "# sg_fft = sg_fft.reshape(-1,202)\n",
    "\n",
    "# glitch_fft = glitch_fft.reshape(-1,202)\n",
    "\n",
    "glitch_for_ae_fft = glitch_for_ae_fft.reshape(-1,202)\n",
    "\n",
    "one_glitch_one_noise_fft = one_glitch_one_noise_fft.reshape(-1,202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = {}\n",
    "dataset_raw_fft = {}\n",
    "\n",
    "dataset_raw[\"noise\"] = realbkg.reshape(-1,400)\n",
    "# dataset_raw_fft[\"bbh\"] = bbh_fft\n",
    "# dataset_raw_fft[\"sg\"] = sg_fft\n",
    "for snr in snr_range:\n",
    "    dataset_raw['bbh' + snr] = realbbh_list[snr].reshape(-1,400)\n",
    "    dataset_raw['sglf' + snr] = realsglf_list[snr].reshape(-1,400)\n",
    "    dataset_raw['sghf' + snr] = realsghf_list[snr].reshape(-1,400)\n",
    "# dataset_raw[\"glitch\"] = glitch.reshape(-1,400)\n",
    "\n",
    "\n",
    "dataset_raw_fft[\"noise\"] = bkg_fft\n",
    "# dataset_raw_fft[\"bbh\"] = bbh_fft\n",
    "# dataset_raw_fft[\"sg\"] = sg_fft\n",
    "for snr in snr_range:\n",
    "    dataset_raw_fft['bbh' + snr] = realbbh_fft_list[snr].reshape(-1,202)\n",
    "    dataset_raw_fft['sglf' + snr] = realsglf_fft_list[snr].reshape(-1,202)\n",
    "    dataset_raw_fft['sghf' + snr] = realsghf_fft_list[snr].reshape(-1,202)\n",
    "# dataset_raw_fft[\"glitch\"] = glitch_fft\n",
    "\n",
    "dataset_wsl = {};\n",
    "dataset_ae = {};\n",
    "dataset_wsl_fft = {};\n",
    "dataset_ae_fft = {};\n",
    "\n",
    "for dt in list_datatype_full[:-1]:\n",
    "    perm = np.random.permutation(len(dataset_raw_fft[dt]))\n",
    "    # perm = np.loadtxt(\"../Data_Cached/SequentialTraining/WSL/perm_\"+dt+\"_2det_Chia-Jui_v7_GWAK.dat\").astype(int)\n",
    "    \n",
    "    nwsl = N_wsl[dt]\n",
    "    dataset_wsl[dt] = dataset_raw[dt][perm[:nwsl]]\n",
    "    dataset_wsl_fft[dt] = dataset_raw_fft[dt][perm[:nwsl]]\n",
    "    # dataset_wsl[dt] = dataset_wsl[dt] / np.linalg.norm([dataset_wsl[dt]], axis=2).T\n",
    "    # dataset_wsl_fft[dt] = abs(np.fft.rfft(dataset_wsl[dt]))\n",
    "    # dataset_wsl_fft[dt] = dataset_wsl_fft[dt]/np.linalg.norm([dataset_wsl_fft[dt]], axis=2).T\n",
    "    \n",
    "    dataset_ae[dt] = dataset_raw[dt][perm[nwsl:]]\n",
    "    dataset_ae_fft[dt]  = dataset_raw_fft[dt][perm[nwsl:]]\n",
    "    # dataset_ae[dt] = dataset_ae[dt] / np.linalg.norm([dataset_ae[dt]], axis=2).T\n",
    "    # dataset_ae_fft[dt] = abs(np.fft.rfft(dataset_ae[dt]))\n",
    "    # dataset_ae_fft[dt] = dataset_ae_fft[dt]/np.linalg.norm([dataset_ae_fft[dt]], axis=2).T\n",
    "    \n",
    "    # np.savetxt(\"../Data_Cached/SequentialTraining/WSL/perm_\"+dt+\"_2det_Chia-Jui_\"+version+\"_2.dat\", perm)\n",
    "    \n",
    "dataset_ae['bbh'] = np.zeros((0,400))\n",
    "dataset_ae['sglf'] = np.zeros((0,400))\n",
    "dataset_ae['sghf'] = np.zeros((0,400))\n",
    "\n",
    "dataset_ae_fft['bbh'] = np.zeros((0,202))\n",
    "dataset_ae_fft['sglf'] = np.zeros((0,202))\n",
    "dataset_ae_fft['sghf'] = np.zeros((0,202))\n",
    "\n",
    "for snr in snr_range:\n",
    "    dataset_ae['bbh'] = np.append(dataset_ae['bbh'], dataset_ae['bbh'+snr])\n",
    "    dataset_ae_fft['bbh'] = np.append(dataset_ae_fft['bbh'], dataset_ae_fft['bbh'+snr])\n",
    "    \n",
    "    dataset_ae['sglf'] = np.append(dataset_ae['sglf'], dataset_ae['sglf'+snr])\n",
    "    dataset_ae_fft['sglf'] = np.append(dataset_ae_fft['sglf'], dataset_ae_fft['sglf'+snr])\n",
    "    \n",
    "    dataset_ae['sghf'] = np.append(dataset_ae['sghf'], dataset_ae['sghf'+snr])\n",
    "    dataset_ae_fft['sghf'] = np.append(dataset_ae_fft['sghf'], dataset_ae_fft['sghf'+snr])\n",
    "\n",
    "dataset_ae['bbh'] = dataset_ae['bbh'].reshape(-1,400)\n",
    "dataset_ae['sglf'] = dataset_ae['sglf'].reshape(-1,400)\n",
    "dataset_ae['sghf'] = dataset_ae['sghf'].reshape(-1,400)\n",
    "\n",
    "dataset_ae_fft['bbh'] = dataset_ae_fft['bbh'].reshape(-1,202)\n",
    "dataset_ae_fft['sglf'] = dataset_ae_fft['sglf'].reshape(-1,202)\n",
    "dataset_ae_fft['sghf'] = dataset_ae_fft['sghf'].reshape(-1,202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw[dt].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl[dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_raw_fft[dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft['glitch'] = one_glitch_one_noise_fft[:N_wsl['glitch']]\n",
    "dataset_wsl['glitch'] = one_glitch_one_noise[:2 * N_wsl['glitch']].reshape(-1,400)\n",
    "\n",
    "dataset_ae_fft['glitch'] = glitch_for_ae_fft\n",
    "\n",
    "# Missing dataset ae ['glitch'] here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ae['glitch'] = glitch_for_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_wsl.keys():\n",
    "\n",
    "    print(np.linalg.norm(dataset_wsl[key][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_ae.keys():\n",
    "\n",
    "    print(np.linalg.norm(dataset_wsl[key][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_wsl_fft.keys():\n",
    "\n",
    "    print(np.linalg.norm(dataset_wsl[key][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ae_fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_ae.keys():\n",
    "    print(key)\n",
    "    print(dataset_ae[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_ae_fft.keys():\n",
    "    print(key)\n",
    "    print(dataset_ae_fft[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_ae_fft.keys():\n",
    "    print(key)\n",
    "    print(dataset_ae_fft[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_wsl.keys():\n",
    "    print(key)\n",
    "    print(dataset_wsl[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_wsl_fft.keys():\n",
    "    print(key)\n",
    "    print(dataset_wsl_fft[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft_collected = np.empty((0, 202))\n",
    "for dt in sequence:\n",
    "    dataset_wsl_fft_collected = np.vstack((dataset_wsl_fft_collected, dataset_wsl_fft[dt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_collected = np.empty((0, 400))\n",
    "for dt in sequence:\n",
    "    dataset_wsl_collected = np.vstack((dataset_wsl_collected, dataset_wsl[dt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft_collected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_collected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(202, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(10, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 202),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def count_trainable_params(model):\n",
    "    \"\"\"\n",
    "    计算给定PyTorch模型的可训练参数数量。\n",
    "    \n",
    "    参数:\n",
    "    model (nn.Module) - 要计算参数的PyTorch模型\n",
    "    \n",
    "    返回:\n",
    "    int - 模型的可训练参数数量\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    return trainable_params\n",
    "\n",
    "\n",
    "model = AutoEncoder()\n",
    "trainable_params = count_trainable_params(model)\n",
    "print(f\"Model has {trainable_params} trainable parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params_AE = count_trainable_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder_1det(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder_1det, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(101, 20),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(20, 101),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSClassifier_Onedetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WSClassifier_Onedetector, self).__init__()\n",
    "        self.fc1 = nn.Linear(101, 32)  # 第一层全连接层，输入维度为4，输出维度为64\n",
    "        self.norm1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()  # 激活函数\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        # self.norm2 = nn.BatchNorm1d(8)\n",
    "        # self.fc4 = nn.Linear(8, 1)  # 第三层全连接层，输入维度为32，输出维度为类别数目\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        # nn.init.kaiming_normal_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(self.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "        # x = self.relu(x)\n",
    "#         x = self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WSClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(202, 32)  # 第一层全连接层，输入维度为4，输出维度为64\n",
    "        self.norm1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()  # 激活函数\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        # self.norm2 = nn.BatchNorm1d(8)\n",
    "        # self.fc4 = nn.Linear(8, 1)  # 第三层全连接层，输入维度为32，输出维度为类别数目\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        # nn.init.kaiming_normal_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(self.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "        # x = self.relu(x)\n",
    "#         x = self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def count_trainable_params(model):\n",
    "    \"\"\"\n",
    "    计算给定PyTorch模型的可训练参数数量。\n",
    "    \n",
    "    参数:\n",
    "    model (nn.Module) - 要计算参数的PyTorch模型\n",
    "    \n",
    "    返回:\n",
    "    int - 模型的可训练参数数量\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    return trainable_params\n",
    "\n",
    "\n",
    "model = WSClassifier()\n",
    "trainable_params = count_trainable_params(model)\n",
    "print(f\"Model has {trainable_params} trainable parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params_WSC = count_trainable_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveletCNNAE_xc(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        num_ifos: int,\n",
    "        c_depth: int=8, \n",
    "        n_chann: int=64, \n",
    "        l1: int=1024\n",
    "        # lx: int=200\n",
    "    ):\n",
    "        \n",
    "        super(WaveletCNNAE_xc, self).__init__()\n",
    "        \n",
    "        self.c_depth = c_depth\n",
    "        self.n_chann = n_chann\n",
    "        \n",
    "        self.cap_norm = nn.GroupNorm(num_ifos, num_ifos)\n",
    "        \n",
    "        self.Conv_In_encode = nn.Conv1d(\n",
    "                in_channels=num_ifos, \n",
    "                out_channels=self.n_chann, \n",
    "                kernel_size=1\n",
    "            )\n",
    "        \n",
    "        self.Conv_Out_encode = nn.Conv1d(\n",
    "                in_channels=self.n_chann, \n",
    "                out_channels=1, \n",
    "                kernel_size=1\n",
    "            )\n",
    "        \n",
    "        self.Conv_In_decode = nn.Conv1d(\n",
    "                in_channels=1, \n",
    "                out_channels=self.n_chann, \n",
    "                kernel_size=1\n",
    "            )\n",
    "        \n",
    "        self.Conv_Out_decode = nn.Conv1d(\n",
    "                in_channels=self.n_chann, \n",
    "                out_channels=num_ifos, \n",
    "                kernel_size=1\n",
    "            )\n",
    "        \n",
    "        self.body_norm_encode = nn.GroupNorm(4 ,n_chann)\n",
    "        self.body_norm_decode = nn.GroupNorm(4 ,n_chann)\n",
    "        self.end_norm_encode = nn.BatchNorm1d(1)\n",
    "        self.end_norm_decode = nn.BatchNorm1d(1)\n",
    "        \n",
    "        self.WaveNet_layers_encode = nn.ModuleList()\n",
    "        self.WaveNet_layers_decode = nn.ModuleList()\n",
    "        self.WaveNet_layers_dp = nn.ModuleList()\n",
    "        \n",
    "        \n",
    "        for i in range(self.c_depth):\n",
    "\n",
    "            conv_layer = nn.Conv1d(\n",
    "                in_channels=self.n_chann, \n",
    "                out_channels=self.n_chann,\n",
    "                kernel_size=2,\n",
    "                dilation=2**i\n",
    "            )\n",
    "            \n",
    "            self.WaveNet_layers_encode.append(conv_layer)\n",
    "            \n",
    "        for i in range(self.c_depth-1, -1, -1):\n",
    "\n",
    "            conv_layer = nn.Conv1d(\n",
    "                in_channels=self.n_chann, \n",
    "                out_channels=self.n_chann,\n",
    "                kernel_size=2,\n",
    "                dilation=2**i\n",
    "            )\n",
    "            \n",
    "            self.WaveNet_layers_decode.append(conv_layer)\n",
    "            self.WaveNet_layers_dp.append(nn.ZeroPad1d(2**i))\n",
    "        \n",
    "        \n",
    "#         self.Padding_layer = nn.ZeroPad1d(2**c_depth - 1)\n",
    "                \n",
    "        # self.L1 = nn.Linear(8192-2**c_depth, l1)\n",
    "        \n",
    "        # Consider replacing other batch normalizatoin layers with other nor method\n",
    "        # Because batch norm are baised by the population of the CCSN rate in one batch \n",
    "        # This may produce overfitting model and will not be able to found at test phase\n",
    "        # Question: Will we be able to figure out the side effect at infereceing phase?\n",
    "                \n",
    "#         self.conv_norm = nn.BatchNorm1d(200-2**c_depth + 1)\n",
    "        self.L1 = nn.Linear(200-2**c_depth + 1, l1)\n",
    "        self.L1_norm = nn.BatchNorm1d(l1)\n",
    "        self.L2 = nn.Linear(l1, 200-2**c_depth + 1)\n",
    "        self.L2_norm = nn.BatchNorm1d(200-2**c_depth + 1)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.Conv_In_encode.weight)\n",
    "        nn.init.kaiming_normal_(self.Conv_Out_encode.weight)\n",
    "        nn.init.constant_(self.Conv_In_encode.bias, 0.001)\n",
    "        nn.init.constant_(self.Conv_Out_encode.bias, 0.001)\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.Conv_In_decode.weight)\n",
    "        nn.init.kaiming_normal_(self.Conv_Out_decode.weight)\n",
    "        nn.init.constant_(self.Conv_In_decode.bias, 0.001)\n",
    "        nn.init.constant_(self.Conv_Out_decode.bias, 0.001)\n",
    "\n",
    "        # Initialize all the convolutional layer in between\n",
    "        for conv_layer in self.WaveNet_layers_encode:\n",
    "            nn.init.kaiming_normal_(conv_layer.weight)\n",
    "            nn.init.constant_(conv_layer.bias, 0.001)\n",
    "            \n",
    "        for conv_layer in self.WaveNet_layers_decode:\n",
    "            nn.init.kaiming_normal_(conv_layer.weight)\n",
    "            nn.init.constant_(conv_layer.bias, 0.001)    \n",
    "\n",
    "        nn.init.kaiming_uniform_(self.L1.weight)\n",
    "        nn.init.kaiming_uniform_(self.L2.weight)\n",
    "        nn.init.constant_(self.L1.bias, 0.001)\n",
    "        nn.init.constant_(self.L2.bias, 0.001)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \n",
    "        x = self.cap_norm(x)\n",
    "        x = self.Conv_In_encode(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # x = self.norm(x)\n",
    "        \n",
    "        for what_are_u_wavin_at in self.WaveNet_layers_encode:\n",
    "            x = self.body_norm_encode(x)\n",
    "            x = what_are_u_wavin_at(x)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "        x = self.Conv_Out_encode(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.end_norm_encode(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.L1_norm(F.relu(self.L1(x)))\n",
    "        \n",
    "        # print('Encoder done')\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = self.L2_norm(F.relu(self.L2(x)))\n",
    "        \n",
    "#         x = self.Padding_layer(x)\n",
    "        \n",
    "        x = torch.unsqueeze(x,1)\n",
    "\n",
    "        # print(x.shape)\n",
    "        \n",
    "        # x = self.cap_norm(x)\n",
    "        x = self.Conv_In_decode(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # x = self.norm(x)\n",
    "        \n",
    "        for (pad, dcd) in zip(self.WaveNet_layers_dp, self.WaveNet_layers_decode):\n",
    "            # print(x.shape)\n",
    "            x = self.body_norm_decode(x)\n",
    "            x = pad(x)\n",
    "            x = torch.flip(dcd(torch.flip(x, [-1])), [-1])\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        # print('CNN done')\n",
    "        \n",
    "        x = self.Conv_Out_decode(x)\n",
    "        # print(x.shape)\n",
    "        x = F.tanh(x)\n",
    "        # print(x.shape)\n",
    "        # x = self.end_norm_decode(x)\n",
    "        \n",
    "        # x = torch.flatten(x, 1)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.decode(self.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAE(dataset0_to_be_examined, cutID, version, datatype):\n",
    "    \n",
    "    if len(dataset0_to_be_examined) > 10 * trainable_params_AE:\n",
    "        dataset = dataset0_to_be_examined[np.random.choice(len(dataset0_to_be_examined), 10 * trainable_params_AE, replace = False)]\n",
    "    else:\n",
    "        dataset = dataset0_to_be_examined\n",
    "\n",
    "    print('{} events passed to AE for training. '.format(len(dataset)))\n",
    "\n",
    "    nTotal = len(dataset);\n",
    "    nTrain = int(rTrain * nTotal)\n",
    "    nTest = int(rTest * nTotal)\n",
    "\n",
    "    X_train = dataset[:nTrain]\n",
    "    X_test = dataset[-nTest:]\n",
    "    X_validation = dataset[nTrain:-nTest]\n",
    "\n",
    "    trainData = torch.FloatTensor(X_train)\n",
    "    testData = torch.FloatTensor(X_test)\n",
    "    validationData = torch.FloatTensor(X_validation)\n",
    "\n",
    "    train_dataset = TensorDataset(trainData)\n",
    "    test_dataset = TensorDataset(testData)\n",
    "    validation_dataset = TensorDataset(validationData)\n",
    "\n",
    "    trainDataLoader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validationDataLoader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "    autoencoder = AutoEncoder().cuda()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.00005)\n",
    "    loss_func = nn.MSELoss().cuda()\n",
    "    \n",
    "    loss_train = np.empty(epochs)\n",
    "    loss_validation = np.empty(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        autoencoder.train()\n",
    "        for batchidx, x in enumerate(trainDataLoader):\n",
    "            x = x[0].cuda()\n",
    "            encoded, decoded = autoencoder(x)\n",
    "            loss_overall = loss_func(decoded, x)\n",
    "            weighted_lossTrain = loss_overall\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            weighted_lossTrain.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batchidx, x in enumerate(validationDataLoader):\n",
    "                x = x[0].cuda()\n",
    "                encoded, decoded = autoencoder(x)\n",
    "                lossVal = loss_func(decoded, x)\n",
    "                val_loss += lossVal.item()\n",
    "\n",
    "            val_loss /= len(validationDataLoader)\n",
    "\n",
    "        loss_train[epoch] = weighted_lossTrain.item()\n",
    "        loss_validation[epoch] = val_loss\n",
    "    \n",
    "    autoencoder.cpu().eval()\n",
    "    _, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax[0].plot(loss_train)\n",
    "    ax[0].plot(loss_validation)\n",
    "    \n",
    "    dcd_train = autoencoder(torch.FloatTensor(X_train))[1].detach().numpy()\n",
    "    err_train = np.var(X_train-dcd_train, axis=1)\n",
    "    dcd_test = autoencoder(torch.FloatTensor(X_test))[1].detach().numpy()\n",
    "    err_test = np.var(X_test-dcd_test, axis=1)\n",
    "    foo = ax[1].hist(err_train, range=(0, max(err_train)), bins=50, density=True, histtype=\"step\")\n",
    "    foo = ax[1].hist(err_test, range=(0, max(err_train)), bins=50, density=True, histtype=\"step\")\n",
    "    \n",
    "    plt.savefig(\"../Pic_cached/SequentialTraining/WSL/training_AE_\"+cutID+\"_\" + version + \"_\" + datatype +\"_trained.jpg\")\n",
    "    plt.close()\n",
    "            \n",
    "    return autoencoder.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAE_Onedetector(dataset0_to_be_examined, cutID, version, datatype):\n",
    "    \n",
    "    if len(dataset0_to_be_examined) > 10 * trainable_params_AE:\n",
    "        dataset = dataset0_to_be_examined[np.random.choice(len(dataset0_to_be_examined), 10 * trainable_params_AE, replace = False)]\n",
    "    else:\n",
    "        dataset = dataset0_to_be_examined\n",
    "\n",
    "    print('{} events passed to AE for training. '.format(len(dataset)))\n",
    "\n",
    "    nTotal = len(dataset);\n",
    "    nTrain = int(rTrain * nTotal)\n",
    "    nTest = int(rTest * nTotal)\n",
    "\n",
    "    X_train = dataset[:nTrain]\n",
    "    X_test = dataset[-nTest:]\n",
    "    X_validation = dataset[nTrain:-nTest]\n",
    "\n",
    "    trainData = torch.FloatTensor(X_train)\n",
    "    testData = torch.FloatTensor(X_test)\n",
    "    validationData = torch.FloatTensor(X_validation)\n",
    "\n",
    "    train_dataset = TensorDataset(trainData)\n",
    "    test_dataset = TensorDataset(testData)\n",
    "    validation_dataset = TensorDataset(validationData)\n",
    "\n",
    "    trainDataLoader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validationDataLoader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "    autoencoder = AutoEncoder_1det().cuda()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.00005)\n",
    "    loss_func = nn.MSELoss().cuda()\n",
    "    \n",
    "    loss_train = np.empty(epochs)\n",
    "    loss_validation = np.empty(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        autoencoder.train()\n",
    "        for batchidx, x in enumerate(trainDataLoader):\n",
    "            x = x[0].cuda()\n",
    "            encoded, decoded = autoencoder(x)\n",
    "            loss_overall = loss_func(decoded, x)\n",
    "            weighted_lossTrain = loss_overall\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            weighted_lossTrain.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batchidx, x in enumerate(validationDataLoader):\n",
    "                x = x[0].cuda()\n",
    "                encoded, decoded = autoencoder(x)\n",
    "                lossVal = loss_func(decoded, x)\n",
    "                val_loss += lossVal.item()\n",
    "\n",
    "            val_loss /= len(validationDataLoader)\n",
    "\n",
    "        loss_train[epoch] = weighted_lossTrain.item()\n",
    "        loss_validation[epoch] = val_loss\n",
    "    \n",
    "    autoencoder.cpu().eval()\n",
    "    _, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax[0].plot(loss_train)\n",
    "    ax[0].plot(loss_validation)\n",
    "    \n",
    "    dcd_train = autoencoder(torch.FloatTensor(X_train))[1].detach().numpy()\n",
    "    err_train = np.var(X_train-dcd_train, axis=1)\n",
    "    dcd_test = autoencoder(torch.FloatTensor(X_test))[1].detach().numpy()\n",
    "    err_test = np.var(X_test-dcd_test, axis=1)\n",
    "    foo = ax[1].hist(err_train, range=(0, max(err_train)), bins=50, density=True, histtype=\"step\")\n",
    "    foo = ax[1].hist(err_test, range=(0, max(err_train)), bins=50, density=True, histtype=\"step\")\n",
    "    \n",
    "    plt.savefig(\"../Pic_cached/SequentialTraining/WSL/training_AE_\"+cutID+\"_\" + version + \"_\" + datatype +\"_trained.jpg\")\n",
    "    plt.close()\n",
    "            \n",
    "    return autoencoder.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWSC(dataset0_to_be_examined, dataset1, cutID, version, datatype):\n",
    "# dataset0: bkg set from AE\n",
    "# dataset1: identified signal from AE\n",
    "\n",
    "    if len(dataset0_to_be_examined) > 10 * trainable_params_WSC:\n",
    "        dataset0 = dataset0_to_be_examined[np.random.choice(len(dataset0_to_be_examined), 10 * trainable_params_WSC, replace = False)]\n",
    "    else:\n",
    "        dataset0 = dataset0_to_be_examined\n",
    "    \n",
    "    print('{} noise events and {} signal events passed to WSC for training. '.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    nTotal0, nTotal1 = len(dataset0), len(dataset1);\n",
    "    nTrain0, nTrain1 = int(rTrain * nTotal0), int(rTrain * nTotal1)\n",
    "    nTest0 , nTest1  = int(rTest * nTotal0) , int(rTest * nTotal1)\n",
    "\n",
    "    X_train = np.concatenate((dataset0[:nTrain0], dataset1[:nTrain1]))\n",
    "    X_test = np.concatenate((dataset0[-nTest0:], dataset1[-nTest1:]))\n",
    "    X_validation = np.concatenate((dataset0[nTrain0:-nTest0], dataset1[nTrain1:-nTest1]))\n",
    "    \n",
    "    Y_train = np.concatenate((np.zeros((nTrain0, 1)), np.ones((nTrain1, 1))))\n",
    "    Y_test = np.concatenate((np.zeros((nTest0, 1)), np.ones((nTest1, 1))))\n",
    "    Y_validation = np.concatenate((np.zeros((dataset0[nTrain0:-nTest0].shape[0], 1)), np.ones((dataset1[nTrain1:-nTest1].shape[0], 1))))\n",
    "\n",
    "#     trainData = torch.FloatTensor(X_train)\n",
    "#     testData = torch.FloatTensor(X_test)\n",
    "#     validationData = torch.FloatTensor(X_validation)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(Y_train))\n",
    "    validation_dataset = TensorDataset(torch.FloatTensor(X_validation), torch.FloatTensor(Y_validation))\n",
    "#     train_dataset = TensorDataset(torch.FloatTensor(X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))), torch.FloatTensor(Y_train.reshape((Y_train.shape[0], 1, Y_train.shape[1]))))\n",
    "#     validation_dataset = TensorDataset(torch.FloatTensor(X_validation.reshape((X_validation.shape[0], 1, X_validation.shape[1]))), torch.FloatTensor(Y_validation.reshape((Y_validation.shape[0], 1, Y_validation.shape[1]))))\n",
    "\n",
    "    trainDataLoader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    validationDataLoader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle = True, drop_last=True)\n",
    "\n",
    "    wsc = WSClassifier().cuda()\n",
    "    optimizer = optim.Adam(wsc.parameters(), lr=0.00005)\n",
    "    loss_func = nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([nTrain0/nTrain1])).cuda()\n",
    "    \n",
    "    loss_train = np.empty(epochs)\n",
    "    loss_validation = np.empty(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "#         t0 = time.time()\n",
    "        wsc.train()\n",
    "        for batchidx, (x, y) in enumerate(trainDataLoader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            yprime = wsc(x)\n",
    "            loss = loss_func(yprime, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        wsc.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batchidx, (x, y) in enumerate(validationDataLoader):\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                yprime = wsc(x)\n",
    "                lossVal = loss_func(yprime, y)\n",
    "                val_loss += lossVal.item()\n",
    "\n",
    "            val_loss /= len(validationDataLoader)\n",
    "\n",
    "        loss_train[epoch] = loss.item()\n",
    "        loss_validation[epoch] = val_loss\n",
    "#         print(time.time() - t0)\n",
    "        \n",
    "    wsc.cpu().eval()\n",
    "    \n",
    "    _, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax[0].plot(loss_train)\n",
    "    ax[0].plot(loss_validation)\n",
    "    foo = ax[1].hist(nn.Sigmoid()(wsc(torch.FloatTensor(X_train))).detach().numpy().flatten(), range=(0, 1), bins=20, density=True, histtype=\"step\")\n",
    "    foo = ax[1].hist(nn.Sigmoid()(wsc(torch.FloatTensor(X_test ))).detach().numpy().flatten(), range=(0, 1), bins=20, density=True, histtype=\"step\")\n",
    "    \n",
    "    plt.savefig(\"../Pic_cached/SequentialTraining/WSL/training_WSC_\"+cutID+\"_\" + version + \"_\" + datatype +\"_trained.jpg\")\n",
    "    plt.close()\n",
    "    \n",
    "    return wsc.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWSC_Onedetector(dataset0_to_be_examined, dataset1, cutID, version, datatype):\n",
    "# dataset0: bkg set from AE\n",
    "# dataset1: identified signal from AE\n",
    "\n",
    "    if len(dataset0_to_be_examined) > 10 * trainable_params_WSC:\n",
    "        dataset0 = dataset0_to_be_examined[np.random.choice(len(dataset0_to_be_examined), 10 * trainable_params_WSC, replace = False)]\n",
    "    else:\n",
    "        dataset0 = dataset0_to_be_examined\n",
    "    \n",
    "    print('{} noise events and {} signal events passed to WSC for training. '.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    nTotal0, nTotal1 = len(dataset0), len(dataset1);\n",
    "    nTrain0, nTrain1 = int(rTrain * nTotal0), int(rTrain * nTotal1)\n",
    "    nTest0 , nTest1  = int(rTest * nTotal0) , int(rTest * nTotal1)\n",
    "\n",
    "    X_train = np.concatenate((dataset0[:nTrain0], dataset1[:nTrain1]))\n",
    "    X_test = np.concatenate((dataset0[-nTest0:], dataset1[-nTest1:]))\n",
    "    X_validation = np.concatenate((dataset0[nTrain0:-nTest0], dataset1[nTrain1:-nTest1]))\n",
    "    \n",
    "    Y_train = np.concatenate((np.zeros((nTrain0, 1)), np.ones((nTrain1, 1))))\n",
    "    Y_test = np.concatenate((np.zeros((nTest0, 1)), np.ones((nTest1, 1))))\n",
    "    Y_validation = np.concatenate((np.zeros((dataset0[nTrain0:-nTest0].shape[0], 1)), np.ones((dataset1[nTrain1:-nTest1].shape[0], 1))))\n",
    "\n",
    "#     trainData = torch.FloatTensor(X_train)\n",
    "#     testData = torch.FloatTensor(X_test)\n",
    "#     validationData = torch.FloatTensor(X_validation)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(Y_train))\n",
    "    validation_dataset = TensorDataset(torch.FloatTensor(X_validation), torch.FloatTensor(Y_validation))\n",
    "#     train_dataset = TensorDataset(torch.FloatTensor(X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))), torch.FloatTensor(Y_train.reshape((Y_train.shape[0], 1, Y_train.shape[1]))))\n",
    "#     validation_dataset = TensorDataset(torch.FloatTensor(X_validation.reshape((X_validation.shape[0], 1, X_validation.shape[1]))), torch.FloatTensor(Y_validation.reshape((Y_validation.shape[0], 1, Y_validation.shape[1]))))\n",
    "\n",
    "    trainDataLoader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    validationDataLoader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle = True, drop_last=True)\n",
    "\n",
    "    wsc = WSClassifier_Onedetector().cuda()\n",
    "    optimizer = optim.Adam(wsc.parameters(), lr=0.00005)\n",
    "    loss_func = nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([nTrain0/nTrain1])).cuda()\n",
    "    \n",
    "    loss_train = np.empty(epochs)\n",
    "    loss_validation = np.empty(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "#         t0 = time.time()\n",
    "        wsc.train()\n",
    "        for batchidx, (x, y) in enumerate(trainDataLoader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            yprime = wsc(x)\n",
    "            loss = loss_func(yprime, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        wsc.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batchidx, (x, y) in enumerate(validationDataLoader):\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                yprime = wsc(x)\n",
    "                lossVal = loss_func(yprime, y)\n",
    "                val_loss += lossVal.item()\n",
    "\n",
    "            val_loss /= len(validationDataLoader)\n",
    "\n",
    "        loss_train[epoch] = loss.item()\n",
    "        loss_validation[epoch] = val_loss\n",
    "#         print(time.time() - t0)\n",
    "        \n",
    "    wsc.cpu().eval()\n",
    "    \n",
    "    _, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax[0].plot(loss_train)\n",
    "    ax[0].plot(loss_validation)\n",
    "    foo = ax[1].hist(nn.Sigmoid()(wsc(torch.FloatTensor(X_train))).detach().numpy().flatten(), range=(0, 1), bins=20, density=True, histtype=\"step\")\n",
    "    foo = ax[1].hist(nn.Sigmoid()(wsc(torch.FloatTensor(X_test ))).detach().numpy().flatten(), range=(0, 1), bins=20, density=True, histtype=\"step\")\n",
    "    \n",
    "    plt.savefig(\"../Pic_cached/SequentialTraining/WSL/training_WSC_\"+cutID+\"_\" + version + \"_\" + datatype +\"_trained.jpg\")\n",
    "    plt.close()\n",
    "    \n",
    "    return wsc.cpu().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass the first glith double WSL (pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the noise AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the noise WSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the BBH CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the BBH CNN WSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, let the test sample (WSL training sample) pass the procedure to get the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final label WSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v16'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First assume the CNN AE and the WSL is pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ncut = 5;\n",
    "cutList = {};\n",
    "\n",
    "max_glitch_L = 0.0024;\n",
    "min_glitch_L = 0.0008;\n",
    "cutList[\"glitch_L\"] = np.linspace(min_glitch_L, max_glitch_L, Ncut);\n",
    "\n",
    "max_glitch_H = 0.0024;\n",
    "min_glitch_H = 0.0004;\n",
    "cutList[\"glitch_H\"] = np.linspace(min_glitch_H, max_glitch_H, Ncut);\n",
    "\n",
    "max_bkg = 0.0018;\n",
    "min_bkg = 0.0008;\n",
    "cutList[\"noise\"] = np.linspace(min_bkg, max_bkg, Ncut);\n",
    "\n",
    "\n",
    "max_bbh_cnn = 0.0056\n",
    "min_bbh_cnn = 0.0045\n",
    "cutList['bbh_CNN'] = np.linspace(min_bbh_cnn, max_bbh_cnn, Ncut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "epochs_wsl = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0;\n",
    "\n",
    "ic = np.zeros(2, dtype=\"int\")\n",
    "\n",
    "# loop for only the cut in glitch, noise and bbh as it's not really meaningful to set cut in sg w/o new signals\n",
    "# ic[3] = Ncut-1;\n",
    "# ic[4] = Ncut-1;\n",
    "\n",
    "# listResult = {};\n",
    "# listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)), dtype=\"int\");\n",
    "# listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype)-1), len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype)-1), 2))\n",
    "\n",
    "for ic[0], ic[1] in itertools.product(np.arange(Ncut), np.arange(Ncut)):\n",
    "# for ic[0], ic[1], ic[2], ic[3] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "    cnt += 1;\n",
    "    \n",
    "    if cnt < 23:\n",
    "        continue\n",
    "    # elif cnt > 85:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in ['glitch', 'noise', 'bbh', 'sglf', 'sghf']:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_fft_filtered = dataset_wsl_fft_collected\n",
    "    dataset_wsl_filtered = dataset_wsl_collected\n",
    "    \n",
    "    cutID = \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version\n",
    "    \n",
    "    \n",
    "\n",
    "        # --- Glitch AE+WSL ---\n",
    "\n",
    "    iPrev = 0\n",
    "    \n",
    "    previousStep = 'glitch';\n",
    "    # modelPrev_L = models['glitch_L']; # previous step AE\n",
    "    # modelPrev_H = models['glitch_H']; # previous step AE\n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    # dcd_L = modelPrev_L(torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))[1].detach().numpy();\n",
    "    # dcd_H = modelPrev_H(torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))[1].detach().numpy();\n",
    "    # dataset1 = dataset_wsl_fft_filtered[np.logical_and(np.var(dataset_wsl_fft_filtered[:,:101]-dcd_L, axis=1) >= cutList['glitch_L'][ic[0]],\n",
    "    #                                                np.var(dataset_wsl_fft_filtered[:,101:]-dcd_H, axis=1) >= cutList['glitch_H'][ic[1]])]\n",
    "    \n",
    "    # model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    # models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "    # model_L = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_L']\n",
    "    # model_H = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_H']\n",
    "\n",
    "    # model_L = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(['1','0']) + '_v2.json')['glitch_L']\n",
    "    # model_H = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(['1','0']) + '_v2.json')['glitch_H']\n",
    "    \n",
    "    # --- Train the glitch AE for L\n",
    "\n",
    "    models[previousStep+\"_WSC_L\"] = model_L\n",
    "    models[previousStep+\"_WSC_H\"] = model_H\n",
    "\n",
    "    \n",
    "    # filter the data according to previous WSC\n",
    "    for dt in ['glitch', 'noise', 'bbh', 'sglf', 'sghf']:\n",
    "        dcd_L = nn.Sigmoid()(model_L(torch.FloatTensor(data_filtered[dt][:,:101]))).detach().numpy().flatten();\n",
    "        dcd_H = nn.Sigmoid()(model_H(torch.FloatTensor(data_filtered[dt][:,101:]))).detach().numpy().flatten();\n",
    "        data_filtered[dt] = data_filtered[dt][np.logical_and(dcd_L > 0.5, dcd_H > 0.5)]\n",
    "    \n",
    "    dcd_L = nn.Sigmoid()(model_L(torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))).detach().numpy().flatten();\n",
    "    dcd_H = nn.Sigmoid()(model_H(torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))).detach().numpy().flatten();\n",
    "\n",
    "    dataset_wsl_fft_filtered = dataset_wsl_fft_filtered[np.logical_and(dcd_L > 0.5, dcd_H > 0.5)]\n",
    "    dataset_wsl_filtered = dataset_wsl_filtered[np.logical_and(dcd_L > 0.5, dcd_H > 0.5)]\n",
    "\n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "    \n",
    "    # train the current step AE\n",
    "\n",
    "    print('Start noise AE')\n",
    "\n",
    "    print('Start training the noise WSL, dataset 0 size {}'.format(len(data_filtered['noise'])))\n",
    "\n",
    "    currentStep = 'noise';\n",
    "    model = trainAE(data_filtered[currentStep], cutID, version, currentStep);\n",
    "    models[currentStep] = model;   \n",
    "    \n",
    "    print('Noise AE trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "    \n",
    "    # --- Noise AE+WSL ---\n",
    "    # Here's a cut scan for the noise AE\n",
    "\n",
    "    print('Start noise WSL')\n",
    "\n",
    "    iPrev = 1\n",
    "\n",
    "    previousStep = 'noise';\n",
    "    modelPrev = models[previousStep]; # previous step AE\n",
    "    \n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    # dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "    # dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[0]]]\n",
    "    \n",
    "    dcd = modelPrev(torch.FloatTensor(dataset_wsl_fft_filtered))[1].detach().numpy();\n",
    "    dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered-dcd, axis=1) >= cutList[previousStep][ic[0]]]\n",
    "    \n",
    "    print('Start training the noise WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "    # --- Here I just filter the noise sample ---\n",
    "\n",
    "    data_noise_noiselike = data_filtered['noise'][nn.Sigmoid()(model(torch.FloatTensor(data_filtered['noise']))).detach().numpy().flatten() <= 0.5]\n",
    "    signal_noise_like_args = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft_filtered))).detach().numpy().flatten() <= 0.5\n",
    "    \n",
    "    print('Noise WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "\n",
    "    # data_wsl_noiselike = {}\n",
    "\n",
    "    # filter the data according to previous WSC\n",
    "    # for j in range(iPrev, 4):\n",
    "    #     dt = ind2datatype[j];\n",
    "    #     dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "    #     data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "    #     data_noiselike[dt] = data_filtered[dt][dcd<=0.5]\n",
    "\n",
    "    # dcd = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_filtered))).detach().numpy().flatten()\n",
    "    # data_wsl_noiselike = dataset_wsl_collected[dcd <= 0.5].reshape(-1,2,200)\n",
    "    \n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "    # --- BBH CNN+WSL ---\n",
    "    # Here's a cut scan for BBH CNN\n",
    "\n",
    "    print('Start BBHCNN WSL')\n",
    "\n",
    "    models['BBH_CNN'] = torch.load('../Model_cached/CNN_BBH/model_dep_3_chnl_4_btn_20_v4.pt')\n",
    "\n",
    "    # Note that for CNN, we have to use the initial input data (2 * 200 in timeseries)\n",
    "\n",
    "    dataset0 = data_noise_noiselike\n",
    "\n",
    "    dcd = models['BBH_CNN'](torch.FloatTensor(dataset_wsl_filtered[signal_noise_like_args].reshape(-1,2,200))).detach().numpy()\n",
    "    dataset1 = dataset_wsl_fft_filtered[signal_noise_like_args][np.mean(np.var(dataset_wsl_filtered[signal_noise_like_args].reshape(-1,2,200)-dcd, axis = 2), axis = 1) <= cutList['bbh_CNN'][ic[1]]]\n",
    "\n",
    "    print('Start training the BBHCNN WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "    \n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, 'BBH_CNN')\n",
    "    models['BBH_CNN_WSC'] = model\n",
    "\n",
    "    print('BBHCNN WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "    \n",
    "    # --- Finalized WSL ---\n",
    "\n",
    "    print('Start final WSL for cnt = {}'.format(cnt))\n",
    "\n",
    "    dcd = {}\n",
    "\n",
    "    dcd['glitch_WSC_L'] = nn.Sigmoid()(models['glitch_WSC_L'](torch.FloatTensor(dataset_wsl_fft_collected[:,:101]))).detach().numpy().flatten()\n",
    "    dcd['glitch_WSC_H'] = nn.Sigmoid()(models['glitch_WSC_H'](torch.FloatTensor(dataset_wsl_fft_collected[:,101:]))).detach().numpy().flatten()\n",
    "\n",
    "\n",
    "    for dt in ['noise_WSC', 'BBH_CNN_WSC']:\n",
    "        dcd[dt] = nn.Sigmoid()(models[dt](torch.FloatTensor(dataset_wsl_fft_collected))).detach().numpy().flatten()\n",
    "\n",
    "    glitch_pass = np.logical_and(dcd['glitch_WSC_L'] > 0.5, dcd['glitch_WSC_H'] > 0.5)\n",
    "\n",
    "    noise_pass = np.logical_or(dcd['noise_WSC'] > 0.5, np.logical_and(dcd['noise_WSC'] <= 0.5, dcd['BBH_CNN_WSC'] > 0.5))\n",
    "\n",
    "    passed_args = np.logical_and(glitch_pass, noise_pass)\n",
    "\n",
    "    # dataset0 = np.concatenate((dataset_ae_fft['glitch'], dataset_ae_fft['noise']), axis = 0)\n",
    "\n",
    "    dataset0 = data_filtered['noise']\n",
    "\n",
    "    dataset1 = dataset_wsl_fft_collected[passed_args]\n",
    "\n",
    "    print('Start training the final WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, 'Final')\n",
    "\n",
    "    models['Final_WSC'] = model\n",
    "\n",
    "\n",
    "    print('Final WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "\n",
    "    torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    print(models.keys())\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     dcd = {};\n",
    "#     err = {};\n",
    "#     ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "#     for datatype in list_datatype:\n",
    "#         dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "#         err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "        \n",
    "#     not_select = np.array([True]*len(testset));\n",
    "\n",
    "#     for iStep in range(len(list_datatype)):\n",
    "#         datatype = ind2datatype[iStep];\n",
    "#         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "#         ans[ind_pass] = iStep;\n",
    "#         not_select[ind_pass] = False;\n",
    "        \n",
    "#     ans[not_select] = -1;\n",
    "    \n",
    "#     listResult[\"cut\"][cnt] = ic;\n",
    "#     listResult[\"ans\"][cnt] = ans;\n",
    "    \n",
    "#     acc = np.zeros(len(ind2datatype));\n",
    "    \n",
    "#     for i in range(len(ind2datatype)):\n",
    "#         acc[i] = np.sum(np.logical_and(ans==i, correct_ans==i))/Nsample[ind2datatype[i]];\n",
    "        \n",
    "#     listResult[\"accuracy_4\"][cnt] = acc;\n",
    "    \n",
    "#     listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(Nsample[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "#                                      np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(Nsample[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "    # cnt += 1\n",
    "    print(cnt)\n",
    "    print(time.time() - t0)\n",
    "    \n",
    "# listResult[\"total_accuracy\"] = np.sum(listResult[\"ans\"]==correct_ans, axis=1)/len(testset);\n",
    "# torch.save(listResult, \"../data/SequentialTraining/training_performance_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0;\n",
    "\n",
    "ic = np.zeros(2, dtype=\"int\")\n",
    "\n",
    "# loop for only the cut in glitch, noise and bbh as it's not really meaningful to set cut in sg w/o new signals\n",
    "# ic[3] = Ncut-1;\n",
    "# ic[4] = Ncut-1;\n",
    "\n",
    "# listResult = {};\n",
    "# listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)), dtype=\"int\");\n",
    "# listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype)-1), len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype)-1), 2))\n",
    "\n",
    "for ic[0], ic[1] in itertools.product(np.arange(Ncut), np.arange(Ncut)):\n",
    "# for ic[0], ic[1], ic[2], ic[3] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "    cnt += 1;\n",
    "    \n",
    "    if not np.all(ic == np.array([4,0])):\n",
    "        continue\n",
    "    # elif cnt > 85:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in ['glitch', 'noise', 'bbh', 'sglf', 'sghf']:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_fft_filtered = dataset_wsl_fft_collected\n",
    "    dataset_wsl_filtered = dataset_wsl_collected\n",
    "    \n",
    "    cutID = \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version\n",
    "    \n",
    "    \n",
    "\n",
    "        # --- Glitch AE+WSL ---\n",
    "\n",
    "    iPrev = 0\n",
    "    \n",
    "    previousStep = 'glitch';\n",
    "    # modelPrev_L = models['glitch_L']; # previous step AE\n",
    "    # modelPrev_H = models['glitch_H']; # previous step AE\n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    # dcd_L = modelPrev_L(torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))[1].detach().numpy();\n",
    "    # dcd_H = modelPrev_H(torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))[1].detach().numpy();\n",
    "    # dataset1 = dataset_wsl_fft_filtered[np.logical_and(np.var(dataset_wsl_fft_filtered[:,:101]-dcd_L, axis=1) >= cutList['glitch_L'][ic[0]],\n",
    "    #                                                np.var(dataset_wsl_fft_filtered[:,101:]-dcd_H, axis=1) >= cutList['glitch_H'][ic[1]])]\n",
    "    \n",
    "    # model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    # models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "    # model_L = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_L']\n",
    "    # model_H = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_H']\n",
    "\n",
    "    model_L = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(['1','0']) + '_v2.json')['glitch_L']\n",
    "    model_H = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(['1','0']) + '_v2.json')['glitch_H']\n",
    "    \n",
    "    # --- Train the glitch AE for L\n",
    "\n",
    "    models[previousStep+\"_WSC_L\"] = model_L\n",
    "    models[previousStep+\"_WSC_H\"] = model_H\n",
    "\n",
    "    \n",
    "    # filter the data according to previous WSC\n",
    "    for dt in ['glitch', 'noise', 'bbh', 'sglf', 'sghf']:\n",
    "        dcd_L = nn.Sigmoid()(model_L(torch.FloatTensor(data_filtered[dt][:,:101]))).detach().numpy().flatten();\n",
    "        dcd_H = nn.Sigmoid()(model_H(torch.FloatTensor(data_filtered[dt][:,101:]))).detach().numpy().flatten();\n",
    "        data_filtered[dt] = data_filtered[dt][np.logical_and(dcd_L >= 0, dcd_H > 0.5)]\n",
    "    \n",
    "    dcd_L = nn.Sigmoid()(model_L(torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))).detach().numpy().flatten();\n",
    "    dcd_H = nn.Sigmoid()(model_H(torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))).detach().numpy().flatten();\n",
    "\n",
    "    dataset_wsl_fft_filtered = dataset_wsl_fft_filtered[np.logical_and(dcd_L >= 0, dcd_H > 0.5)]\n",
    "    dataset_wsl_filtered = dataset_wsl_filtered[np.logical_and(dcd_L >= 0, dcd_H > 0.5)]\n",
    "\n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "    \n",
    "    # train the current step AE\n",
    "\n",
    "    print('Start noise AE')\n",
    "\n",
    "    print('Start training the noise WSL, dataset 0 size {}'.format(len(data_filtered['noise'])))\n",
    "\n",
    "    currentStep = 'noise';\n",
    "    model = trainAE(data_filtered[currentStep], cutID, version, currentStep);\n",
    "    models[currentStep] = model;   \n",
    "    \n",
    "    print('Noise AE trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "    \n",
    "    # --- Noise AE+WSL ---\n",
    "    # Here's a cut scan for the noise AE\n",
    "\n",
    "    print('Start noise WSL')\n",
    "\n",
    "    iPrev = 1\n",
    "\n",
    "    previousStep = 'noise';\n",
    "    modelPrev = models[previousStep]; # previous step AE\n",
    "    \n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    # dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "    # dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[0]]]\n",
    "    \n",
    "    dcd = modelPrev(torch.FloatTensor(dataset_wsl_fft_filtered))[1].detach().numpy();\n",
    "    dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered-dcd, axis=1) >= cutList[previousStep][ic[0]]]\n",
    "    \n",
    "    print('Start training the noise WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "    # --- Here I just filter the noise sample ---\n",
    "\n",
    "    data_noise_noiselike = data_filtered['noise'][nn.Sigmoid()(model(torch.FloatTensor(data_filtered['noise']))).detach().numpy().flatten() <= 0.5]\n",
    "    signal_noise_like_args = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft_filtered))).detach().numpy().flatten() <= 0.5\n",
    "    \n",
    "    print('Noise WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "\n",
    "    # data_wsl_noiselike = {}\n",
    "\n",
    "    # filter the data according to previous WSC\n",
    "    # for j in range(iPrev, 4):\n",
    "    #     dt = ind2datatype[j];\n",
    "    #     dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "    #     data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "    #     data_noiselike[dt] = data_filtered[dt][dcd<=0.5]\n",
    "\n",
    "    # dcd = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_filtered))).detach().numpy().flatten()\n",
    "    # data_wsl_noiselike = dataset_wsl_collected[dcd <= 0.5].reshape(-1,2,200)\n",
    "    \n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "    # --- BBH CNN+WSL ---\n",
    "    # Here's a cut scan for BBH CNN\n",
    "\n",
    "    print('Start BBHCNN WSL')\n",
    "\n",
    "    models['BBH_CNN'] = torch.load('../Model_cached/CNN_BBH/model_dep_3_chnl_4_btn_20_v4.pt')\n",
    "\n",
    "    # Note that for CNN, we have to use the initial input data (2 * 200 in timeseries)\n",
    "\n",
    "    dataset0 = data_noise_noiselike\n",
    "\n",
    "    dcd = models['BBH_CNN'](torch.FloatTensor(dataset_wsl_filtered[signal_noise_like_args].reshape(-1,2,200))).detach().numpy()\n",
    "    dataset1 = dataset_wsl_fft_filtered[signal_noise_like_args][np.mean(np.var(dataset_wsl_filtered[signal_noise_like_args].reshape(-1,2,200)-dcd, axis = 2), axis = 1) <= cutList['bbh_CNN'][ic[1]]]\n",
    "\n",
    "    print('Start training the BBHCNN WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "    \n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, 'BBH_CNN')\n",
    "    models['BBH_CNN_WSC'] = model\n",
    "\n",
    "    print('BBHCNN WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "    \n",
    "    # --- Finalized WSL ---\n",
    "\n",
    "    print('Start final WSL for cnt = {}'.format(cnt))\n",
    "\n",
    "    dcd = {}\n",
    "\n",
    "    dcd['glitch_WSC_L'] = nn.Sigmoid()(models['glitch_WSC_L'](torch.FloatTensor(dataset_wsl_fft_collected[:,:101]))).detach().numpy().flatten()\n",
    "    dcd['glitch_WSC_H'] = nn.Sigmoid()(models['glitch_WSC_H'](torch.FloatTensor(dataset_wsl_fft_collected[:,101:]))).detach().numpy().flatten()\n",
    "\n",
    "\n",
    "    for dt in ['noise_WSC', 'BBH_CNN_WSC']:\n",
    "        dcd[dt] = nn.Sigmoid()(models[dt](torch.FloatTensor(dataset_wsl_fft_collected))).detach().numpy().flatten()\n",
    "\n",
    "    glitch_pass = np.logical_and(dcd['glitch_WSC_L'] > 0.5, dcd['glitch_WSC_H'] > 0.5)\n",
    "\n",
    "    noise_pass = np.logical_or(dcd['noise_WSC'] > 0.5, np.logical_and(dcd['noise_WSC'] <= 0.5, dcd['BBH_CNN_WSC'] > 0.5))\n",
    "\n",
    "    passed_args = np.logical_and(glitch_pass, noise_pass)\n",
    "\n",
    "    # dataset0 = np.concatenate((dataset_ae_fft['glitch'], dataset_ae_fft['noise']), axis = 0)\n",
    "\n",
    "    dataset0 = data_filtered['noise']\n",
    "\n",
    "    dataset1 = dataset_wsl_fft_collected[passed_args]\n",
    "\n",
    "    print('Start training the final WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, 'Final')\n",
    "\n",
    "    models['Final_WSC'] = model\n",
    "\n",
    "\n",
    "    print('Final WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "\n",
    "    torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    print(models.keys())\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     dcd = {};\n",
    "#     err = {};\n",
    "#     ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "#     for datatype in list_datatype:\n",
    "#         dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "#         err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "        \n",
    "#     not_select = np.array([True]*len(testset));\n",
    "\n",
    "#     for iStep in range(len(list_datatype)):\n",
    "#         datatype = ind2datatype[iStep];\n",
    "#         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "#         ans[ind_pass] = iStep;\n",
    "#         not_select[ind_pass] = False;\n",
    "        \n",
    "#     ans[not_select] = -1;\n",
    "    \n",
    "#     listResult[\"cut\"][cnt] = ic;\n",
    "#     listResult[\"ans\"][cnt] = ans;\n",
    "    \n",
    "#     acc = np.zeros(len(ind2datatype));\n",
    "    \n",
    "#     for i in range(len(ind2datatype)):\n",
    "#         acc[i] = np.sum(np.logical_and(ans==i, correct_ans==i))/Nsample[ind2datatype[i]];\n",
    "        \n",
    "#     listResult[\"accuracy_4\"][cnt] = acc;\n",
    "    \n",
    "#     listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(Nsample[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "#                                      np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(Nsample[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "    # cnt += 1\n",
    "    print(cnt)\n",
    "    print(time.time() - t0)\n",
    "    \n",
    "# listResult[\"total_accuracy\"] = np.sum(listResult[\"ans\"]==correct_ans, axis=1)/len(testset);\n",
    "# torch.save(listResult, \"../data/SequentialTraining/training_performance_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not np.all(ic == np.array([4,0,1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0;\n",
    "\n",
    "ic = np.zeros(4, dtype=\"int\")\n",
    "\n",
    "# loop for only the cut in glitch, noise and bbh as it's not really meaningful to set cut in sg w/o new signals\n",
    "# ic[3] = Ncut-1;\n",
    "# ic[4] = Ncut-1;\n",
    "\n",
    "# listResult = {};\n",
    "# listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)), dtype=\"int\");\n",
    "# listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype)-1), len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype)-1), 2))\n",
    "\n",
    "for ic[0], ic[1], ic[2], ic[3] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "# for ic[0], ic[1], ic[2], ic[3] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "    cnt += 1;\n",
    "    \n",
    "    print(ic)\n",
    "    \n",
    "    if not np.all(ic == np.array([4,0,1,0])):\n",
    "        continue\n",
    "    # elif cnt > 85:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in ['glitch', 'noise', 'bbh', 'sglf', 'sghf']:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_fft_filtered = dataset_wsl_fft_collected\n",
    "    dataset_wsl_filtered = dataset_wsl_collected\n",
    "    \n",
    "    cutID = \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version\n",
    "    \n",
    "    \n",
    "    print('Starting training on ic = {}'.format(''.join(str(ic[j] for j in range(4)))))\n",
    "    \n",
    "        # --- Glitch AE+WSL ---\n",
    "\n",
    "    iPrev = 0\n",
    "    \n",
    "    previousStep = 'glitch';\n",
    "    # modelPrev_L = models['glitch_L']; # previous step AE\n",
    "    # modelPrev_H = models['glitch_H']; # previous step AE\n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    # dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    # dcd_L = modelPrev_L(torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))[1].detach().numpy();\n",
    "    # dcd_H = modelPrev_H(torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))[1].detach().numpy();\n",
    "    # dataset1 = dataset_wsl_fft_filtered[np.logical_and(np.var(dataset_wsl_fft_filtered[:,:101]-dcd_L, axis=1) >= cutList['glitch_L'][ic[0]],\n",
    "    #                                                np.var(dataset_wsl_fft_filtered[:,101:]-dcd_H, axis=1) >= cutList['glitch_H'][ic[1]])]\n",
    "    \n",
    "    # model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    # models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "    # model_L = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_L']\n",
    "    # model_H = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_H']\n",
    "    \n",
    "    model = trainAE_Onedetector(dataset_ae_fft['glitch'][:,:101], cutID, version, 'glitch')\n",
    "    models['glitch_L'] = model\n",
    "    \n",
    "    model = trainAE_Onedetector(dataset_ae_fft['glitch'][:,101:], cutID, version, 'glitch')\n",
    "    models['glitch_H'] = model\n",
    "    \n",
    "    dcd = models['glitch_L'](torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))[1].detach().numpy()\n",
    "    \n",
    "    dataset0 = data_filtered['glitch'][:,:101]\n",
    "    dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered[:,:101]-dcd, axis=1) > cutList['glitch_L'][ic[2]]][:,:101]\n",
    "    \n",
    "    model_L = trainWSC_Onedetector(dataset0, dataset1, cutID, version, 'glitch')\n",
    "    models[previousStep+\"_WSC_L\"] = model_L\n",
    "    \n",
    "    \n",
    "    dcd = models['glitch_H'](torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))[1].detach().numpy()\n",
    "    \n",
    "    dataset0 = data_filtered['glitch'][:,101:]\n",
    "    dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered[:,101:]-dcd, axis=1) > cutList['glitch_H'][ic[3]]][:,101:]\n",
    "    \n",
    "    model_H = trainWSC_Onedetector(dataset0, dataset1, cutID, version, 'glitch')\n",
    "    models[previousStep+\"_WSC_H\"] = model_H\n",
    "\n",
    "    \n",
    "    # filter the data according to previous WSC\n",
    "    for dt in ['glitch', 'noise', 'bbh', 'sglf', 'sghf']:\n",
    "        dcd_L = nn.Sigmoid()(model_L(torch.FloatTensor(data_filtered[dt][:,:101]))).detach().numpy().flatten();\n",
    "        dcd_H = nn.Sigmoid()(model_H(torch.FloatTensor(data_filtered[dt][:,101:]))).detach().numpy().flatten();\n",
    "        data_filtered[dt] = data_filtered[dt][np.logical_and(dcd_L > 0.5, dcd_H > 0.5)]\n",
    "    \n",
    "    dcd_L = nn.Sigmoid()(model_L(torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))).detach().numpy().flatten();\n",
    "    dcd_H = nn.Sigmoid()(model_H(torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))).detach().numpy().flatten();\n",
    "\n",
    "    dataset_wsl_fft_filtered = dataset_wsl_fft_filtered[np.logical_and(dcd_L > 0.5, dcd_H > 0.5)]\n",
    "    dataset_wsl_filtered = dataset_wsl_filtered[np.logical_and(dcd_L > 0.5, dcd_H > 0.5)]\n",
    "\n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "    \n",
    "    # train the current step AE\n",
    "\n",
    "    print('Start noise AE')\n",
    "\n",
    "    print('Start training the noise WSL, dataset 0 size {}'.format(len(data_filtered['noise'])))\n",
    "\n",
    "    currentStep = 'noise';\n",
    "    model = trainAE(data_filtered[currentStep], cutID, version, currentStep);\n",
    "    models[currentStep] = model;   \n",
    "    \n",
    "    print('Noise AE trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "    \n",
    "    # --- Noise AE+WSL ---\n",
    "    # Here's a cut scan for the noise AE\n",
    "\n",
    "    print('Start noise WSL')\n",
    "\n",
    "    iPrev = 1\n",
    "\n",
    "    previousStep = 'noise';\n",
    "    modelPrev = models[previousStep]; # previous step AE\n",
    "    \n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    # dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "    # dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[0]]]\n",
    "    \n",
    "    dcd = modelPrev(torch.FloatTensor(dataset_wsl_fft_filtered))[1].detach().numpy();\n",
    "    dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered-dcd, axis=1) >= cutList[previousStep][ic[0]]]\n",
    "    \n",
    "    print('Start training the noise WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "    # --- Here I just filter the noise sample ---\n",
    "\n",
    "    data_noise_noiselike = data_filtered['noise'][nn.Sigmoid()(model(torch.FloatTensor(data_filtered['noise']))).detach().numpy().flatten() <= 0.5]\n",
    "    signal_noise_like_args = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft_filtered))).detach().numpy().flatten() <= 0.5\n",
    "    \n",
    "    print('Noise WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "\n",
    "    # data_wsl_noiselike = {}\n",
    "\n",
    "    # filter the data according to previous WSC\n",
    "    # for j in range(iPrev, 4):\n",
    "    #     dt = ind2datatype[j];\n",
    "    #     dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "    #     data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "    #     data_noiselike[dt] = data_filtered[dt][dcd<=0.5]\n",
    "\n",
    "    # dcd = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_filtered))).detach().numpy().flatten()\n",
    "    # data_wsl_noiselike = dataset_wsl_collected[dcd <= 0.5].reshape(-1,2,200)\n",
    "    \n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "    # --- BBH CNN+WSL ---\n",
    "    # Here's a cut scan for BBH CNN\n",
    "\n",
    "    print('Start BBHCNN WSL')\n",
    "\n",
    "    models['BBH_CNN'] = torch.load('../Model_cached/CNN_BBH/model_dep_3_chnl_4_btn_20_v4.pt')\n",
    "\n",
    "    # Note that for CNN, we have to use the initial input data (2 * 200 in timeseries)\n",
    "\n",
    "    dataset0 = data_noise_noiselike\n",
    "\n",
    "    dcd = models['BBH_CNN'](torch.FloatTensor(dataset_wsl_filtered[signal_noise_like_args].reshape(-1,2,200))).detach().numpy()\n",
    "    dataset1 = dataset_wsl_fft_filtered[signal_noise_like_args][np.mean(np.var(dataset_wsl_filtered[signal_noise_like_args].reshape(-1,2,200)-dcd, axis = 2), axis = 1) <= cutList['bbh_CNN'][ic[1]]]\n",
    "\n",
    "    print('Start training the BBHCNN WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "    \n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, 'BBH_CNN')\n",
    "    models['BBH_CNN_WSC'] = model\n",
    "\n",
    "    print('BBHCNN WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "    \n",
    "    # --- Finalized WSL ---\n",
    "\n",
    "    print('Start final WSL for cnt = {}'.format(cnt))\n",
    "\n",
    "    dcd = {}\n",
    "\n",
    "    dcd['glitch_WSC_L'] = nn.Sigmoid()(models['glitch_WSC_L'](torch.FloatTensor(dataset_wsl_fft_collected[:,:101]))).detach().numpy().flatten()\n",
    "    dcd['glitch_WSC_H'] = nn.Sigmoid()(models['glitch_WSC_H'](torch.FloatTensor(dataset_wsl_fft_collected[:,101:]))).detach().numpy().flatten()\n",
    "\n",
    "\n",
    "    for dt in ['noise_WSC', 'BBH_CNN_WSC']:\n",
    "        dcd[dt] = nn.Sigmoid()(models[dt](torch.FloatTensor(dataset_wsl_fft_collected))).detach().numpy().flatten()\n",
    "\n",
    "    glitch_pass = np.logical_and(dcd['glitch_WSC_L'] > 0.5, dcd['glitch_WSC_H'] > 0.5)\n",
    "\n",
    "    noise_pass = np.logical_or(dcd['noise_WSC'] > 0.5, np.logical_and(dcd['noise_WSC'] <= 0.5, dcd['BBH_CNN_WSC'] > 0.5))\n",
    "\n",
    "    passed_args = np.logical_and(glitch_pass, noise_pass)\n",
    "\n",
    "    # dataset0 = np.concatenate((dataset_ae_fft['glitch'], dataset_ae_fft['noise']), axis = 0)\n",
    "\n",
    "    dataset0 = data_filtered['noise']\n",
    "\n",
    "    dataset1 = dataset_wsl_fft_collected[passed_args]\n",
    "\n",
    "    print('Start training the final WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, 'Final')\n",
    "\n",
    "    models['Final_WSC'] = model\n",
    "\n",
    "\n",
    "    print('Final WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "\n",
    "    torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    print(models.keys())\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     dcd = {};\n",
    "#     err = {};\n",
    "#     ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "#     for datatype in list_datatype:\n",
    "#         dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "#         err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "        \n",
    "#     not_select = np.array([True]*len(testset));\n",
    "\n",
    "#     for iStep in range(len(list_datatype)):\n",
    "#         datatype = ind2datatype[iStep];\n",
    "#         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "#         ans[ind_pass] = iStep;\n",
    "#         not_select[ind_pass] = False;\n",
    "        \n",
    "#     ans[not_select] = -1;\n",
    "    \n",
    "#     listResult[\"cut\"][cnt] = ic;\n",
    "#     listResult[\"ans\"][cnt] = ans;\n",
    "    \n",
    "#     acc = np.zeros(len(ind2datatype));\n",
    "    \n",
    "#     for i in range(len(ind2datatype)):\n",
    "#         acc[i] = np.sum(np.logical_and(ans==i, correct_ans==i))/Nsample[ind2datatype[i]];\n",
    "        \n",
    "#     listResult[\"accuracy_4\"][cnt] = acc;\n",
    "    \n",
    "#     listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(Nsample[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "#                                      np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(Nsample[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "    # cnt += 1\n",
    "    print(cnt)\n",
    "    print(time.time() - t0)\n",
    "    \n",
    "# listResult[\"total_accuracy\"] = np.sum(listResult[\"ans\"]==correct_ans, axis=1)/len(testset);\n",
    "# torch.save(listResult, \"../data/SequentialTraining/training_performance_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.mean(np.var(dataset_wsl_collected[signal_noise_like_args].reshape(-1,2,200)-dcd, axis = 2), axis = 1) <= cutList['bbh_CNN'][ic[1]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_noise_like_args = np.argwhere(nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_filtered))).detach().numpy().flatten() <= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_noise_like_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_collected[signal_noise_like_args].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the pipeline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft_collected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_wsl.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans = np.hstack(([0]*N_wsl['glitch'], [1]*N_wsl['noise'], [2]*N_wsl['bbh'], [3]*N_wsl['sglf'], [4]*N_wsl['sghf']))\n",
    "correct_ans_withoutsignal = np.hstack(([0]*N_wsl['glitch'], [1]*N_wsl['noise'], [-1]*(N_wsl['bbh']+N_wsl['sglf']+N_wsl['sghf'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans_withoutsignal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datatype_withoutsignal = ['glitch','noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = dataset_wsl_fft_collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult = {};\n",
    "listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(list_datatype_withoutsignal)), dtype=\"int\");\n",
    "listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(list_datatype_withoutsignal)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), 2))\n",
    "cnt = 0\n",
    "ic_withoutsignal = np.zeros(2, dtype = int)\n",
    "\n",
    "listResult[\"FPR\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)),1))\n",
    "\n",
    "\n",
    "for ic_withoutsignal[0], ic_withoutsignal[1] in itertools.product(np.arange(Ncut), np.arange(Ncut)):\n",
    "\n",
    "    \n",
    "    # if cnt < 86:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in sequence:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_filtered = dataset_wsl_fft_collected\n",
    "        \n",
    "#     for iPrev in range(3):\n",
    "#         previousStep = ind2datatype[iPrev];\n",
    "#         modelPrev = models[previousStep]; # previous step AE\n",
    "        \n",
    "#         # train the WSC according to previous AE's cut\n",
    "        \n",
    "#         dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "#         dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset_wsl_filtered))[1].detach().numpy();\n",
    "#         dataset1 = dataset_wsl_filtered[np.var(dataset_wsl_filtered-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         model = trainWSC(dataset0, dataset1, cutID)\n",
    "#         models[previousStep+\"_WSC\"] = model;\n",
    "        \n",
    "#         # filter the data according to previous WSC\n",
    "#         for j in range(iPrev, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "#             data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "        \n",
    "# #         # filter the data\n",
    "# #         for j in range(iPrev+1, 4):\n",
    "# #             dt = ind2datatype[j];\n",
    "# #             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "# #             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "#         # train the current step AE\n",
    "#         currentStep = ind2datatype[iPrev+1];\n",
    "#         model = trainAE(data_filtered[currentStep], cutID);\n",
    "#         models[currentStep] = model;\n",
    "        \n",
    "#     torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version+\".json\")\n",
    "#     print(models.keys())\n",
    "    models = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic_withoutsignal[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    # print(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    dcd = {};\n",
    "    err = {};\n",
    "    ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "    dcd['glitch_L'] = nn.Sigmoid()(models['glitch_WSC_L'](torch.FloatTensor(testset[:,:101]))).detach().numpy().reshape(-1)\n",
    "    dcd['glitch_H'] = nn.Sigmoid()(models['glitch_WSC_H'](torch.FloatTensor(testset[:,101:]))).detach().numpy().reshape(-1)\n",
    "    dcd['noise_WSC'] = nn.Sigmoid()(models[\"noise_WSC\"](torch.FloatTensor(testset))).detach().numpy().reshape(-1)\n",
    "    dcd['Final_WSC'] = nn.Sigmoid()(models[\"Final_WSC\"](torch.FloatTensor(testset))).detach().numpy().reshape(-1)\n",
    "    \n",
    "    not_select = np.array([True]*len(testset));\n",
    "\n",
    "    # for iStep in range(len(list_datatype_withoutsignal)):\n",
    "    #     datatype = ind2datatype[iStep];\n",
    "    #     if datatype == 'sg':\n",
    "    #         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "    #     else:\n",
    "    #         ind_pass = np.logical_and(not_select, dcd[datatype] <= 0.5);\n",
    "    #     ans[ind_pass] = iStep;\n",
    "    #     not_select[ind_pass] = False;\n",
    "        \n",
    "    # Pass glitch first\n",
    "    \n",
    "    datatype = 'glitch'\n",
    "    ind_pass = np.logical_and(not_select, np.logical_or(dcd['glitch_L'] <= 0.5, dcd['glitch_H'] <= 0.5))\n",
    "    ans[ind_pass] = 0;\n",
    "    not_select[ind_pass] = False;\n",
    "    # print(dcd['glitch'])\n",
    "    \n",
    "    # Leftover are noise and signals\n",
    "    \n",
    "    # datatype = 'noise_WSC'\n",
    "    datatype = 'Final_WSC'\n",
    "    ind_pass = np.logical_and(not_select, dcd[datatype] <= np.sort(dcd[datatype][-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:][not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:]])[int(0.1 * (np.sum(not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:])))])\n",
    "    # ind_pass = np.logical_and(not_select, err[datatype] <= np.sort(err[datatype][-N_wsl['bbh']-N_wsl['sg']:])[int(0.1 * (N_wsl['bbh']+N_wsl['sg']))])\n",
    "    noise_number = np.sum(np.logical_and(not_select, correct_ans_withoutsignal == 1))\n",
    "    passed_noise_number = noise_number - np.sum(np.logical_and(ind_pass, correct_ans_withoutsignal == 1))\n",
    "    ans[ind_pass] = 1;\n",
    "    not_select[ind_pass] = False;\n",
    "    \n",
    "    ans[not_select] = -1\n",
    "    \n",
    "    FPR = passed_noise_number / noise_number\n",
    "    \n",
    "    # print(dcd['noise'])\n",
    "    print('For cnt = {}, totally {} noise events passed the glitch WSL, and {} noise events within the threshold for TPR=0.9'.format(cnt, noise_number, passed_noise_number))\n",
    "    listResult['FPR'][cnt] = FPR\n",
    "    listResult['cut'][cnt] = ic_withoutsignal\n",
    "    listResult['ans'][cnt] = ans\n",
    "        \n",
    "    # ans[not_select] = -1;\n",
    "\n",
    "    # listResult[\"cut\"][cnt] = ic_withoutsignal;\n",
    "    # listResult[\"ans\"][cnt] = ans;\n",
    "\n",
    "    # acc = np.zeros(len(ind2datatype));\n",
    "\n",
    "    # for i in range(len(ind2datatype)):\n",
    "    #     acc[i] = np.sum(np.logical_and(ans==i, correct_ans_withoutsignal==i))/N_wsl[ind2datatype[i]];\n",
    "        \n",
    "    # listResult[\"accuracy_4\"][cnt] = acc;\n",
    "\n",
    "    # listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(N_wsl[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "    #                                     np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "\n",
    "    cnt += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult = {};\n",
    "listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(list_datatype_withoutsignal)), dtype=\"int\");\n",
    "listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(list_datatype_withoutsignal)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), 2))\n",
    "cnt = 0\n",
    "ic_withoutsignal = np.zeros(2, dtype = int)\n",
    "\n",
    "listResult[\"FPR\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)),1))\n",
    "\n",
    "\n",
    "for ic_withoutsignal[0], ic_withoutsignal[1] in itertools.product(np.arange(Ncut), np.arange(Ncut)):\n",
    "\n",
    "    \n",
    "    # if cnt < 86:\n",
    "    #     continue\n",
    "    \n",
    "    if not np.all(ic_withoutsignal == np.array([4,0])):\n",
    "        continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in sequence:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_filtered = dataset_wsl_fft_collected\n",
    "        \n",
    "#     for iPrev in range(3):\n",
    "#         previousStep = ind2datatype[iPrev];\n",
    "#         modelPrev = models[previousStep]; # previous step AE\n",
    "        \n",
    "#         # train the WSC according to previous AE's cut\n",
    "        \n",
    "#         dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "#         dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset_wsl_filtered))[1].detach().numpy();\n",
    "#         dataset1 = dataset_wsl_filtered[np.var(dataset_wsl_filtered-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         model = trainWSC(dataset0, dataset1, cutID)\n",
    "#         models[previousStep+\"_WSC\"] = model;\n",
    "        \n",
    "#         # filter the data according to previous WSC\n",
    "#         for j in range(iPrev, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "#             data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "        \n",
    "# #         # filter the data\n",
    "# #         for j in range(iPrev+1, 4):\n",
    "# #             dt = ind2datatype[j];\n",
    "# #             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "# #             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "#         # train the current step AE\n",
    "#         currentStep = ind2datatype[iPrev+1];\n",
    "#         model = trainAE(data_filtered[currentStep], cutID);\n",
    "#         models[currentStep] = model;\n",
    "        \n",
    "#     torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version+\".json\")\n",
    "#     print(models.keys())\n",
    "    models = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic_withoutsignal[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    # print(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    dcd = {};\n",
    "    err = {};\n",
    "    ans = np.ones(len(testset), dtype=\"int\") * 2\n",
    "    \n",
    "    dcd['glitch_L'] = nn.Sigmoid()(models['glitch_WSC_L'](torch.FloatTensor(testset[:,:101]))).detach().numpy().reshape(-1)\n",
    "    dcd['glitch_H'] = nn.Sigmoid()(models['glitch_WSC_H'](torch.FloatTensor(testset[:,101:]))).detach().numpy().reshape(-1)\n",
    "    dcd['noise_WSC'] = nn.Sigmoid()(models[\"noise_WSC\"](torch.FloatTensor(testset))).detach().numpy().reshape(-1)\n",
    "    dcd['Final_WSC'] = nn.Sigmoid()(models[\"Final_WSC\"](torch.FloatTensor(testset))).detach().numpy().reshape(-1)\n",
    "    \n",
    "    not_select = np.array([True]*len(testset));\n",
    "\n",
    "    # for iStep in range(len(list_datatype_withoutsignal)):\n",
    "    #     datatype = ind2datatype[iStep];\n",
    "    #     if datatype == 'sg':\n",
    "    #         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "    #     else:\n",
    "    #         ind_pass = np.logical_and(not_select, dcd[datatype] <= 0.5);\n",
    "    #     ans[ind_pass] = iStep;\n",
    "    #     not_select[ind_pass] = False;\n",
    "        \n",
    "    # Pass glitch first\n",
    "    \n",
    "    datatype = 'glitch'\n",
    "    ind_pass = np.logical_and(not_select, np.logical_or(dcd['glitch_L'] < 0, dcd['glitch_H'] <= 0.5))\n",
    "    ans[ind_pass] = 0;\n",
    "    not_select[ind_pass] = False;\n",
    "    # print(dcd['glitch'])\n",
    "    \n",
    "    # Leftover are noise and signals\n",
    "    \n",
    "    # datatype = 'noise_WSC'\n",
    "    datatype = 'Final_WSC'\n",
    "    # ind_pass = np.logical_and(not_select, dcd[datatype] <= np.sort(dcd[datatype][-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:][not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:]])[int(0.1 * (np.sum(not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:])))])\n",
    "    # print(dcd[datatype][-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:][not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:]].shape)\n",
    "    # print(int(0.1 * (np.sum(not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:]))))\n",
    "    # print(np.sort(dcd[datatype][-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:][not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:]])[int(0.1 * (np.sum(not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:])))])\n",
    "    \n",
    "    # ind_pass = np.logical_and(not_select, dcd[datatype] <= 0.5)\n",
    "    ind_pass = np.logical_and(not_select, err[datatype] <= np.sort(err[datatype][-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:])[int(0.1 * (N_wsl['bbh']+N_wsl['sg']))])\n",
    "    noise_number = np.sum(np.logical_and(not_select, correct_ans_withoutsignal == 1))\n",
    "    passed_noise_number = noise_number - np.sum(np.logical_and(ind_pass, correct_ans_withoutsignal == 1))\n",
    "    ans[ind_pass] = 1;\n",
    "    not_select[ind_pass] = False;\n",
    "    \n",
    "    ans[not_select] = -1\n",
    "    \n",
    "    FPR = passed_noise_number / noise_number\n",
    "    \n",
    "    # print(dcd['noise'])\n",
    "    print('For cnt = {}, totally {} noise events passed the glitch WSL, and {} noise events within the threshold for TPR=0.9'.format(cnt, noise_number, passed_noise_number))\n",
    "    listResult['FPR'][cnt] = FPR\n",
    "    listResult['cut'][cnt] = ic_withoutsignal\n",
    "    listResult['ans'][cnt] = ans\n",
    "        \n",
    "    # ans[not_select] = -1;\n",
    "\n",
    "    # listResult[\"cut\"][cnt] = ic_withoutsignal;\n",
    "    # listResult[\"ans\"][cnt] = ans;\n",
    "\n",
    "    # acc = np.zeros(len(ind2datatype));\n",
    "\n",
    "    # for i in range(len(ind2datatype)):\n",
    "    #     acc[i] = np.sum(np.logical_and(ans==i, correct_ans_withoutsignal==i))/N_wsl[ind2datatype[i]];\n",
    "        \n",
    "    # listResult[\"accuracy_4\"][cnt] = acc;\n",
    "\n",
    "    # listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(N_wsl[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "    #                                     np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "\n",
    "    cnt += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_wsl['noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['ans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "\n",
    "np.sum(listResult['ans'][cnt][-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 20\n",
    "\n",
    "np.sum(listResult['ans'][cnt][-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "\n",
    "np.sum(listResult['ans'][cnt][N_wsl['glitch']:N_wsl['glitch']+N_wsl['noise']] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "\n",
    "np.sum(listResult['ans'][cnt][:N_wsl['glitch']] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcd[datatype][-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:][not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(['4','0']) + \"_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['noise_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "# glitch_filtered = dataset_wsl_fft['glitch']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    plt.title(\"Performance of noise WSC\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['BBH_CNN_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "# glitch_filtered = dataset_wsl_fft['glitch']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    plt.title(\"Performance of BBH_CNN WSC\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v12\n",
    "\n",
    "model = models['Final_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "# glitch_filtered = dataset_wsl_fft['glitch']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    plt.title(\"Performance of Final WSC\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v15\n",
    "\n",
    "model = models['Final_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "# glitch_filtered = dataset_wsl_fft['glitch']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    plt.title(\"Performance of Final WSC\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_L = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(['1','4']) + '_v2.json')['glitch_L']\n",
    "model_H = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(['1','4']) + '_v2.json')['glitch_H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casual Plot\n",
    "\n",
    "model = model_L\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_ae_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = nn.Sigmoid()(model(torch.FloatTensor(glitch_filtered))).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L, dataset L\")\n",
    "        \n",
    "        \n",
    "    # for snr in snr_range:\n",
    "\n",
    "    #     key = signal_type+snr\n",
    "    #     dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "    #     foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casual Plot\n",
    "\n",
    "model = model_L\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,101:]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_ae_fft['glitch'][:,101:]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = nn.Sigmoid()(model(torch.FloatTensor(glitch_filtered))).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L, dataset H\")\n",
    "        \n",
    "        \n",
    "    # for snr in snr_range:\n",
    "\n",
    "    #     key = signal_type+snr\n",
    "    #     dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "    #     foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casual Plot\n",
    "\n",
    "model = model_H\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,101:]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_ae_fft['glitch'][:,101:]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = nn.Sigmoid()(model(torch.FloatTensor(glitch_filtered))).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC H, dataset H\")\n",
    "        \n",
    "        \n",
    "    # for snr in snr_range:\n",
    "\n",
    "    #     key = signal_type+snr\n",
    "    #     dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "    #     foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casual Plot\n",
    "\n",
    "model = model_H\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_ae_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = nn.Sigmoid()(model(torch.FloatTensor(glitch_filtered))).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC H, dataset L\")\n",
    "        \n",
    "        \n",
    "    # for snr in snr_range:\n",
    "\n",
    "    #     key = signal_type+snr\n",
    "    #     dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "    #     foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['glitch_WSC_L']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = nn.Sigmoid()(model(torch.FloatTensor(glitch_filtered))).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['glitch_WSC_H']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,101:]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,101:]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = nn.Sigmoid()(model(torch.FloatTensor(glitch_filtered))).detach().numpy()\n",
    "\n",
    "count = 0\n",
    "\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC H\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,101:]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "        \n",
    "        count += np.sum(dcd_signal < 0.5)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14\n",
    "\n",
    "model = models['Final_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "# glitch_filtered = dataset_wsl_fft['glitch']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    plt.title(\"Performance of Final WSC\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14\n",
    "\n",
    "model = models['glitch_WSC_L']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14\n",
    "\n",
    "model = models['glitch_WSC_H']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,101:]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,101:]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC H\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,101:]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutList['glitch_L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14\n",
    "\n",
    "model = trainAE_Onedetector(dataset_ae_fft['glitch'][:,:101], 'test', 'test', 'test')\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,:101]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,:101]))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,:101]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,:101]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "plt.axvline(cutList['glitch_L'][ic[2]], color=\"k\", linestyle=\"--\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14\n",
    "\n",
    "model = models['glitch_L']\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,:101]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,:101]))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,:101]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,:101]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "plt.axvline(cutList['glitch_L'][ic[2]], color=\"k\", linestyle=\"--\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14\n",
    "\n",
    "model = models['glitch_H']\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,101:]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,101:]))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,101:]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,101:]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "plt.axvline(cutList['glitch_H'][ic[3]], color=\"k\", linestyle=\"--\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test training the glitch AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft_filtered = dataset_wsl_fft_collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = trainAE_Onedetector(dataset_ae_fft['glitch'][:,:101], 'test', 'test', 'glitch')\n",
    "# models['glitch_L'] = model\n",
    "\n",
    "# model = trainAE_Onedetector(dataset_ae_fft['glitch'][:,101:], 'test', 'test', 'glitch')\n",
    "# models['glitch_H'] = model\n",
    "\n",
    "dcd = models['glitch_L'](torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))[1].detach().numpy()\n",
    "\n",
    "dataset0 = dataset_ae_fft['glitch'][:,:101]\n",
    "dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered[:,:101]-dcd, axis=1) > cutList['glitch_L'][1]][:,:101]\n",
    "\n",
    "model_L = trainWSC_Onedetector(dataset0, dataset1, 'test', 'test', 'glitch')\n",
    "models[\"glitch_WSC_L\"] = model_L\n",
    "\n",
    "\n",
    "dcd = models['glitch_H'](torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))[1].detach().numpy()\n",
    "\n",
    "dataset0 = dataset_ae_fft['glitch'][:,101:]\n",
    "dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered[:,101:]-dcd, axis=1) > cutList['glitch_H'][0]][:,101:]\n",
    "\n",
    "model_H = trainWSC_Onedetector(dataset0, dataset1, 'test', 'test', 'glitch')\n",
    "models[\"glitch_WSC_H\"] = model_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcd = models['glitch_L'](torch.FloatTensor(dataset_wsl_fft['noise'][:,:101]))[1].detach().numpy()\n",
    "\n",
    "dataset0 = dataset_ae_fft['glitch'][:,:101]\n",
    "dataset1 = dataset_wsl_fft['noise'][np.var(dataset_wsl_fft['noise'][:,:101]-dcd, axis=1) > cutList['glitch_L'][0]][:,:101]\n",
    "\n",
    "model_L = trainWSC_Onedetector(dataset0, dataset1, 'test', 'test', 'glitch')\n",
    "models[\"glitch_WSC_L\"] = model_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't use this. This is not strict as we are using Hanford glitch model to fit for the Livinston\n",
    "\n",
    "dcd = models['glitch_H'](torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))[1].detach().numpy()\n",
    "\n",
    "dataset0 = dataset_ae_fft['glitch'][:,:101]\n",
    "dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered[:,:101]-dcd, axis=1) > cutList['glitch_H'][0]][:,:101]\n",
    "\n",
    "model_L = trainWSC_Onedetector(dataset0, dataset1, 'test', 'test', 'glitch')\n",
    "models[\"glitch_WSC_L\"] = model_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainAE_Onedetector(dataset_ae_fft['glitch'][:,:101], 'test', 'test', 'glitch')\n",
    "models['glitch_L'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_L']\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,:101]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,:101]))[1].detach().numpy()\n",
    "\n",
    "for key in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,:101]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,:101]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "\n",
    "    for snr in snr_range:\n",
    "        key_to_plot = key+snr\n",
    "        dcd_signal = model(torch.FloatTensor(dataset_wsl_fft[key_to_plot][:,:101]))[1].detach().numpy()\n",
    "        plt.hist(np.var(dataset_wsl_fft[key_to_plot][:,:101] - dcd_signal, axis = 1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=key_to_plot)\n",
    "\n",
    "\n",
    "\n",
    "    plt.title(\"trained with Livinston noise\")\n",
    "    # plt.axvline(cutList['glitch_L'][0], color=\"k\", linestyle=\"--\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_L']\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,:101]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,:101]))[1].detach().numpy()\n",
    "\n",
    "for key in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,:101]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,:101]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "\n",
    "    for snr in snr_range:\n",
    "        key_to_plot = key+snr\n",
    "        dcd_signal = model(torch.FloatTensor(dataset_wsl_fft[key_to_plot][:,:101]))[1].detach().numpy()\n",
    "        plt.hist(np.var(dataset_wsl_fft[key_to_plot][:,:101] - dcd_signal, axis = 1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=key_to_plot)\n",
    "\n",
    "\n",
    "\n",
    "    plt.title(\"trained with Livinston noise\")\n",
    "    plt.axvline(cutList['glitch_L'][0], color=\"k\", linestyle=\"--\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# Be careful here. The AE is using the Hanford AE\n",
    "\n",
    "model = models['glitch_WSC_L']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_ae_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = nn.Sigmoid()(model(torch.FloatTensor(glitch_filtered))).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# Be careful here. The AE is using the Hanford AE\n",
    "\n",
    "model = models['glitch_WSC_L']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_ae_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = nn.Sigmoid()(model(torch.FloatTensor(glitch_filtered))).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# Be careful here. The AE is using the Hanford AE\n",
    "\n",
    "model = models['glitch_WSC_H']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_ae_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = nn.Sigmoid()(model(torch.FloatTensor(glitch_filtered))).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_WSC_L']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_WSC_L']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_WSC_L']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_H']\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,101:]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,101:]))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,101:]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,101:]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "plt.axvline(cutList['glitch_H'][0], color=\"k\", linestyle=\"--\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# How about Livinston data in Hanford model\n",
    "\n",
    "model = models['glitch_H']\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,:101]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,:101]))[1].detach().numpy()\n",
    "\n",
    "for key in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,:101]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,:101]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "\n",
    "    for snr in snr_range:\n",
    "        key_to_plot = key+snr\n",
    "        dcd_signal = model(torch.FloatTensor(dataset_wsl_fft[key_to_plot][:,:101]))[1].detach().numpy()\n",
    "        plt.hist(np.var(dataset_wsl_fft[key_to_plot][:,:101] - dcd_signal, axis = 1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=key_to_plot)\n",
    "\n",
    "\n",
    "\n",
    "    plt.title(\"trained with Hanford glitch\")\n",
    "    plt.axvline(cutList['glitch_H'][1], color=\"k\", linestyle=\"--\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_H']\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,101:]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,101:]))[1].detach().numpy()\n",
    "\n",
    "for key in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,101:]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,101:]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "\n",
    "    for snr in snr_range:\n",
    "        key_to_plot = key+snr\n",
    "        dcd_signal = model(torch.FloatTensor(dataset_wsl_fft[key_to_plot][:,101:]))[1].detach().numpy()\n",
    "        plt.hist(np.var(dataset_wsl_fft[key_to_plot][:,101:] - dcd_signal, axis = 1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=key_to_plot)\n",
    "\n",
    "\n",
    "\n",
    "    plt.title(\"trained with Hanford noise\")\n",
    "    plt.axvline(cutList['glitch_H'][1], color=\"k\", linestyle=\"--\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "model = models['glitch_WSC_H']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,101:]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,101:]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC H\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,101:]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about the performance for glitch trained CNN AE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN = torch.load('../Model_cached/CNN_GLITCH/model_dep_6_chnl_12_btn_20.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcd_glitch = model(torch.FloatTensor(dataset_ae['glitch'][:10000][:,:200].reshape(-1,1,200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(dcd_glitch.detach().numpy() - dataset_ae['glitch'][:10000][:,:200].reshape(-1,1,200), axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl['noise'][:,:200].reshape(-1,1,200))).detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae['glitch'][:10000][:,:200].reshape(-1,1,200))).detach().numpy()\n",
    "\n",
    "for key in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    foo = plt.hist(np.var(dataset_wsl['noise'][:,:200].reshape(-1,1,200)-dcd_bkg, axis=-1).flatten(), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(np.var(dataset_ae['glitch'][:10000][:,:200].reshape(-1,1,200)-dcd_glitch, axis=-1).flatten(), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "\n",
    "    for snr in snr_range:\n",
    "        key_to_plot = key+snr\n",
    "        dcd_signal = model(torch.FloatTensor(dataset_wsl[key_to_plot][:,:200].reshape(-1,1,200))).detach().numpy()\n",
    "        plt.hist(np.var(dataset_wsl[key_to_plot][:,:200].reshape(-1,1,200) - dcd_signal, axis = -1).flatten(), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=key_to_plot)\n",
    "\n",
    "\n",
    "\n",
    "    plt.title(\"trained with Livinston glitch\")\n",
    "    # plt.axvline(cutList['glitch_H'][1], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(, color=\"k\", linestyle=\"--\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_collected[:,:200].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcd = model_CNN(torch.FloatTensor(dataset_wsl_collected[:,:200].reshape(-1,1,200))).detach().numpy()\n",
    "\n",
    "dataset0 = dataset_ae_fft['glitch'][:,:101]\n",
    "dataset1 = dataset_wsl_fft_collected[np.var(dataset_wsl_collected[:,:200].reshape(-1,1,200)-dcd, axis=-1).flatten()>0.004][:,:101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainWSC_Onedetector(dataset1, dataset0, 'test', 'test', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_ae_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = nn.Sigmoid()(model(torch.FloatTensor(glitch_filtered))).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_ae_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = nn.Sigmoid()(model(torch.FloatTensor(glitch_filtered))).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a 3-class learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(['4','0']) + \"_\"+'v12'+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft_collected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcd = {}\n",
    "\n",
    "dcd['glitch_WSC_L'] = nn.Sigmoid()(models['glitch_WSC_L'](torch.FloatTensor(dataset_wsl_fft_collected[:,:101]))).detach().numpy().flatten()\n",
    "dcd['glitch_WSC_H'] = nn.Sigmoid()(models['glitch_WSC_H'](torch.FloatTensor(dataset_wsl_fft_collected[:,101:]))).detach().numpy().flatten()\n",
    "\n",
    "for type in ['noise_WSC', 'BBH_CNN_WSC']:\n",
    "    dcd[type] = nn.Sigmoid()(models[type](torch.FloatTensor(dataset_wsl_fft_collected))).detach().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcd['glitch_WSC_L'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glitch_pass = np.logical_and(dcd['glitch_WSC_L'] > 0.5, dcd['glitch_WSC_H'] > 0.5)\n",
    "noise_pass = np.logical_or(dcd['noise_WSC'] > 0.5, np.logical_and(dcd['noise_WSC'] <= 0.5, dcd['BBH_CNN_WSC'] > 0.5))\n",
    "\n",
    "pass_idx = np.logical_and(glitch_pass, noise_pass)\n",
    "\n",
    "signal_events = dataset_wsl_fft_collected[pass_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_events.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ae_fft.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3class = trainWSC_3class(dataset_ae_fft['glitch_oneglitch_onenoise'], dataset_ae_fft['noise'], signal_events, 'test', 'test', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined WSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick check for roburtness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_wsl_total = 30000;\n",
    "N_wsl = {}\n",
    "N_wsl[\"noise\"] = int(0.65*N_wsl_total)\n",
    "N_wsl[\"bbh\"] = int(0.1*N_wsl_total)\n",
    "N_wsl[\"sglf\"] = int(0.1*N_wsl_total)\n",
    "N_wsl[\"sghf\"] = int(0.1*N_wsl_total)\n",
    "N_wsl[\"glitch\"] = int(0.05*N_wsl_total)\n",
    "\n",
    "\n",
    "snr_range = ['5-12','12-24','24-48','48-96']\n",
    "ratio = [0.25, 0.25, 0.25, 0.25]\n",
    "list_datatype = [\"noise\", \"bbh\", \"sglf\", \"sghf\", \"glitch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datatype_full = ['noise']\n",
    "\n",
    "for snr in snr_range:\n",
    "    list_datatype_full.append('bbh' + snr)\n",
    "    list_datatype_full.append('sglf' + snr)\n",
    "    list_datatype_full.append('sghf' + snr)\n",
    "    \n",
    "list_datatype_full.append('glitch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = ['glitch','noise', \n",
    "            'bbh5-12', 'bbh12-24', 'bbh24-48', 'bbh48-96',\n",
    "            'sglf5-12', 'sglf12-24', 'sglf24-48', 'sglf48-96',\n",
    "            'sghf5-12', 'sghf12-24', 'sghf24-48', 'sghf48-96',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(snr_range) == len(ratio)\n",
    "\n",
    "for i in range(len(ratio)):\n",
    "\n",
    "    N_wsl['bbh' + snr_range[i]] = int(ratio[i] * N_wsl['bbh'])\n",
    "    \n",
    "    N_wsl['sglf' + snr_range[i]] = int(ratio[i] * N_wsl['sglf'])\n",
    "    \n",
    "    N_wsl['sghf' + snr_range[i]] = int(ratio[i] * N_wsl['sghf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Introducing Chia-Jui's data\n",
    "\n",
    "renorm_factor_0 = 20;\n",
    "renorm_factor_1 = 20;\n",
    "\n",
    "realbkg_L = np.load('../Data_cached/real_bkg_2202000_63917s_4000Hz_50ms.npy')[:1000000].reshape(-1,1,200) / renorm_factor_0;\n",
    "realbkg_H = np.load('../Data_cached/real_bkg_H_1466640_58803s_4000Hz_50ms.npy')[:1000000].reshape(-1,1,200) / renorm_factor_0;\n",
    "\n",
    "realbkg = np.concatenate((realbkg_L, realbkg_H), axis = 1).reshape(-1,200)\n",
    "\n",
    "# realbbh = np.load('../Data_cached/injected_BBH_1823_around_merger_time_63917_58803.npz')['strain'].reshape(-1,200) / renorm_factor_0;\n",
    "\n",
    "# realsg = np.load('../Data_cached/injected_lfsg_1835_around_merger_time_63917_58803.npz')['strain'].reshape(-1,200) / renorm_factor_0;\n",
    "# realglitch = np.load(\"../data/real_glitches_9998_4000Hz_25ms.npz\")[\"strain_time_data\"]\n",
    "\n",
    "realbbh_list = {}\n",
    "realsglf_list = {}\n",
    "realsghf_list = {}\n",
    "\n",
    "\n",
    "for snr in snr_range:\n",
    "    realbbh_list[snr] = np.load('../Data_cached/injected_BBH_55k_snr{}_0th_events_before_merger_time_windowlength_200.npz'.format(snr))['strain'][:25000].reshape(-1,200)\n",
    "    realsglf_list[snr] = np.load('../Data_cached/injected_SGLF_55k_snr{}_0th_events_before_merger_time_windowlength_200.npz'.format(snr))['strain'][:25000].reshape(-1,200)\n",
    "    realsghf_list[snr] = np.load('../Data_cached/injected_SGHF_55k_snr{}_0th_events_before_merger_time_windowlength_200.npz'.format(snr))['strain'][:25000].reshape(-1,200)\n",
    "\n",
    "    \n",
    "\n",
    "realglitch_L = np.load(\"../Data_cached/real_glitches_snrlt5_60132_4000Hz_25ms.npz\")[\"strain_time_data\"][:50000].reshape(-1,1,200) / renorm_factor_1\n",
    "realglitch_H = np.load('../Data_cached/real_glitches_H_snrlt5_59732_4000Hz_25ms.npz')[\"strain_time_data\"][:50000].reshape(-1,1,200) / renorm_factor_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novel procedure. For training sample, the model is purely glitch trained. For testing and WSL sample, the model is one glitch + one noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_glitch_for_AE = int(len(realglitch_L) * 0.8) - N_wsl['glitch'] // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_glitch_one_noise = int(len(realglitch_L) * 0.2) + N_wsl['glitch'] // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(realglitch_L)\n",
    "np.random.shuffle(realglitch_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realglitch_L.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realbkg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glitch_for_ae = np.concatenate((realglitch_L[:num_glitch_for_AE], realglitch_H[:num_glitch_for_AE]), axis=1).reshape(-1,200)\n",
    "glitch_for_ae_fft = abs(np.fft.rfft(glitch_for_ae))\n",
    "glitch_for_ae_fft = glitch_for_ae_fft/np.linalg.norm([glitch_for_ae_fft], axis=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_for_glitch_building = realbkg[:2 * num_one_glitch_one_noise].reshape(-1,2,200)\n",
    "\n",
    "glitch_L_noise_H = np.concatenate((realglitch_L[-num_one_glitch_one_noise:], noise_for_glitch_building[:,[1],:]), axis = 1)\n",
    "noise_L_glitch_H = np.concatenate((noise_for_glitch_building[:,[0],:], realglitch_H[-num_one_glitch_one_noise:]), axis = 1)\n",
    "one_glitch_one_noise = np.vstack((glitch_L_noise_H, noise_L_glitch_H))\n",
    "np.random.shuffle(one_glitch_one_noise)\n",
    "one_glitch_one_noise = one_glitch_one_noise.reshape(-1,200)\n",
    "\n",
    "realbkg = realbkg[2 * num_one_glitch_one_noise:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_glitch_one_noise_fft = abs(np.fft.rfft(one_glitch_one_noise))\n",
    "one_glitch_one_noise_fft = one_glitch_one_noise_fft / np.linalg.norm([one_glitch_one_noise_fft], axis=2).T\n",
    "\n",
    "one_glitch_one_noise = one_glitch_one_noise / np.linalg.norm([one_glitch_one_noise], axis = 2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realbkg_reserved = realbkg[-int(0.2 * len(realbkg)):]\n",
    "# realbbh_reserved = realbbh[-int(0.2 * len(realbbh)):]\n",
    "# realsg_reserved = realsg[-int(0.2 * len(realsg)):]\n",
    "# realglitch_reserved = realglitch[-int(0.2 * len(realglitch)):]\n",
    "\n",
    "realbkg = realbkg[:-int(0.2 * len(realbkg))]\n",
    "# realbbh = realbbh[:-int(0.2 * len(realbbh))-int(0.2 * len(realbbh))%2]\n",
    "# realsg = realsg[:-int(0.2 * len(realsg))-int(0.2 * len(realsg))%2]\n",
    "# realglitch = realglitch[:-int(0.2 * len(realglitch))]\n",
    "\n",
    "realbkg = realbkg / np.linalg.norm([realbkg], axis = 2).T\n",
    "\n",
    "bkg_fft = abs(np.fft.rfft(realbkg))\n",
    "bkg_fft = bkg_fft/np.linalg.norm([bkg_fft], axis=2).T\n",
    "\n",
    "# bbh_fft = abs(np.fft.rfft(realbbh))\n",
    "# bbh_fft = bbh_fft/np.linalg.norm([bbh_fft], axis=2).T\n",
    "\n",
    "# sg_fft = abs(np.fft.rfft(realsg))\n",
    "# sg_fft = sg_fft/np.linalg.norm([sg_fft], axis=2).T\n",
    "\n",
    "# glitch_fft = abs(np.fft.rfft(realglitch))\n",
    "# glitch_fft = glitch_fft/np.linalg.norm([glitch_fft], axis=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realbbh_reserved_list = {}\n",
    "realsglf_reserved_list = {}\n",
    "realsghf_reserved_list = {}\n",
    "\n",
    "realbbh_fft_list = {}\n",
    "realsglf_fft_list = {}\n",
    "realsghf_fft_list = {}\n",
    "\n",
    "\n",
    "for snr in snr_range:\n",
    "    realbbh_reserved_list[snr] = realbbh_list[snr][-int(0.2 * len(realbbh_list[snr]))-int(0.2 * len(realbbh_list[snr]))%2:]\n",
    "    realbbh_list[snr] = realbbh_list[snr][:-int(0.2 * len(realbbh_list[snr]))-int(0.2 * len(realbbh_list[snr]))%2]\n",
    "\n",
    "    realbbh_fft_list[snr] = abs(np.fft.rfft(realbbh_list[snr]))\n",
    "    realbbh_fft_list[snr] = realbbh_fft_list[snr]/np.linalg.norm([realbbh_fft_list[snr]], axis=2).T\n",
    "    \n",
    "    realbbh_list[snr] = realbbh_list[snr]/np.linalg.norm([realbbh_list[snr]], axis=2).T\n",
    "    \n",
    "    \n",
    "    realsglf_reserved_list[snr] = realsglf_list[snr][-int(0.2 * len(realsglf_list[snr]))-int(0.2 * len(realsglf_list[snr]))%2:]\n",
    "    realsglf_list[snr] = realsglf_list[snr][:-int(0.2 * len(realsglf_list[snr]))-int(0.2 * len(realsglf_list[snr]))%2]\n",
    "\n",
    "    realsglf_fft_list[snr] = abs(np.fft.rfft(realsglf_list[snr]))\n",
    "    realsglf_fft_list[snr] = realsglf_fft_list[snr]/np.linalg.norm([realsglf_fft_list[snr]], axis=2).T\n",
    "    \n",
    "    realsglf_list[snr] = realsglf_list[snr]/np.linalg.norm([realsglf_list[snr]], axis=2).T\n",
    "    \n",
    "    \n",
    "    realsghf_reserved_list[snr] = realsghf_list[snr][-int(0.2 * len(realsghf_list[snr]))-int(0.2 * len(realsghf_list[snr]))%2:]\n",
    "    realsghf_list[snr] = realsghf_list[snr][:-int(0.2 * len(realsghf_list[snr]))-int(0.2 * len(realsghf_list[snr]))%2]\n",
    "\n",
    "    realsghf_fft_list[snr] = abs(np.fft.rfft(realsghf_list[snr]))\n",
    "    realsghf_fft_list[snr] = realsghf_fft_list[snr]/np.linalg.norm([realsghf_fft_list[snr]], axis=2).T\n",
    "    \n",
    "    realsghf_list[snr] = realsghf_list[snr]/np.linalg.norm([realsghf_list[snr]], axis=2).T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_fft = bkg_fft.reshape(-1,202)\n",
    "\n",
    "# bbh_fft = bbh_fft.reshape(-1,202)\n",
    "\n",
    "# sg_fft = sg_fft.reshape(-1,202)\n",
    "\n",
    "# glitch_fft = glitch_fft.reshape(-1,202)\n",
    "\n",
    "glitch_for_ae_fft = glitch_for_ae_fft.reshape(-1,202)\n",
    "\n",
    "one_glitch_one_noise_fft = one_glitch_one_noise_fft.reshape(-1,202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = {}\n",
    "dataset_raw_fft = {}\n",
    "\n",
    "dataset_raw[\"noise\"] = realbkg.reshape(-1,400)\n",
    "# dataset_raw_fft[\"bbh\"] = bbh_fft\n",
    "# dataset_raw_fft[\"sg\"] = sg_fft\n",
    "for snr in snr_range:\n",
    "    dataset_raw['bbh' + snr] = realbbh_list[snr].reshape(-1,400)\n",
    "    dataset_raw['sglf' + snr] = realsglf_list[snr].reshape(-1,400)\n",
    "    dataset_raw['sghf' + snr] = realsghf_list[snr].reshape(-1,400)\n",
    "# dataset_raw[\"glitch\"] = glitch.reshape(-1,400)\n",
    "\n",
    "\n",
    "dataset_raw_fft[\"noise\"] = bkg_fft\n",
    "# dataset_raw_fft[\"bbh\"] = bbh_fft\n",
    "# dataset_raw_fft[\"sg\"] = sg_fft\n",
    "for snr in snr_range:\n",
    "    dataset_raw_fft['bbh' + snr] = realbbh_fft_list[snr].reshape(-1,202)\n",
    "    dataset_raw_fft['sglf' + snr] = realsglf_fft_list[snr].reshape(-1,202)\n",
    "    dataset_raw_fft['sghf' + snr] = realsghf_fft_list[snr].reshape(-1,202)\n",
    "# dataset_raw_fft[\"glitch\"] = glitch_fft\n",
    "\n",
    "dataset_wsl = {};\n",
    "dataset_ae = {};\n",
    "dataset_wsl_fft = {};\n",
    "dataset_ae_fft = {};\n",
    "\n",
    "for dt in list_datatype_full[:-1]:\n",
    "    perm = np.random.permutation(len(dataset_raw_fft[dt]))\n",
    "    # perm = np.loadtxt(\"../Data_Cached/SequentialTraining/WSL/perm_\"+dt+\"_2det_Chia-Jui_v7_GWAK.dat\").astype(int)\n",
    "    \n",
    "    nwsl = N_wsl[dt]\n",
    "    dataset_wsl[dt] = dataset_raw[dt][perm[:nwsl]]\n",
    "    dataset_wsl_fft[dt] = dataset_raw_fft[dt][perm[:nwsl]]\n",
    "    # dataset_wsl[dt] = dataset_wsl[dt] / np.linalg.norm([dataset_wsl[dt]], axis=2).T\n",
    "    # dataset_wsl_fft[dt] = abs(np.fft.rfft(dataset_wsl[dt]))\n",
    "    # dataset_wsl_fft[dt] = dataset_wsl_fft[dt]/np.linalg.norm([dataset_wsl_fft[dt]], axis=2).T\n",
    "    \n",
    "    dataset_ae[dt] = dataset_raw[dt][perm[nwsl:]]\n",
    "    dataset_ae_fft[dt]  = dataset_raw_fft[dt][perm[nwsl:]]\n",
    "    # dataset_ae[dt] = dataset_ae[dt] / np.linalg.norm([dataset_ae[dt]], axis=2).T\n",
    "    # dataset_ae_fft[dt] = abs(np.fft.rfft(dataset_ae[dt]))\n",
    "    # dataset_ae_fft[dt] = dataset_ae_fft[dt]/np.linalg.norm([dataset_ae_fft[dt]], axis=2).T\n",
    "    \n",
    "    # np.savetxt(\"../Data_Cached/SequentialTraining/WSL/perm_\"+dt+\"_2det_Chia-Jui_\"+version+\"_2.dat\", perm)\n",
    "    \n",
    "dataset_ae['bbh'] = np.zeros((0,400))\n",
    "dataset_ae['sglf'] = np.zeros((0,400))\n",
    "dataset_ae['sghf'] = np.zeros((0,400))\n",
    "\n",
    "dataset_ae_fft['bbh'] = np.zeros((0,202))\n",
    "dataset_ae_fft['sglf'] = np.zeros((0,202))\n",
    "dataset_ae_fft['sghf'] = np.zeros((0,202))\n",
    "\n",
    "for snr in snr_range:\n",
    "    dataset_ae['bbh'] = np.append(dataset_ae['bbh'], dataset_ae['bbh'+snr])\n",
    "    dataset_ae_fft['bbh'] = np.append(dataset_ae_fft['bbh'], dataset_ae_fft['bbh'+snr])\n",
    "    \n",
    "    dataset_ae['sglf'] = np.append(dataset_ae['sglf'], dataset_ae['sglf'+snr])\n",
    "    dataset_ae_fft['sglf'] = np.append(dataset_ae_fft['sglf'], dataset_ae_fft['sglf'+snr])\n",
    "    \n",
    "    dataset_ae['sghf'] = np.append(dataset_ae['sghf'], dataset_ae['sghf'+snr])\n",
    "    dataset_ae_fft['sghf'] = np.append(dataset_ae_fft['sghf'], dataset_ae_fft['sghf'+snr])\n",
    "\n",
    "dataset_ae['bbh'] = dataset_ae['bbh'].reshape(-1,400)\n",
    "dataset_ae['sglf'] = dataset_ae['sglf'].reshape(-1,400)\n",
    "dataset_ae['sghf'] = dataset_ae['sghf'].reshape(-1,400)\n",
    "\n",
    "dataset_ae_fft['bbh'] = dataset_ae_fft['bbh'].reshape(-1,202)\n",
    "dataset_ae_fft['sglf'] = dataset_ae_fft['sglf'].reshape(-1,202)\n",
    "dataset_ae_fft['sghf'] = dataset_ae_fft['sghf'].reshape(-1,202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw[dt].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl[dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_raw_fft[dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft['glitch'] = one_glitch_one_noise_fft[:N_wsl['glitch']]\n",
    "dataset_wsl['glitch'] = one_glitch_one_noise[:2 * N_wsl['glitch']].reshape(-1,400)\n",
    "\n",
    "dataset_ae_fft['glitch'] = glitch_for_ae_fft\n",
    "\n",
    "dataset_ae_fft['glitch_oneglitch_onenoise'] = one_glitch_one_noise_fft[N_wsl['glitch']:]\n",
    "\n",
    "# Missing dataset ae ['glitch'] here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_wsl.keys():\n",
    "\n",
    "    print(np.linalg.norm(dataset_wsl[key][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_ae.keys():\n",
    "\n",
    "    print(np.linalg.norm(dataset_wsl[key][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_wsl_fft.keys():\n",
    "\n",
    "    print(np.linalg.norm(dataset_wsl[key][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ae_fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_ae.keys():\n",
    "    print(key)\n",
    "    print(dataset_ae[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_ae_fft.keys():\n",
    "    print(key)\n",
    "    print(dataset_ae_fft[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_ae_fft.keys():\n",
    "    print(key)\n",
    "    print(dataset_ae_fft[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_wsl.keys():\n",
    "    print(key)\n",
    "    print(dataset_wsl[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataset_wsl_fft.keys():\n",
    "    print(key)\n",
    "    print(dataset_wsl_fft[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft_collected = np.empty((0, 202))\n",
    "for dt in sequence:\n",
    "    dataset_wsl_fft_collected = np.vstack((dataset_wsl_fft_collected, dataset_wsl_fft[dt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_collected = np.empty((0, 400))\n",
    "for dt in sequence:\n",
    "    dataset_wsl_collected = np.vstack((dataset_wsl_collected, dataset_wsl[dt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft_collected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_collected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(202, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(10, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 202),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def count_trainable_params(model):\n",
    "    \"\"\"\n",
    "    计算给定PyTorch模型的可训练参数数量。\n",
    "    \n",
    "    参数:\n",
    "    model (nn.Module) - 要计算参数的PyTorch模型\n",
    "    \n",
    "    返回:\n",
    "    int - 模型的可训练参数数量\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    return trainable_params\n",
    "\n",
    "\n",
    "model = AutoEncoder()\n",
    "trainable_params = count_trainable_params(model)\n",
    "print(f\"Model has {trainable_params} trainable parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params_AE = count_trainable_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder_1det(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder_1det, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(101, 20),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(20, 101),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSClassifier_Onedetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WSClassifier_Onedetector, self).__init__()\n",
    "        self.fc1 = nn.Linear(101, 32)  # 第一层全连接层，输入维度为4，输出维度为64\n",
    "        self.norm1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()  # 激活函数\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        # self.norm2 = nn.BatchNorm1d(8)\n",
    "        # self.fc4 = nn.Linear(8, 1)  # 第三层全连接层，输入维度为32，输出维度为类别数目\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        # nn.init.kaiming_normal_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(self.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "        # x = self.relu(x)\n",
    "#         x = self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WSClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(202, 32)  # 第一层全连接层，输入维度为4，输出维度为64\n",
    "        self.norm1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()  # 激活函数\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        # self.norm2 = nn.BatchNorm1d(8)\n",
    "        # self.fc4 = nn.Linear(8, 1)  # 第三层全连接层，输入维度为32，输出维度为类别数目\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        # nn.init.kaiming_normal_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(self.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "        # x = self.relu(x)\n",
    "#         x = self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSClassifier_3class(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WSClassifier_3class, self).__init__()\n",
    "        self.fc1 = nn.Linear(202, 32)  # 第一层全连接层，输入维度为4，输出维度为64\n",
    "        self.norm1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU()  # 激活函数\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(32, 8)\n",
    "        self.norm2 = nn.BatchNorm1d(8)\n",
    "        self.fc4 = nn.Linear(8, 3)  # 第三层全连接层，输入维度为32，输出维度为类别数目\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        nn.init.kaiming_normal_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm2(self.relu(self.fc2(self.norm1(self.relu(self.fc1(x))))))\n",
    "        return self.fc4(x)\n",
    "        # x = self.relu(x)\n",
    "#         x = self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def count_trainable_params(model):\n",
    "    \"\"\"\n",
    "    计算给定PyTorch模型的可训练参数数量。\n",
    "    \n",
    "    参数:\n",
    "    model (nn.Module) - 要计算参数的PyTorch模型\n",
    "    \n",
    "    返回:\n",
    "    int - 模型的可训练参数数量\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    return trainable_params\n",
    "\n",
    "\n",
    "model = WSClassifier_3class()\n",
    "trainable_params = count_trainable_params(model)\n",
    "print(f\"Model has {trainable_params} trainable parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params_WSC = count_trainable_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveletCNNAE_xc(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        num_ifos: int,\n",
    "        c_depth: int=8, \n",
    "        n_chann: int=64, \n",
    "        l1: int=1024\n",
    "        # lx: int=200\n",
    "    ):\n",
    "        \n",
    "        super(WaveletCNNAE_xc, self).__init__()\n",
    "        \n",
    "        self.c_depth = c_depth\n",
    "        self.n_chann = n_chann\n",
    "        \n",
    "        self.cap_norm = nn.GroupNorm(num_ifos, num_ifos)\n",
    "        \n",
    "        self.Conv_In_encode = nn.Conv1d(\n",
    "                in_channels=num_ifos, \n",
    "                out_channels=self.n_chann, \n",
    "                kernel_size=1\n",
    "            )\n",
    "        \n",
    "        self.Conv_Out_encode = nn.Conv1d(\n",
    "                in_channels=self.n_chann, \n",
    "                out_channels=1, \n",
    "                kernel_size=1\n",
    "            )\n",
    "        \n",
    "        self.Conv_In_decode = nn.Conv1d(\n",
    "                in_channels=1, \n",
    "                out_channels=self.n_chann, \n",
    "                kernel_size=1\n",
    "            )\n",
    "        \n",
    "        self.Conv_Out_decode = nn.Conv1d(\n",
    "                in_channels=self.n_chann, \n",
    "                out_channels=num_ifos, \n",
    "                kernel_size=1\n",
    "            )\n",
    "        \n",
    "        self.body_norm_encode = nn.GroupNorm(4 ,n_chann)\n",
    "        self.body_norm_decode = nn.GroupNorm(4 ,n_chann)\n",
    "        self.end_norm_encode = nn.BatchNorm1d(1)\n",
    "        self.end_norm_decode = nn.BatchNorm1d(1)\n",
    "        \n",
    "        self.WaveNet_layers_encode = nn.ModuleList()\n",
    "        self.WaveNet_layers_decode = nn.ModuleList()\n",
    "        self.WaveNet_layers_dp = nn.ModuleList()\n",
    "        \n",
    "        \n",
    "        for i in range(self.c_depth):\n",
    "\n",
    "            conv_layer = nn.Conv1d(\n",
    "                in_channels=self.n_chann, \n",
    "                out_channels=self.n_chann,\n",
    "                kernel_size=2,\n",
    "                dilation=2**i\n",
    "            )\n",
    "            \n",
    "            self.WaveNet_layers_encode.append(conv_layer)\n",
    "            \n",
    "        for i in range(self.c_depth-1, -1, -1):\n",
    "\n",
    "            conv_layer = nn.Conv1d(\n",
    "                in_channels=self.n_chann, \n",
    "                out_channels=self.n_chann,\n",
    "                kernel_size=2,\n",
    "                dilation=2**i\n",
    "            )\n",
    "            \n",
    "            self.WaveNet_layers_decode.append(conv_layer)\n",
    "            self.WaveNet_layers_dp.append(nn.ZeroPad1d(2**i))\n",
    "        \n",
    "        \n",
    "#         self.Padding_layer = nn.ZeroPad1d(2**c_depth - 1)\n",
    "                \n",
    "        # self.L1 = nn.Linear(8192-2**c_depth, l1)\n",
    "        \n",
    "        # Consider replacing other batch normalizatoin layers with other nor method\n",
    "        # Because batch norm are baised by the population of the CCSN rate in one batch \n",
    "        # This may produce overfitting model and will not be able to found at test phase\n",
    "        # Question: Will we be able to figure out the side effect at infereceing phase?\n",
    "                \n",
    "#         self.conv_norm = nn.BatchNorm1d(200-2**c_depth + 1)\n",
    "        self.L1 = nn.Linear(200-2**c_depth + 1, l1)\n",
    "        self.L1_norm = nn.BatchNorm1d(l1)\n",
    "        self.L2 = nn.Linear(l1, 200-2**c_depth + 1)\n",
    "        self.L2_norm = nn.BatchNorm1d(200-2**c_depth + 1)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.Conv_In_encode.weight)\n",
    "        nn.init.kaiming_normal_(self.Conv_Out_encode.weight)\n",
    "        nn.init.constant_(self.Conv_In_encode.bias, 0.001)\n",
    "        nn.init.constant_(self.Conv_Out_encode.bias, 0.001)\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.Conv_In_decode.weight)\n",
    "        nn.init.kaiming_normal_(self.Conv_Out_decode.weight)\n",
    "        nn.init.constant_(self.Conv_In_decode.bias, 0.001)\n",
    "        nn.init.constant_(self.Conv_Out_decode.bias, 0.001)\n",
    "\n",
    "        # Initialize all the convolutional layer in between\n",
    "        for conv_layer in self.WaveNet_layers_encode:\n",
    "            nn.init.kaiming_normal_(conv_layer.weight)\n",
    "            nn.init.constant_(conv_layer.bias, 0.001)\n",
    "            \n",
    "        for conv_layer in self.WaveNet_layers_decode:\n",
    "            nn.init.kaiming_normal_(conv_layer.weight)\n",
    "            nn.init.constant_(conv_layer.bias, 0.001)    \n",
    "\n",
    "        nn.init.kaiming_uniform_(self.L1.weight)\n",
    "        nn.init.kaiming_uniform_(self.L2.weight)\n",
    "        nn.init.constant_(self.L1.bias, 0.001)\n",
    "        nn.init.constant_(self.L2.bias, 0.001)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \n",
    "        x = self.cap_norm(x)\n",
    "        x = self.Conv_In_encode(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # x = self.norm(x)\n",
    "        \n",
    "        for what_are_u_wavin_at in self.WaveNet_layers_encode:\n",
    "            x = self.body_norm_encode(x)\n",
    "            x = what_are_u_wavin_at(x)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "        x = self.Conv_Out_encode(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.end_norm_encode(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.L1_norm(F.relu(self.L1(x)))\n",
    "        \n",
    "        # print('Encoder done')\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = self.L2_norm(F.relu(self.L2(x)))\n",
    "        \n",
    "#         x = self.Padding_layer(x)\n",
    "        \n",
    "        x = torch.unsqueeze(x,1)\n",
    "\n",
    "        # print(x.shape)\n",
    "        \n",
    "        # x = self.cap_norm(x)\n",
    "        x = self.Conv_In_decode(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # x = self.norm(x)\n",
    "        \n",
    "        for (pad, dcd) in zip(self.WaveNet_layers_dp, self.WaveNet_layers_decode):\n",
    "            # print(x.shape)\n",
    "            x = self.body_norm_decode(x)\n",
    "            x = pad(x)\n",
    "            x = torch.flip(dcd(torch.flip(x, [-1])), [-1])\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        # print(x.shape)\n",
    "        # print('CNN done')\n",
    "        \n",
    "        x = self.Conv_Out_decode(x)\n",
    "        # print(x.shape)\n",
    "        x = F.tanh(x)\n",
    "        # print(x.shape)\n",
    "        # x = self.end_norm_decode(x)\n",
    "        \n",
    "        # x = torch.flatten(x, 1)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.decode(self.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAE(dataset0_to_be_examined, cutID, version, datatype):\n",
    "    \n",
    "    if len(dataset0_to_be_examined) > 10 * trainable_params_AE:\n",
    "        dataset = dataset0_to_be_examined[np.random.choice(len(dataset0_to_be_examined), 10 * trainable_params_AE, replace = False)]\n",
    "    else:\n",
    "        dataset = dataset0_to_be_examined\n",
    "\n",
    "    print('{} events passed to AE for training. '.format(len(dataset)))\n",
    "\n",
    "    nTotal = len(dataset);\n",
    "    nTrain = int(rTrain * nTotal)\n",
    "    nTest = int(rTest * nTotal)\n",
    "\n",
    "    X_train = dataset[:nTrain]\n",
    "    X_test = dataset[-nTest:]\n",
    "    X_validation = dataset[nTrain:-nTest]\n",
    "\n",
    "    trainData = torch.FloatTensor(X_train)\n",
    "    testData = torch.FloatTensor(X_test)\n",
    "    validationData = torch.FloatTensor(X_validation)\n",
    "\n",
    "    train_dataset = TensorDataset(trainData)\n",
    "    test_dataset = TensorDataset(testData)\n",
    "    validation_dataset = TensorDataset(validationData)\n",
    "\n",
    "    trainDataLoader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validationDataLoader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "    autoencoder = AutoEncoder().cuda()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.00005)\n",
    "    loss_func = nn.MSELoss().cuda()\n",
    "    \n",
    "    loss_train = np.empty(epochs)\n",
    "    loss_validation = np.empty(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        autoencoder.train()\n",
    "        for batchidx, x in enumerate(trainDataLoader):\n",
    "            x = x[0].cuda()\n",
    "            encoded, decoded = autoencoder(x)\n",
    "            loss_overall = loss_func(decoded, x)\n",
    "            weighted_lossTrain = loss_overall\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            weighted_lossTrain.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batchidx, x in enumerate(validationDataLoader):\n",
    "                x = x[0].cuda()\n",
    "                encoded, decoded = autoencoder(x)\n",
    "                lossVal = loss_func(decoded, x)\n",
    "                val_loss += lossVal.item()\n",
    "\n",
    "            val_loss /= len(validationDataLoader)\n",
    "\n",
    "        loss_train[epoch] = weighted_lossTrain.item()\n",
    "        loss_validation[epoch] = val_loss\n",
    "    \n",
    "    autoencoder.cpu().eval()\n",
    "    _, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax[0].plot(loss_train)\n",
    "    ax[0].plot(loss_validation)\n",
    "    \n",
    "    dcd_train = autoencoder(torch.FloatTensor(X_train))[1].detach().numpy()\n",
    "    err_train = np.var(X_train-dcd_train, axis=1)\n",
    "    dcd_test = autoencoder(torch.FloatTensor(X_test))[1].detach().numpy()\n",
    "    err_test = np.var(X_test-dcd_test, axis=1)\n",
    "    foo = ax[1].hist(err_train, range=(0, max(err_train)), bins=50, density=True, histtype=\"step\")\n",
    "    foo = ax[1].hist(err_test, range=(0, max(err_train)), bins=50, density=True, histtype=\"step\")\n",
    "    \n",
    "    plt.savefig(\"../Pic_cached/SequentialTraining/WSL/training_AE_\"+cutID+\"_\" + version + \"_\" + datatype +\"_trained.jpg\")\n",
    "    plt.close()\n",
    "            \n",
    "    return autoencoder.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAE_Onedetector(dataset0_to_be_examined, cutID, version, datatype):\n",
    "    \n",
    "    if len(dataset0_to_be_examined) > 10 * trainable_params_AE:\n",
    "        dataset = dataset0_to_be_examined[np.random.choice(len(dataset0_to_be_examined), 10 * trainable_params_AE, replace = False)]\n",
    "    else:\n",
    "        dataset = dataset0_to_be_examined\n",
    "\n",
    "    print('{} events passed to AE for training. '.format(len(dataset)))\n",
    "\n",
    "    nTotal = len(dataset);\n",
    "    nTrain = int(rTrain * nTotal)\n",
    "    nTest = int(rTest * nTotal)\n",
    "\n",
    "    X_train = dataset[:nTrain]\n",
    "    X_test = dataset[-nTest:]\n",
    "    X_validation = dataset[nTrain:-nTest]\n",
    "\n",
    "    trainData = torch.FloatTensor(X_train)\n",
    "    testData = torch.FloatTensor(X_test)\n",
    "    validationData = torch.FloatTensor(X_validation)\n",
    "\n",
    "    train_dataset = TensorDataset(trainData)\n",
    "    test_dataset = TensorDataset(testData)\n",
    "    validation_dataset = TensorDataset(validationData)\n",
    "\n",
    "    trainDataLoader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validationDataLoader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle = True)\n",
    "\n",
    "    autoencoder = AutoEncoder_1det().cuda()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.00005)\n",
    "    loss_func = nn.MSELoss().cuda()\n",
    "    \n",
    "    loss_train = np.empty(epochs)\n",
    "    loss_validation = np.empty(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        autoencoder.train()\n",
    "        for batchidx, x in enumerate(trainDataLoader):\n",
    "            x = x[0].cuda()\n",
    "            encoded, decoded = autoencoder(x)\n",
    "            loss_overall = loss_func(decoded, x)\n",
    "            weighted_lossTrain = loss_overall\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            weighted_lossTrain.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batchidx, x in enumerate(validationDataLoader):\n",
    "                x = x[0].cuda()\n",
    "                encoded, decoded = autoencoder(x)\n",
    "                lossVal = loss_func(decoded, x)\n",
    "                val_loss += lossVal.item()\n",
    "\n",
    "            val_loss /= len(validationDataLoader)\n",
    "\n",
    "        loss_train[epoch] = weighted_lossTrain.item()\n",
    "        loss_validation[epoch] = val_loss\n",
    "    \n",
    "    autoencoder.cpu().eval()\n",
    "    _, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax[0].plot(loss_train)\n",
    "    ax[0].plot(loss_validation)\n",
    "    \n",
    "    dcd_train = autoencoder(torch.FloatTensor(X_train))[1].detach().numpy()\n",
    "    err_train = np.var(X_train-dcd_train, axis=1)\n",
    "    dcd_test = autoencoder(torch.FloatTensor(X_test))[1].detach().numpy()\n",
    "    err_test = np.var(X_test-dcd_test, axis=1)\n",
    "    foo = ax[1].hist(err_train, range=(0, max(err_train)), bins=50, density=True, histtype=\"step\")\n",
    "    foo = ax[1].hist(err_test, range=(0, max(err_train)), bins=50, density=True, histtype=\"step\")\n",
    "    \n",
    "    plt.savefig(\"../Pic_cached/SequentialTraining/WSL/training_AE_\"+cutID+\"_\" + version + \"_\" + datatype +\"_trained.jpg\")\n",
    "    plt.close()\n",
    "            \n",
    "    return autoencoder.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWSC(dataset0_to_be_examined, dataset1, cutID, version, datatype):\n",
    "# dataset0: bkg set from AE\n",
    "# dataset1: identified signal from AE\n",
    "\n",
    "    if len(dataset0_to_be_examined) > 10 * trainable_params_WSC:\n",
    "        dataset0 = dataset0_to_be_examined[np.random.choice(len(dataset0_to_be_examined), 10 * trainable_params_WSC, replace = False)]\n",
    "    else:\n",
    "        dataset0 = dataset0_to_be_examined\n",
    "    \n",
    "    print('{} noise events and {} signal events passed to WSC for training. '.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    nTotal0, nTotal1 = len(dataset0), len(dataset1);\n",
    "    nTrain0, nTrain1 = int(rTrain * nTotal0), int(rTrain * nTotal1)\n",
    "    nTest0 , nTest1  = int(rTest * nTotal0) , int(rTest * nTotal1)\n",
    "\n",
    "    X_train = np.concatenate((dataset0[:nTrain0], dataset1[:nTrain1]))\n",
    "    X_test = np.concatenate((dataset0[-nTest0:], dataset1[-nTest1:]))\n",
    "    X_validation = np.concatenate((dataset0[nTrain0:-nTest0], dataset1[nTrain1:-nTest1]))\n",
    "    \n",
    "    Y_train = np.concatenate((np.zeros((nTrain0, 1)), np.ones((nTrain1, 1))))\n",
    "    Y_test = np.concatenate((np.zeros((nTest0, 1)), np.ones((nTest1, 1))))\n",
    "    Y_validation = np.concatenate((np.zeros((dataset0[nTrain0:-nTest0].shape[0], 1)), np.ones((dataset1[nTrain1:-nTest1].shape[0], 1))))\n",
    "\n",
    "#     trainData = torch.FloatTensor(X_train)\n",
    "#     testData = torch.FloatTensor(X_test)\n",
    "#     validationData = torch.FloatTensor(X_validation)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(Y_train))\n",
    "    validation_dataset = TensorDataset(torch.FloatTensor(X_validation), torch.FloatTensor(Y_validation))\n",
    "#     train_dataset = TensorDataset(torch.FloatTensor(X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))), torch.FloatTensor(Y_train.reshape((Y_train.shape[0], 1, Y_train.shape[1]))))\n",
    "#     validation_dataset = TensorDataset(torch.FloatTensor(X_validation.reshape((X_validation.shape[0], 1, X_validation.shape[1]))), torch.FloatTensor(Y_validation.reshape((Y_validation.shape[0], 1, Y_validation.shape[1]))))\n",
    "\n",
    "    trainDataLoader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    validationDataLoader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle = True, drop_last=True)\n",
    "\n",
    "    wsc = WSClassifier().cuda()\n",
    "    optimizer = optim.Adam(wsc.parameters(), lr=0.00005)\n",
    "    loss_func = nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([nTrain0/nTrain1])).cuda()\n",
    "    \n",
    "    loss_train = np.empty(epochs)\n",
    "    loss_validation = np.empty(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "#         t0 = time.time()\n",
    "        wsc.train()\n",
    "        for batchidx, (x, y) in enumerate(trainDataLoader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            yprime = wsc(x)\n",
    "            loss = loss_func(yprime, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        wsc.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batchidx, (x, y) in enumerate(validationDataLoader):\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                yprime = wsc(x)\n",
    "                lossVal = loss_func(yprime, y)\n",
    "                val_loss += lossVal.item()\n",
    "\n",
    "            val_loss /= len(validationDataLoader)\n",
    "\n",
    "        loss_train[epoch] = loss.item()\n",
    "        loss_validation[epoch] = val_loss\n",
    "#         print(time.time() - t0)\n",
    "        \n",
    "    wsc.cpu().eval()\n",
    "    \n",
    "    _, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax[0].plot(loss_train)\n",
    "    ax[0].plot(loss_validation)\n",
    "    foo = ax[1].hist(nn.Sigmoid()(wsc(torch.FloatTensor(X_train))).detach().numpy().flatten(), range=(0, 1), bins=20, density=True, histtype=\"step\")\n",
    "    foo = ax[1].hist(nn.Sigmoid()(wsc(torch.FloatTensor(X_test ))).detach().numpy().flatten(), range=(0, 1), bins=20, density=True, histtype=\"step\")\n",
    "    \n",
    "    plt.savefig(\"../Pic_cached/SequentialTraining/WSL/training_WSC_\"+cutID+\"_\" + version + \"_\" + datatype +\"_trained.jpg\")\n",
    "    plt.close()\n",
    "    \n",
    "    return wsc.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWSC_Onedetector(dataset0_to_be_examined, dataset1, cutID, version, datatype):\n",
    "# dataset0: bkg set from AE\n",
    "# dataset1: identified signal from AE\n",
    "\n",
    "    if len(dataset0_to_be_examined) > 10 * trainable_params_WSC:\n",
    "        dataset0 = dataset0_to_be_examined[np.random.choice(len(dataset0_to_be_examined), 10 * trainable_params_WSC, replace = False)]\n",
    "    else:\n",
    "        dataset0 = dataset0_to_be_examined\n",
    "    \n",
    "    print('{} noise events and {} signal events passed to WSC for training. '.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    nTotal0, nTotal1 = len(dataset0), len(dataset1);\n",
    "    nTrain0, nTrain1 = int(rTrain * nTotal0), int(rTrain * nTotal1)\n",
    "    nTest0 , nTest1  = int(rTest * nTotal0) , int(rTest * nTotal1)\n",
    "\n",
    "    X_train = np.concatenate((dataset0[:nTrain0], dataset1[:nTrain1]))\n",
    "    X_test = np.concatenate((dataset0[-nTest0:], dataset1[-nTest1:]))\n",
    "    X_validation = np.concatenate((dataset0[nTrain0:-nTest0], dataset1[nTrain1:-nTest1]))\n",
    "    \n",
    "    Y_train = np.concatenate((np.zeros((nTrain0, 1)), np.ones((nTrain1, 1))))\n",
    "    Y_test = np.concatenate((np.zeros((nTest0, 1)), np.ones((nTest1, 1))))\n",
    "    Y_validation = np.concatenate((np.zeros((dataset0[nTrain0:-nTest0].shape[0], 1)), np.ones((dataset1[nTrain1:-nTest1].shape[0], 1))))\n",
    "\n",
    "#     trainData = torch.FloatTensor(X_train)\n",
    "#     testData = torch.FloatTensor(X_test)\n",
    "#     validationData = torch.FloatTensor(X_validation)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(Y_train))\n",
    "    validation_dataset = TensorDataset(torch.FloatTensor(X_validation), torch.FloatTensor(Y_validation))\n",
    "#     train_dataset = TensorDataset(torch.FloatTensor(X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))), torch.FloatTensor(Y_train.reshape((Y_train.shape[0], 1, Y_train.shape[1]))))\n",
    "#     validation_dataset = TensorDataset(torch.FloatTensor(X_validation.reshape((X_validation.shape[0], 1, X_validation.shape[1]))), torch.FloatTensor(Y_validation.reshape((Y_validation.shape[0], 1, Y_validation.shape[1]))))\n",
    "\n",
    "    trainDataLoader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    validationDataLoader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle = True, drop_last=True)\n",
    "\n",
    "    wsc = WSClassifier_Onedetector().cuda()\n",
    "    optimizer = optim.Adam(wsc.parameters(), lr=0.00005)\n",
    "    loss_func = nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([nTrain0/nTrain1])).cuda()\n",
    "    \n",
    "    loss_train = np.empty(epochs)\n",
    "    loss_validation = np.empty(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "#         t0 = time.time()\n",
    "        wsc.train()\n",
    "        for batchidx, (x, y) in enumerate(trainDataLoader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            yprime = wsc(x)\n",
    "            loss = loss_func(yprime, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        wsc.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batchidx, (x, y) in enumerate(validationDataLoader):\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                yprime = wsc(x)\n",
    "                lossVal = loss_func(yprime, y)\n",
    "                val_loss += lossVal.item()\n",
    "\n",
    "            val_loss /= len(validationDataLoader)\n",
    "\n",
    "        loss_train[epoch] = loss.item()\n",
    "        loss_validation[epoch] = val_loss\n",
    "#         print(time.time() - t0)\n",
    "        \n",
    "    wsc.cpu().eval()\n",
    "    \n",
    "    _, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax[0].plot(loss_train)\n",
    "    ax[0].plot(loss_validation)\n",
    "    foo = ax[1].hist(nn.Sigmoid()(wsc(torch.FloatTensor(X_train))).detach().numpy().flatten(), range=(0, 1), bins=20, density=True, histtype=\"step\")\n",
    "    foo = ax[1].hist(nn.Sigmoid()(wsc(torch.FloatTensor(X_test ))).detach().numpy().flatten(), range=(0, 1), bins=20, density=True, histtype=\"step\")\n",
    "    \n",
    "    plt.savefig(\"../Pic_cached/SequentialTraining/WSL/training_WSC_\"+cutID+\"_\" + version + \"_\" + datatype +\"_trained.jpg\")\n",
    "    plt.close()\n",
    "    \n",
    "    return wsc.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWSC_3class(dataset0_to_be_examined, dataset1_to_be_examined, dataset2, cutID, version, datatype):\n",
    "# dataset0: bkg set from AE\n",
    "# dataset1: identified signal from AE\n",
    "\n",
    "    if len(dataset0_to_be_examined) > 10 * trainable_params_WSC:\n",
    "        dataset0 = dataset0_to_be_examined[np.random.choice(len(dataset0_to_be_examined), 10 * trainable_params_WSC, replace = False)]\n",
    "    else:\n",
    "        dataset0 = dataset0_to_be_examined\n",
    "    \n",
    "    if len(dataset1_to_be_examined) > 10 * trainable_params_WSC:\n",
    "        dataset1 = dataset1_to_be_examined[np.random.choice(len(dataset1_to_be_examined), 10 * trainable_params_WSC, replace = False)]\n",
    "    else:\n",
    "        dataset1 = dataset1_to_be_examined\n",
    "\n",
    "    print('{}, {} noise events and {} signal events passed to WSC for training. '.format(len(dataset0), len(dataset1), len(dataset2)))\n",
    "\n",
    "    nTotal0, nTotal1, nTotal2 = len(dataset0), len(dataset1), len(dataset2);\n",
    "    nTrain0, nTrain1, nTrain2 = int(rTrain * nTotal0), int(rTrain * nTotal1), int(rTrain * nTotal2)\n",
    "    nTest0 , nTest1, nTest2  = int(rTest * nTotal0) , int(rTest * nTotal1), int(rTest * nTotal2)\n",
    "\n",
    "    X_train = np.concatenate((dataset0[:nTrain0], dataset1[:nTrain1], dataset2[:nTrain2]))\n",
    "    X_test = np.concatenate((dataset0[-nTest0:], dataset1[-nTest1:], dataset2[-nTest2:]))\n",
    "    X_validation = np.concatenate((dataset0[nTrain0:-nTest0], dataset1[nTrain1:-nTest1], dataset2[nTrain2:-nTest2]))\n",
    "    \n",
    "    Y_train = np.concatenate((np.zeros((nTrain0, 1)), np.ones((nTrain1, 1)), 2 * np.ones((nTrain2, 1)))).flatten().astype(int)\n",
    "    Y_test = np.concatenate((np.zeros((nTest0, 1)), np.ones((nTest1, 1)), 2 * np.ones((nTest2, 1)))).flatten().astype(int)\n",
    "    Y_validation = np.concatenate((np.zeros((dataset0[nTrain0:-nTest0].shape[0], 1)), np.ones((dataset1[nTrain1:-nTest1].shape[0], 1)), 2 * np.ones((dataset2[nTrain2:-nTest2].shape[0], 1)))).flatten().astype(int)\n",
    "\n",
    "#     trainData = torch.FloatTensor(X_train)\n",
    "#     testData = torch.FloatTensor(X_test)\n",
    "#     validationData = torch.FloatTensor(X_validation)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(Y_train))\n",
    "    validation_dataset = TensorDataset(torch.FloatTensor(X_validation), torch.LongTensor(Y_validation))\n",
    "#     train_dataset = TensorDataset(torch.FloatTensor(X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))), torch.FloatTensor(Y_train.reshape((Y_train.shape[0], 1, Y_train.shape[1]))))\n",
    "#     validation_dataset = TensorDataset(torch.FloatTensor(X_validation.reshape((X_validation.shape[0], 1, X_validation.shape[1]))), torch.FloatTensor(Y_validation.reshape((Y_validation.shape[0], 1, Y_validation.shape[1]))))\n",
    "\n",
    "    trainDataLoader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    validationDataLoader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle = True, drop_last=True)\n",
    "\n",
    "    wsc = WSClassifier_3class().cuda()\n",
    "    optimizer = optim.Adam(wsc.parameters(), lr=0.00005)\n",
    "    loss_func = nn.CrossEntropyLoss().cuda()\n",
    "    \n",
    "    loss_train = np.empty(epochs)\n",
    "    loss_validation = np.empty(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "#         t0 = time.time()\n",
    "        wsc.train()\n",
    "        for batchidx, (x, y) in enumerate(trainDataLoader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            yprime = wsc(x)\n",
    "            loss = loss_func(yprime, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        wsc.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batchidx, (x, y) in enumerate(validationDataLoader):\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                yprime = wsc(x)\n",
    "                lossVal = loss_func(yprime, y)\n",
    "                val_loss += lossVal.item()\n",
    "\n",
    "            val_loss /= len(validationDataLoader)\n",
    "\n",
    "        loss_train[epoch] = loss.item()\n",
    "        loss_validation[epoch] = val_loss\n",
    "#         print(time.time() - t0)\n",
    "        \n",
    "    wsc.cpu().eval()\n",
    "    \n",
    "    _, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax[0].plot(loss_train)\n",
    "    ax[0].plot(loss_validation)\n",
    "    foo = ax[1].hist(nn.Softmax(dim=1)(wsc(torch.FloatTensor(X_train))).detach().numpy()[:,2].flatten(), range=(0, 1), bins=20, density=True, histtype=\"step\")\n",
    "    foo = ax[1].hist(nn.Softmax(dim=1)(wsc(torch.FloatTensor(X_test ))).detach().numpy()[:,2].flatten(), range=(0, 1), bins=20, density=True, histtype=\"step\")\n",
    "    \n",
    "    plt.savefig(\"../Pic_cached/SequentialTraining/WSL/training_WSC_\"+cutID+\"_\" + version + \"_\" + datatype +\"_trained.jpg\")\n",
    "    plt.close()\n",
    "    \n",
    "    return wsc.cpu().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass the first glith double WSL (pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the noise AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the noise WSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the BBH CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the BBH CNN WSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, let the test sample (WSL training sample) pass the procedure to get the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final label WSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v12'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First assume the CNN AE and the WSL is pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ncut = 5;\n",
    "cutList = {};\n",
    "\n",
    "max_glitch_L = 0.0024;\n",
    "min_glitch_L = 0.0008;\n",
    "cutList[\"glitch_L\"] = np.linspace(min_glitch_L, max_glitch_L, Ncut);\n",
    "\n",
    "max_glitch_H = 0.0024;\n",
    "min_glitch_H = 0.0004;\n",
    "cutList[\"glitch_H\"] = np.linspace(min_glitch_H, max_glitch_H, Ncut);\n",
    "\n",
    "max_bkg = 0.0018;\n",
    "min_bkg = 0.0008;\n",
    "cutList[\"noise\"] = np.linspace(min_bkg, max_bkg, Ncut);\n",
    "\n",
    "\n",
    "max_bbh_cnn = 0.0056\n",
    "min_bbh_cnn = 0.0045\n",
    "cutList['bbh_CNN'] = np.linspace(min_bbh_cnn, max_bbh_cnn, Ncut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0;\n",
    "\n",
    "ic = np.zeros(3, dtype=\"int\")\n",
    "\n",
    "# loop for only the cut in glitch, noise and bbh as it's not really meaningful to set cut in sg w/o new signals\n",
    "# ic[3] = Ncut-1;\n",
    "# ic[4] = Ncut-1;\n",
    "\n",
    "# listResult = {};\n",
    "# listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)), dtype=\"int\");\n",
    "# listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype)-1), len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype)-1), 2))\n",
    "\n",
    "for ic[0], ic[1], ic[2] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "# for ic[0], ic[1], ic[2], ic[3] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "    cnt += 1;\n",
    "    \n",
    "    # if cnt < 23:\n",
    "    #     continue\n",
    "    # elif cnt > 85:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in ['glitch', 'noise', 'bbh', 'sglf', 'sghf', 'glitch_oneglitch_onenoise']:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_fft_filtered = dataset_wsl_fft_collected\n",
    "    dataset_wsl_filtered = dataset_wsl_collected\n",
    "    \n",
    "    cutID = \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version\n",
    "    \n",
    "    \n",
    "\n",
    "    # --- Glitch AE+WSL ---\n",
    "\n",
    "    iPrev = 0\n",
    "    \n",
    "    previousStep = 'glitch';\n",
    "    # modelPrev_L = models['glitch_L']; # previous step AE\n",
    "    # modelPrev_H = models['glitch_H']; # previous step AE\n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    # dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    # dcd_L = modelPrev_L(torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))[1].detach().numpy();\n",
    "    # dcd_H = modelPrev_H(torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))[1].detach().numpy();\n",
    "    # dataset1 = dataset_wsl_fft_filtered[np.logical_and(np.var(dataset_wsl_fft_filtered[:,:101]-dcd_L, axis=1) >= cutList['glitch_L'][ic[0]],\n",
    "    #                                                np.var(dataset_wsl_fft_filtered[:,101:]-dcd_H, axis=1) >= cutList['glitch_H'][ic[1]])]\n",
    "    \n",
    "    # model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    # models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "    # model_L = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_L']\n",
    "    # model_H = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_H']\n",
    "\n",
    "    # model_L = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(['1','0']) + '_v2.json')['glitch_L']\n",
    "    # model_H = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(['1','0']) + '_v2.json')['glitch_H']\n",
    "    \n",
    "    # --- Train the glitch AE for L\n",
    "\n",
    "    \n",
    "    # models = {};\n",
    "    models[\"glitch_L\"] = torch.load('../Model_cached/1_det_L_glitch_trained_with_permutation_1.pt')\n",
    "    models[\"glitch_H\"] = torch.load('../Model_cached/1_det_H_glitch_trained_with_permutation_1.pt')\n",
    "\n",
    "    \n",
    "    # filter the data according to previous WSC\n",
    "    for dt in ['glitch', 'noise', 'bbh', 'sglf', 'sghf']:\n",
    "        dcd_L = models['glitch_L'](torch.FloatTensor(data_filtered[dt][:,:101]))[1].detach().numpy();\n",
    "        dcd_H = models['glitch_H'](torch.FloatTensor(data_filtered[dt][:,101:]))[1].detach().numpy();\n",
    "\n",
    "        err_L = np.var(dcd_H - data_filtered[dt][:,:101], axis = -1).flatten()\n",
    "        err_H = np.var(dcd_H - data_filtered[dt][:,101:], axis = -1).flatten()\n",
    "\n",
    "        data_filtered[dt] = data_filtered[dt][np.logical_and(err_L > cutList['glitch_L'][ic[0]], err_H > cutList['glitch_H'][ic[1]])]\n",
    "    \n",
    "    # dcd_L = nn.Sigmoid()(model_L(torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))).detach().numpy().flatten();\n",
    "    # dcd_H = nn.Sigmoid()(model_H(torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))).detach().numpy().flatten();\n",
    "\n",
    "    # dataset_wsl_fft_filtered = dataset_wsl_fft_filtered[np.logical_and(dcd_L > 0.5, dcd_H > 0.5)]\n",
    "    # dataset_wsl_filtered = dataset_wsl_filtered[np.logical_and(dcd_L > 0.5, dcd_H > 0.5)]\n",
    "\n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "    \n",
    "    # train the current step AE\n",
    "\n",
    "    if cnt != 1:\n",
    "        \n",
    "        print('Start noise AE')\n",
    "\n",
    "        print('Start training the noise AE, dataset 0 size {}'.format(len(data_filtered['noise'])))\n",
    "\n",
    "        currentStep = 'noise';\n",
    "        model = trainAE(data_filtered[currentStep], cutID, version, currentStep);\n",
    "        models[currentStep] = model;   \n",
    "        \n",
    "        print('Noise AE trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "        \n",
    "    # --- Noise AE+WSL ---\n",
    "    # Here's a cut scan for the noise AE\n",
    "\n",
    "#     print('Start noise WSL')\n",
    "\n",
    "#     iPrev = 1\n",
    "\n",
    "#     previousStep = 'noise';\n",
    "#     modelPrev = models[previousStep]; # previous step AE\n",
    "    \n",
    "#     # train the WSC according to previous AE's cut\n",
    "    \n",
    "#     dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "#     # dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "#     # dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[0]]]\n",
    "    \n",
    "#     dcd = modelPrev(torch.FloatTensor(dataset_wsl_fft_filtered))[1].detach().numpy();\n",
    "#     dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered-dcd, axis=1) >= cutList[previousStep][ic[0]]]\n",
    "    \n",
    "#     print('Start training the noise WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "#     model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "#     models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "#     # --- Here I just filter the noise sample ---\n",
    "\n",
    "#     data_noise_noiselike = data_filtered['noise'][nn.Sigmoid()(model(torch.FloatTensor(data_filtered['noise']))).detach().numpy().flatten() <= 0.5]\n",
    "#     signal_noise_like_args = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft_filtered))).detach().numpy().flatten() <= 0.5\n",
    "    \n",
    "#     print('Noise WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "\n",
    "#     # data_wsl_noiselike = {}\n",
    "\n",
    "#     # filter the data according to previous WSC\n",
    "#     # for j in range(iPrev, 4):\n",
    "#     #     dt = ind2datatype[j];\n",
    "#     #     dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "#     #     data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "#     #     data_noiselike[dt] = data_filtered[dt][dcd<=0.5]\n",
    "\n",
    "#     # dcd = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_filtered))).detach().numpy().flatten()\n",
    "#     # data_wsl_noiselike = dataset_wsl_collected[dcd <= 0.5].reshape(-1,2,200)\n",
    "    \n",
    "# #         # filter the data\n",
    "# #         for j in range(iPrev+1, 4):\n",
    "# #             dt = ind2datatype[j];\n",
    "# #             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "# #             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "#     # --- BBH CNN+WSL ---\n",
    "#     # Here's a cut scan for BBH CNN\n",
    "\n",
    "#     print('Start BBHCNN WSL')\n",
    "\n",
    "#     models['BBH_CNN'] = torch.load('../Model_cached/CNN_BBH/model_dep_3_chnl_4_btn_20_v4.pt')\n",
    "\n",
    "#     # Note that for CNN, we have to use the initial input data (2 * 200 in timeseries)\n",
    "\n",
    "#     dataset0 = data_noise_noiselike\n",
    "\n",
    "#     dcd = models['BBH_CNN'](torch.FloatTensor(dataset_wsl_filtered[signal_noise_like_args].reshape(-1,2,200))).detach().numpy()\n",
    "#     dataset1 = dataset_wsl_fft_filtered[signal_noise_like_args][np.mean(np.var(dataset_wsl_filtered[signal_noise_like_args].reshape(-1,2,200)-dcd, axis = 2), axis = 1) <= cutList['bbh_CNN'][ic[1]]]\n",
    "\n",
    "#     print('Start training the BBHCNN WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "    \n",
    "#     model = trainWSC(dataset0, dataset1, cutID, version, 'BBH_CNN')\n",
    "#     models['BBH_CNN_WSC'] = model\n",
    "\n",
    "#     print('BBHCNN WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "    \n",
    "    # --- Finalized WSL ---\n",
    "\n",
    "    print('Start final WSL for cnt = {}'.format(cnt))\n",
    "\n",
    "    dcd = {}\n",
    "    err = {}\n",
    "\n",
    "    dcd['glitch_L'] = (models['glitch_L'](torch.FloatTensor(dataset_wsl_fft_collected[:,:101])))[1].detach().numpy()\n",
    "    dcd['glitch_H'] = (models['glitch_H'](torch.FloatTensor(dataset_wsl_fft_collected[:,101:])))[1].detach().numpy()\n",
    "\n",
    "    err['glitch_L'] = np.var(dcd['glitch_L'] - dataset_wsl_fft_collected[:,:101], axis = -1).flatten()\n",
    "    err['glitch_H'] = np.var(dcd['glitch_H'] - dataset_wsl_fft_collected[:,101:], axis = -1).flatten()\n",
    "\n",
    "\n",
    "    dcd['noise'] = (models['noise'](torch.FloatTensor(dataset_wsl_fft_collected)))[1].detach().numpy()\n",
    "    err['noise'] = np.var(dcd['noise'] - dataset_wsl_fft_collected, axis = -1).flatten()\n",
    "\n",
    "    # for dt in ['noise_WSC', 'BBH_CNN_WSC']:\n",
    "    #     dcd[dt] = nn.Sigmoid()(models[dt](torch.FloatTensor(dataset_wsl_fft_collected))).detach().numpy().flatten()\n",
    "\n",
    "    glitch_pass = np.logical_and(err['glitch_L'] > cutList['glitch_L'][ic[0]], err['glitch_H'] > cutList['glitch_H'][ic[1]])\n",
    "\n",
    "    noise_pass = err['noise'] > cutList['noise'][ic[2]]\n",
    "\n",
    "    passed_args = np.logical_and(glitch_pass, noise_pass)\n",
    "\n",
    "    # dataset0 = np.concatenate((dataset_ae_fft['glitch'], dataset_ae_fft['noise']), axis = 0)\n",
    "\n",
    "    dataset0 = dataset_ae_fft['glitch_oneglitch_onenoise']\n",
    "\n",
    "    dataset1 = dataset_ae_fft['noise']\n",
    "\n",
    "    dataset2 = dataset_wsl_fft_collected[passed_args]\n",
    "\n",
    "    print('Start training the final WSL, dataset 0 size {}, {}, dataset 2 size {}'.format(len(dataset0), len(dataset1), len(dataset2)))\n",
    "\n",
    "    model = trainWSC_3class(dataset0, dataset1, dataset2, cutID, version, 'Final')\n",
    "\n",
    "    models['Final_WSC'] = model\n",
    "\n",
    "\n",
    "    print('Final WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "\n",
    "    torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version+\".json\")\n",
    "    print(models.keys())\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     dcd = {};\n",
    "#     err = {};\n",
    "#     ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "#     for datatype in list_datatype:\n",
    "#         dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "#         err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "        \n",
    "#     not_select = np.array([True]*len(testset));\n",
    "\n",
    "#     for iStep in range(len(list_datatype)):\n",
    "#         datatype = ind2datatype[iStep];\n",
    "#         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "#         ans[ind_pass] = iStep;\n",
    "#         not_select[ind_pass] = False;\n",
    "        \n",
    "#     ans[not_select] = -1;\n",
    "    \n",
    "#     listResult[\"cut\"][cnt] = ic;\n",
    "#     listResult[\"ans\"][cnt] = ans;\n",
    "    \n",
    "#     acc = np.zeros(len(ind2datatype));\n",
    "    \n",
    "#     for i in range(len(ind2datatype)):\n",
    "#         acc[i] = np.sum(np.logical_and(ans==i, correct_ans==i))/Nsample[ind2datatype[i]];\n",
    "        \n",
    "#     listResult[\"accuracy_4\"][cnt] = acc;\n",
    "    \n",
    "#     listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(Nsample[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "#                                      np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(Nsample[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "    # cnt += 1\n",
    "    print(cnt)\n",
    "    print(time.time() - t0)\n",
    "    \n",
    "# listResult[\"total_accuracy\"] = np.sum(listResult[\"ans\"]==correct_ans, axis=1)/len(testset);\n",
    "# torch.save(listResult, \"../data/SequentialTraining/training_performance_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logical_not(np.array([True, False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0;\n",
    "\n",
    "ic = np.zeros(2, dtype=\"int\")\n",
    "\n",
    "# loop for only the cut in glitch, noise and bbh as it's not really meaningful to set cut in sg w/o new signals\n",
    "# ic[3] = Ncut-1;\n",
    "# ic[4] = Ncut-1;\n",
    "\n",
    "# listResult = {};\n",
    "# listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)), dtype=\"int\");\n",
    "# listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype)-1), len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype)-1), 2))\n",
    "\n",
    "for ic[0], ic[1] in itertools.product(np.arange(Ncut), np.arange(Ncut)):\n",
    "# for ic[0], ic[1], ic[2], ic[3] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "    cnt += 1;\n",
    "    \n",
    "    if not np.all(ic == np.array([4,0])):\n",
    "        continue\n",
    "    # elif cnt > 85:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in ['glitch', 'noise', 'bbh', 'sglf', 'sghf']:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_fft_filtered = dataset_wsl_fft_collected\n",
    "    dataset_wsl_filtered = dataset_wsl_collected\n",
    "    \n",
    "    cutID = \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version\n",
    "    \n",
    "    \n",
    "\n",
    "        # --- Glitch AE+WSL ---\n",
    "\n",
    "    iPrev = 0\n",
    "    \n",
    "    previousStep = 'glitch';\n",
    "    # modelPrev_L = models['glitch_L']; # previous step AE\n",
    "    # modelPrev_H = models['glitch_H']; # previous step AE\n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    # dcd_L = modelPrev_L(torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))[1].detach().numpy();\n",
    "    # dcd_H = modelPrev_H(torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))[1].detach().numpy();\n",
    "    # dataset1 = dataset_wsl_fft_filtered[np.logical_and(np.var(dataset_wsl_fft_filtered[:,:101]-dcd_L, axis=1) >= cutList['glitch_L'][ic[0]],\n",
    "    #                                                np.var(dataset_wsl_fft_filtered[:,101:]-dcd_H, axis=1) >= cutList['glitch_H'][ic[1]])]\n",
    "    \n",
    "    # model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    # models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "    # model_L = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_L']\n",
    "    # model_H = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_H']\n",
    "\n",
    "    model_L = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(['1','0']) + '_v2.json')['glitch_L']\n",
    "    model_H = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(['1','0']) + '_v2.json')['glitch_H']\n",
    "    \n",
    "    # --- Train the glitch AE for L\n",
    "\n",
    "    models[previousStep+\"_WSC_L\"] = model_L\n",
    "    models[previousStep+\"_WSC_H\"] = model_H\n",
    "\n",
    "    \n",
    "    # filter the data according to previous WSC\n",
    "    for dt in ['glitch', 'noise', 'bbh', 'sglf', 'sghf']:\n",
    "        dcd_L = nn.Sigmoid()(model_L(torch.FloatTensor(data_filtered[dt][:,:101]))).detach().numpy().flatten();\n",
    "        dcd_H = nn.Sigmoid()(model_H(torch.FloatTensor(data_filtered[dt][:,101:]))).detach().numpy().flatten();\n",
    "        data_filtered[dt] = data_filtered[dt][np.logical_and(dcd_L >= 0, dcd_H > 0.5)]\n",
    "    \n",
    "    dcd_L = nn.Sigmoid()(model_L(torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))).detach().numpy().flatten();\n",
    "    dcd_H = nn.Sigmoid()(model_H(torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))).detach().numpy().flatten();\n",
    "\n",
    "    dataset_wsl_fft_filtered = dataset_wsl_fft_filtered[np.logical_and(dcd_L >= 0, dcd_H > 0.5)]\n",
    "    dataset_wsl_filtered = dataset_wsl_filtered[np.logical_and(dcd_L >= 0, dcd_H > 0.5)]\n",
    "\n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "    \n",
    "    # train the current step AE\n",
    "\n",
    "    print('Start noise AE')\n",
    "\n",
    "    print('Start training the noise WSL, dataset 0 size {}'.format(len(data_filtered['noise'])))\n",
    "\n",
    "    currentStep = 'noise';\n",
    "    model = trainAE(data_filtered[currentStep], cutID, version, currentStep);\n",
    "    models[currentStep] = model;   \n",
    "    \n",
    "    print('Noise AE trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "    \n",
    "    # --- Noise AE+WSL ---\n",
    "    # Here's a cut scan for the noise AE\n",
    "\n",
    "    print('Start noise WSL')\n",
    "\n",
    "    iPrev = 1\n",
    "\n",
    "    previousStep = 'noise';\n",
    "    modelPrev = models[previousStep]; # previous step AE\n",
    "    \n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    # dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "    # dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[0]]]\n",
    "    \n",
    "    dcd = modelPrev(torch.FloatTensor(dataset_wsl_fft_filtered))[1].detach().numpy();\n",
    "    dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered-dcd, axis=1) >= cutList[previousStep][ic[0]]]\n",
    "    \n",
    "    print('Start training the noise WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "    # --- Here I just filter the noise sample ---\n",
    "\n",
    "    data_noise_noiselike = data_filtered['noise'][nn.Sigmoid()(model(torch.FloatTensor(data_filtered['noise']))).detach().numpy().flatten() <= 0.5]\n",
    "    signal_noise_like_args = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft_filtered))).detach().numpy().flatten() <= 0.5\n",
    "    \n",
    "    print('Noise WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "\n",
    "    # data_wsl_noiselike = {}\n",
    "\n",
    "    # filter the data according to previous WSC\n",
    "    # for j in range(iPrev, 4):\n",
    "    #     dt = ind2datatype[j];\n",
    "    #     dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "    #     data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "    #     data_noiselike[dt] = data_filtered[dt][dcd<=0.5]\n",
    "\n",
    "    # dcd = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_filtered))).detach().numpy().flatten()\n",
    "    # data_wsl_noiselike = dataset_wsl_collected[dcd <= 0.5].reshape(-1,2,200)\n",
    "    \n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "    # --- BBH CNN+WSL ---\n",
    "    # Here's a cut scan for BBH CNN\n",
    "\n",
    "    print('Start BBHCNN WSL')\n",
    "\n",
    "    models['BBH_CNN'] = torch.load('../Model_cached/CNN_BBH/model_dep_3_chnl_4_btn_20_v4.pt')\n",
    "\n",
    "    # Note that for CNN, we have to use the initial input data (2 * 200 in timeseries)\n",
    "\n",
    "    dataset0 = data_noise_noiselike\n",
    "\n",
    "    dcd = models['BBH_CNN'](torch.FloatTensor(dataset_wsl_filtered[signal_noise_like_args].reshape(-1,2,200))).detach().numpy()\n",
    "    dataset1 = dataset_wsl_fft_filtered[signal_noise_like_args][np.mean(np.var(dataset_wsl_filtered[signal_noise_like_args].reshape(-1,2,200)-dcd, axis = 2), axis = 1) <= cutList['bbh_CNN'][ic[1]]]\n",
    "\n",
    "    print('Start training the BBHCNN WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "    \n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, 'BBH_CNN')\n",
    "    models['BBH_CNN_WSC'] = model\n",
    "\n",
    "    print('BBHCNN WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "    \n",
    "    # --- Finalized WSL ---\n",
    "\n",
    "    print('Start final WSL for cnt = {}'.format(cnt))\n",
    "\n",
    "    dcd = {}\n",
    "\n",
    "    dcd['glitch_WSC_L'] = nn.Sigmoid()(models['glitch_WSC_L'](torch.FloatTensor(dataset_wsl_fft_collected[:,:101]))).detach().numpy().flatten()\n",
    "    dcd['glitch_WSC_H'] = nn.Sigmoid()(models['glitch_WSC_H'](torch.FloatTensor(dataset_wsl_fft_collected[:,101:]))).detach().numpy().flatten()\n",
    "\n",
    "\n",
    "    for dt in ['noise_WSC', 'BBH_CNN_WSC']:\n",
    "        dcd[dt] = nn.Sigmoid()(models[dt](torch.FloatTensor(dataset_wsl_fft_collected))).detach().numpy().flatten()\n",
    "\n",
    "    glitch_pass = np.logical_and(dcd['glitch_WSC_L'] > 0.5, dcd['glitch_WSC_H'] > 0.5)\n",
    "\n",
    "    noise_pass = np.logical_or(dcd['noise_WSC'] > 0.5, np.logical_and(dcd['noise_WSC'] <= 0.5, dcd['BBH_CNN_WSC'] > 0.5))\n",
    "\n",
    "    passed_args = np.logical_and(glitch_pass, noise_pass)\n",
    "\n",
    "    # dataset0 = np.concatenate((dataset_ae_fft['glitch'], dataset_ae_fft['noise']), axis = 0)\n",
    "\n",
    "    dataset0 = data_filtered['noise']\n",
    "\n",
    "    dataset1 = dataset_wsl_fft_collected[passed_args]\n",
    "\n",
    "    print('Start training the final WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, 'Final')\n",
    "\n",
    "    models['Final_WSC'] = model\n",
    "\n",
    "\n",
    "    print('Final WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "\n",
    "    torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    print(models.keys())\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     dcd = {};\n",
    "#     err = {};\n",
    "#     ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "#     for datatype in list_datatype:\n",
    "#         dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "#         err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "        \n",
    "#     not_select = np.array([True]*len(testset));\n",
    "\n",
    "#     for iStep in range(len(list_datatype)):\n",
    "#         datatype = ind2datatype[iStep];\n",
    "#         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "#         ans[ind_pass] = iStep;\n",
    "#         not_select[ind_pass] = False;\n",
    "        \n",
    "#     ans[not_select] = -1;\n",
    "    \n",
    "#     listResult[\"cut\"][cnt] = ic;\n",
    "#     listResult[\"ans\"][cnt] = ans;\n",
    "    \n",
    "#     acc = np.zeros(len(ind2datatype));\n",
    "    \n",
    "#     for i in range(len(ind2datatype)):\n",
    "#         acc[i] = np.sum(np.logical_and(ans==i, correct_ans==i))/Nsample[ind2datatype[i]];\n",
    "        \n",
    "#     listResult[\"accuracy_4\"][cnt] = acc;\n",
    "    \n",
    "#     listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(Nsample[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "#                                      np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(Nsample[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "    # cnt += 1\n",
    "    print(cnt)\n",
    "    print(time.time() - t0)\n",
    "    \n",
    "# listResult[\"total_accuracy\"] = np.sum(listResult[\"ans\"]==correct_ans, axis=1)/len(testset);\n",
    "# torch.save(listResult, \"../data/SequentialTraining/training_performance_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not np.all(ic == np.array([4,0,1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0;\n",
    "\n",
    "ic = np.zeros(4, dtype=\"int\")\n",
    "\n",
    "# loop for only the cut in glitch, noise and bbh as it's not really meaningful to set cut in sg w/o new signals\n",
    "# ic[3] = Ncut-1;\n",
    "# ic[4] = Ncut-1;\n",
    "\n",
    "# listResult = {};\n",
    "# listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)), dtype=\"int\");\n",
    "# listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype)-1), len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype)-1), len(list_datatype)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype)-1), 2))\n",
    "\n",
    "for ic[0], ic[1], ic[2], ic[3] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "# for ic[0], ic[1], ic[2], ic[3] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "    cnt += 1;\n",
    "    \n",
    "    print(ic)\n",
    "    \n",
    "    if not np.all(ic == np.array([4,0,1,0])):\n",
    "        continue\n",
    "    # elif cnt > 85:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in ['glitch', 'noise', 'bbh', 'sglf', 'sghf']:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_fft_filtered = dataset_wsl_fft_collected\n",
    "    dataset_wsl_filtered = dataset_wsl_collected\n",
    "    \n",
    "    cutID = \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version\n",
    "    \n",
    "    \n",
    "    print('Starting training on ic = {}'.format(''.join(str(ic[j] for j in range(4)))))\n",
    "    \n",
    "        # --- Glitch AE+WSL ---\n",
    "\n",
    "    iPrev = 0\n",
    "    \n",
    "    previousStep = 'glitch';\n",
    "    # modelPrev_L = models['glitch_L']; # previous step AE\n",
    "    # modelPrev_H = models['glitch_H']; # previous step AE\n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    # dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    # dcd_L = modelPrev_L(torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))[1].detach().numpy();\n",
    "    # dcd_H = modelPrev_H(torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))[1].detach().numpy();\n",
    "    # dataset1 = dataset_wsl_fft_filtered[np.logical_and(np.var(dataset_wsl_fft_filtered[:,:101]-dcd_L, axis=1) >= cutList['glitch_L'][ic[0]],\n",
    "    #                                                np.var(dataset_wsl_fft_filtered[:,101:]-dcd_H, axis=1) >= cutList['glitch_H'][ic[1]])]\n",
    "    \n",
    "    # model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    # models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "    # model_L = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_L']\n",
    "    # model_H = torch.load('../Model_cached/WSL_Chia-Jui_glitches/two_WSL_for_Chia-Jui_noise_cutscheme' + ''.join(str(ic[j]) for j in range(2)) + '_v2.json')['glitch_H']\n",
    "    \n",
    "    model = trainAE_Onedetector(dataset_ae_fft['glitch'][:,:101], cutID, version, 'glitch')\n",
    "    models['glitch_L'] = model\n",
    "    \n",
    "    model = trainAE_Onedetector(dataset_ae_fft['glitch'][:,101:], cutID, version, 'glitch')\n",
    "    models['glitch_H'] = model\n",
    "    \n",
    "    dcd = models['glitch_L'](torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))[1].detach().numpy()\n",
    "    \n",
    "    dataset0 = data_filtered['glitch'][:,:101]\n",
    "    dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered[:,:101]-dcd, axis=1) > cutList['glitch_L'][ic[2]]][:,:101]\n",
    "    \n",
    "    model_L = trainWSC_Onedetector(dataset0, dataset1, cutID, version, 'glitch')\n",
    "    models[previousStep+\"_WSC_L\"] = model_L\n",
    "    \n",
    "    \n",
    "    dcd = models['glitch_H'](torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))[1].detach().numpy()\n",
    "    \n",
    "    dataset0 = data_filtered['glitch'][:,101:]\n",
    "    dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered[:,101:]-dcd, axis=1) > cutList['glitch_H'][ic[3]]][:,101:]\n",
    "    \n",
    "    model_H = trainWSC_Onedetector(dataset0, dataset1, cutID, version, 'glitch')\n",
    "    models[previousStep+\"_WSC_H\"] = model_H\n",
    "\n",
    "    \n",
    "    # filter the data according to previous WSC\n",
    "    for dt in ['glitch', 'noise', 'bbh', 'sglf', 'sghf']:\n",
    "        dcd_L = nn.Sigmoid()(model_L(torch.FloatTensor(data_filtered[dt][:,:101]))).detach().numpy().flatten();\n",
    "        dcd_H = nn.Sigmoid()(model_H(torch.FloatTensor(data_filtered[dt][:,101:]))).detach().numpy().flatten();\n",
    "        data_filtered[dt] = data_filtered[dt][np.logical_and(dcd_L > 0.5, dcd_H > 0.5)]\n",
    "    \n",
    "    dcd_L = nn.Sigmoid()(model_L(torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))).detach().numpy().flatten();\n",
    "    dcd_H = nn.Sigmoid()(model_H(torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))).detach().numpy().flatten();\n",
    "\n",
    "    dataset_wsl_fft_filtered = dataset_wsl_fft_filtered[np.logical_and(dcd_L > 0.5, dcd_H > 0.5)]\n",
    "    dataset_wsl_filtered = dataset_wsl_filtered[np.logical_and(dcd_L > 0.5, dcd_H > 0.5)]\n",
    "\n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "    \n",
    "    # train the current step AE\n",
    "\n",
    "    print('Start noise AE')\n",
    "\n",
    "    print('Start training the noise WSL, dataset 0 size {}'.format(len(data_filtered['noise'])))\n",
    "\n",
    "    currentStep = 'noise';\n",
    "    model = trainAE(data_filtered[currentStep], cutID, version, currentStep);\n",
    "    models[currentStep] = model;   \n",
    "    \n",
    "    print('Noise AE trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "    \n",
    "    # --- Noise AE+WSL ---\n",
    "    # Here's a cut scan for the noise AE\n",
    "\n",
    "    print('Start noise WSL')\n",
    "\n",
    "    iPrev = 1\n",
    "\n",
    "    previousStep = 'noise';\n",
    "    modelPrev = models[previousStep]; # previous step AE\n",
    "    \n",
    "    # train the WSC according to previous AE's cut\n",
    "    \n",
    "    dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "    \n",
    "    # dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "    # dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[0]]]\n",
    "    \n",
    "    dcd = modelPrev(torch.FloatTensor(dataset_wsl_fft_filtered))[1].detach().numpy();\n",
    "    dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered-dcd, axis=1) >= cutList[previousStep][ic[0]]]\n",
    "    \n",
    "    print('Start training the noise WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, previousStep)\n",
    "    models[previousStep+\"_WSC\"] = model;\n",
    "\n",
    "    # --- Here I just filter the noise sample ---\n",
    "\n",
    "    data_noise_noiselike = data_filtered['noise'][nn.Sigmoid()(model(torch.FloatTensor(data_filtered['noise']))).detach().numpy().flatten() <= 0.5]\n",
    "    signal_noise_like_args = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft_filtered))).detach().numpy().flatten() <= 0.5\n",
    "    \n",
    "    print('Noise WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "\n",
    "    # data_wsl_noiselike = {}\n",
    "\n",
    "    # filter the data according to previous WSC\n",
    "    # for j in range(iPrev, 4):\n",
    "    #     dt = ind2datatype[j];\n",
    "    #     dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "    #     data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "    #     data_noiselike[dt] = data_filtered[dt][dcd<=0.5]\n",
    "\n",
    "    # dcd = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_filtered))).detach().numpy().flatten()\n",
    "    # data_wsl_noiselike = dataset_wsl_collected[dcd <= 0.5].reshape(-1,2,200)\n",
    "    \n",
    "#         # filter the data\n",
    "#         for j in range(iPrev+1, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "#             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "    # --- BBH CNN+WSL ---\n",
    "    # Here's a cut scan for BBH CNN\n",
    "\n",
    "    print('Start BBHCNN WSL')\n",
    "\n",
    "    models['BBH_CNN'] = torch.load('../Model_cached/CNN_BBH/model_dep_3_chnl_4_btn_20_v4.pt')\n",
    "\n",
    "    # Note that for CNN, we have to use the initial input data (2 * 200 in timeseries)\n",
    "\n",
    "    dataset0 = data_noise_noiselike\n",
    "\n",
    "    dcd = models['BBH_CNN'](torch.FloatTensor(dataset_wsl_filtered[signal_noise_like_args].reshape(-1,2,200))).detach().numpy()\n",
    "    dataset1 = dataset_wsl_fft_filtered[signal_noise_like_args][np.mean(np.var(dataset_wsl_filtered[signal_noise_like_args].reshape(-1,2,200)-dcd, axis = 2), axis = 1) <= cutList['bbh_CNN'][ic[1]]]\n",
    "\n",
    "    print('Start training the BBHCNN WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "    \n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, 'BBH_CNN')\n",
    "    models['BBH_CNN_WSC'] = model\n",
    "\n",
    "    print('BBHCNN WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "    \n",
    "    # --- Finalized WSL ---\n",
    "\n",
    "    print('Start final WSL for cnt = {}'.format(cnt))\n",
    "\n",
    "    dcd = {}\n",
    "\n",
    "    dcd['glitch_WSC_L'] = nn.Sigmoid()(models['glitch_WSC_L'](torch.FloatTensor(dataset_wsl_fft_collected[:,:101]))).detach().numpy().flatten()\n",
    "    dcd['glitch_WSC_H'] = nn.Sigmoid()(models['glitch_WSC_H'](torch.FloatTensor(dataset_wsl_fft_collected[:,101:]))).detach().numpy().flatten()\n",
    "\n",
    "\n",
    "    for dt in ['noise_WSC', 'BBH_CNN_WSC']:\n",
    "        dcd[dt] = nn.Sigmoid()(models[dt](torch.FloatTensor(dataset_wsl_fft_collected))).detach().numpy().flatten()\n",
    "\n",
    "    glitch_pass = np.logical_and(dcd['glitch_WSC_L'] > 0.5, dcd['glitch_WSC_H'] > 0.5)\n",
    "\n",
    "    noise_pass = np.logical_or(dcd['noise_WSC'] > 0.5, np.logical_and(dcd['noise_WSC'] <= 0.5, dcd['BBH_CNN_WSC'] > 0.5))\n",
    "\n",
    "    passed_args = np.logical_and(glitch_pass, noise_pass)\n",
    "\n",
    "    # dataset0 = np.concatenate((dataset_ae_fft['glitch'], dataset_ae_fft['noise']), axis = 0)\n",
    "\n",
    "    dataset0 = data_filtered['noise']\n",
    "\n",
    "    dataset1 = dataset_wsl_fft_collected[passed_args]\n",
    "\n",
    "    print('Start training the final WSL, dataset 0 size {}, dataset 1 size {}'.format(len(dataset0), len(dataset1)))\n",
    "\n",
    "    model = trainWSC(dataset0, dataset1, cutID, version, 'Final')\n",
    "\n",
    "    models['Final_WSC'] = model\n",
    "\n",
    "\n",
    "    print('Final WSL trained completed, this epoch is using time {}'.format(str(time.time() - t0)))\n",
    "\n",
    "    torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    print(models.keys())\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     dcd = {};\n",
    "#     err = {};\n",
    "#     ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "#     for datatype in list_datatype:\n",
    "#         dcd[datatype] = models[datatype](torch.FloatTensor(testset))[1].detach().numpy()\n",
    "#         err[datatype] = np.var(testset-dcd[datatype], axis=1)\n",
    "        \n",
    "#     not_select = np.array([True]*len(testset));\n",
    "\n",
    "#     for iStep in range(len(list_datatype)):\n",
    "#         datatype = ind2datatype[iStep];\n",
    "#         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "#         ans[ind_pass] = iStep;\n",
    "#         not_select[ind_pass] = False;\n",
    "        \n",
    "#     ans[not_select] = -1;\n",
    "    \n",
    "#     listResult[\"cut\"][cnt] = ic;\n",
    "#     listResult[\"ans\"][cnt] = ans;\n",
    "    \n",
    "#     acc = np.zeros(len(ind2datatype));\n",
    "    \n",
    "#     for i in range(len(ind2datatype)):\n",
    "#         acc[i] = np.sum(np.logical_and(ans==i, correct_ans==i))/Nsample[ind2datatype[i]];\n",
    "        \n",
    "#     listResult[\"accuracy_4\"][cnt] = acc;\n",
    "    \n",
    "#     listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(Nsample[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "#                                      np.sum(acc[datatype2ind[dtype]]*Nsample[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(Nsample[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "    # cnt += 1\n",
    "    print(cnt)\n",
    "    print(time.time() - t0)\n",
    "    \n",
    "# listResult[\"total_accuracy\"] = np.sum(listResult[\"ans\"]==correct_ans, axis=1)/len(testset);\n",
    "# torch.save(listResult, \"../data/SequentialTraining/training_performance_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.mean(np.var(dataset_wsl_collected[signal_noise_like_args].reshape(-1,2,200)-dcd, axis = 2), axis = 1) <= cutList['bbh_CNN'][ic[1]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_noise_like_args = np.argwhere(nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_filtered))).detach().numpy().flatten() <= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_noise_like_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_collected[signal_noise_like_args].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the pipeline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft_collected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_wsl.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans = np.hstack(([0]*N_wsl['glitch'], [1]*N_wsl['noise'], [2]*N_wsl['bbh'], [3]*N_wsl['sglf'], [4]*N_wsl['sghf']))\n",
    "correct_ans_withoutsignal = np.hstack(([0]*N_wsl['glitch'], [1]*N_wsl['noise'], [2]*(N_wsl['bbh']+N_wsl['sglf']+N_wsl['sghf'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ans_withoutsignal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_datatype_withoutsignal = ['glitch','noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = dataset_wsl_fft_collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult = {};\n",
    "listResult[\"cut\"] = np.empty((Ncut**3, 3), dtype=\"int\");\n",
    "listResult[\"ans\"] = np.empty((Ncut**3, len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(list_datatype_withoutsignal)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), 2))\n",
    "cnt = 0\n",
    "ic_withoutsignal = np.zeros(3, dtype = int)\n",
    "\n",
    "listResult[\"FPR\"] = np.empty((Ncut**3,1))\n",
    "\n",
    "\n",
    "for ic_withoutsignal[0], ic_withoutsignal[1], ic_withoutsignal[2] in itertools.product(np.arange(Ncut), np.arange(Ncut), np.arange(Ncut)):\n",
    "\n",
    "    \n",
    "    # if cnt < 86:\n",
    "    #     continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in sequence:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_filtered = dataset_wsl_fft_collected\n",
    "        \n",
    "#     for iPrev in range(3):\n",
    "#         previousStep = ind2datatype[iPrev];\n",
    "#         modelPrev = models[previousStep]; # previous step AE\n",
    "        \n",
    "#         # train the WSC according to previous AE's cut\n",
    "        \n",
    "#         dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "#         dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset_wsl_filtered))[1].detach().numpy();\n",
    "#         dataset1 = dataset_wsl_filtered[np.var(dataset_wsl_filtered-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         model = trainWSC(dataset0, dataset1, cutID)\n",
    "#         models[previousStep+\"_WSC\"] = model;\n",
    "        \n",
    "#         # filter the data according to previous WSC\n",
    "#         for j in range(iPrev, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "#             data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "        \n",
    "# #         # filter the data\n",
    "# #         for j in range(iPrev+1, 4):\n",
    "# #             dt = ind2datatype[j];\n",
    "# #             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "# #             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "#         # train the current step AE\n",
    "#         currentStep = ind2datatype[iPrev+1];\n",
    "#         model = trainAE(data_filtered[currentStep], cutID);\n",
    "#         models[currentStep] = model;\n",
    "        \n",
    "#     torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version+\".json\")\n",
    "#     print(models.keys())\n",
    "    models = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic_withoutsignal[j]) for j in range(3)) + \"_\"+version+\".json\")\n",
    "    # print(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    dcd = {};\n",
    "    err = {};\n",
    "    ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "    # dcd['glitch_L'] = nn.Sigmoid()(models['glitch_WSC_L'](torch.FloatTensor(testset[:,:101]))).detach().numpy().reshape(-1)\n",
    "    # dcd['glitch_H'] = nn.Sigmoid()(models['glitch_WSC_H'](torch.FloatTensor(testset[:,101:]))).detach().numpy().reshape(-1)\n",
    "    # dcd['noise_WSC'] = nn.Sigmoid()(models[\"noise_WSC\"](torch.FloatTensor(testset))).detach().numpy().reshape(-1)\n",
    "    # dcd['Final_WSC'] = nn.Sigmoid()(models[\"Final_WSC\"](torch.FloatTensor(testset))).detach().numpy().reshape(-1)\n",
    "    \n",
    "    dcd['Final_WSC'] = nn.Softmax(dim=1)(models[\"Final_WSC\"](torch.FloatTensor(testset)))[:,2].detach().numpy()\n",
    "    \n",
    "    not_select = np.array([True]*len(testset));\n",
    "\n",
    "    # for iStep in range(len(list_datatype_withoutsignal)):\n",
    "    #     datatype = ind2datatype[iStep];\n",
    "    #     if datatype == 'sg':\n",
    "    #         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "    #     else:\n",
    "    #         ind_pass = np.logical_and(not_select, dcd[datatype] <= 0.5);\n",
    "    #     ans[ind_pass] = iStep;\n",
    "    #     not_select[ind_pass] = False;\n",
    "        \n",
    "    # Pass glitch first\n",
    "    \n",
    "    # datatype = 'glitch'\n",
    "    # ind_pass = np.logical_and(not_select, np.logical_or(dcd['glitch_L'] <= 0.5, dcd['glitch_H'] <= 0.5))\n",
    "    # ans[ind_pass] = 0;\n",
    "    # not_select[ind_pass] = False;\n",
    "    # print(dcd['glitch'])\n",
    "    \n",
    "    # Leftover are noise and signals\n",
    "    \n",
    "    # datatype = 'noise_WSC'\n",
    "    datatype = 'Final_WSC'\n",
    "    ind_pass = np.logical_and(not_select, dcd[datatype] <= np.sort(dcd[datatype][-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:][not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:]])[int(0.1 * (np.sum(not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:])))])\n",
    "    # ind_pass = np.logical_and(not_select, err[datatype] <= np.sort(err[datatype][-N_wsl['bbh']-N_wsl['sg']:])[int(0.1 * (N_wsl['bbh']+N_wsl['sg']))])\n",
    "    noise_number = np.sum(np.logical_and(not_select, correct_ans_withoutsignal == 1))\n",
    "    passed_noise_number = noise_number - np.sum(np.logical_and(ind_pass, correct_ans_withoutsignal == 1))\n",
    "    ans[ind_pass] = 1;\n",
    "    not_select[ind_pass] = False;\n",
    "    \n",
    "    ans[not_select] = -1\n",
    "    \n",
    "    FPR = passed_noise_number / noise_number\n",
    "    \n",
    "    # print(dcd['noise'])\n",
    "    print('For cnt = {}, totally {} noise events passed the glitch WSL, and {} noise events within the threshold for TPR=0.9'.format(cnt, noise_number, passed_noise_number))\n",
    "    listResult['FPR'][cnt] = FPR\n",
    "    listResult['cut'][cnt] = ic_withoutsignal\n",
    "    listResult['ans'][cnt] = ans\n",
    "        \n",
    "    # ans[not_select] = -1;\n",
    "\n",
    "    # listResult[\"cut\"][cnt] = ic_withoutsignal;\n",
    "    # listResult[\"ans\"][cnt] = ans;\n",
    "\n",
    "    # acc = np.zeros(len(ind2datatype));\n",
    "\n",
    "    # for i in range(len(ind2datatype)):\n",
    "    #     acc[i] = np.sum(np.logical_and(ans==i, correct_ans_withoutsignal==i))/N_wsl[ind2datatype[i]];\n",
    "        \n",
    "    # listResult[\"accuracy_4\"][cnt] = acc;\n",
    "\n",
    "    # listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(N_wsl[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "    #                                     np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "\n",
    "    cnt += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult = {};\n",
    "listResult[\"cut\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(list_datatype_withoutsignal)), dtype=\"int\");\n",
    "listResult[\"ans\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(testset)), dtype=\"int\");\n",
    "# listResult[\"accuracy_4\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), len(list_datatype_withoutsignal)))\n",
    "# listResult[\"accuracy_2\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)), 2))\n",
    "cnt = 0\n",
    "ic_withoutsignal = np.zeros(2, dtype = int)\n",
    "\n",
    "listResult[\"FPR\"] = np.empty((Ncut**(len(list_datatype_withoutsignal)),1))\n",
    "\n",
    "\n",
    "for ic_withoutsignal[0], ic_withoutsignal[1] in itertools.product(np.arange(Ncut), np.arange(Ncut)):\n",
    "\n",
    "    \n",
    "    # if cnt < 86:\n",
    "    #     continue\n",
    "    \n",
    "    if not np.all(ic_withoutsignal == np.array([4,0])):\n",
    "        continue\n",
    "    \n",
    "    t0 = time.time()\n",
    "    data_filtered = {};\n",
    "    for dt in sequence:\n",
    "        data_filtered[dt] = dataset_ae_fft[dt]\n",
    "#     data_filtered[\"noise\"] = bkg_fft;\n",
    "#     data_filtered[\"bbh\"] = bbh_fft;\n",
    "#     data_filtered[\"sg\"] = sg_fft;\n",
    "\n",
    "    dataset_wsl_filtered = dataset_wsl_fft_collected\n",
    "        \n",
    "#     for iPrev in range(3):\n",
    "#         previousStep = ind2datatype[iPrev];\n",
    "#         modelPrev = models[previousStep]; # previous step AE\n",
    "        \n",
    "#         # train the WSC according to previous AE's cut\n",
    "        \n",
    "#         dataset0 = data_filtered[previousStep] # here they haven't been updated yet\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset0))[1].detach().numpy();\n",
    "#         dataset1 = dataset0[np.var(dataset0-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         dcd = modelPrev(torch.FloatTensor(dataset_wsl_filtered))[1].detach().numpy();\n",
    "#         dataset1 = dataset_wsl_filtered[np.var(dataset_wsl_filtered-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]\n",
    "        \n",
    "#         model = trainWSC(dataset0, dataset1, cutID)\n",
    "#         models[previousStep+\"_WSC\"] = model;\n",
    "        \n",
    "#         # filter the data according to previous WSC\n",
    "#         for j in range(iPrev, 4):\n",
    "#             dt = ind2datatype[j];\n",
    "#             dcd = nn.Sigmoid()(model(torch.FloatTensor(data_filtered[dt]))).detach().numpy().flatten();\n",
    "#             data_filtered[dt] = data_filtered[dt][dcd>0.5]\n",
    "        \n",
    "# #         # filter the data\n",
    "# #         for j in range(iPrev+1, 4):\n",
    "# #             dt = ind2datatype[j];\n",
    "# #             dcd = modelPrev(torch.FloatTensor(data_filtered[dt]))[1].detach().numpy()\n",
    "# #             data_filtered[dt] = data_filtered[dt][np.var(data_filtered[dt]-dcd, axis=1) >= cutList[previousStep][ic[iPrev]]]            \n",
    "        \n",
    "#         # train the current step AE\n",
    "#         currentStep = ind2datatype[iPrev+1];\n",
    "#         model = trainAE(data_filtered[currentStep], cutID);\n",
    "#         models[currentStep] = model;\n",
    "        \n",
    "#     torch.save(models, \"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(3)) + \"_\"+version+\".json\")\n",
    "#     print(models.keys())\n",
    "    models = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic_withoutsignal[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    # print(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(str(ic[j]) for j in range(2)) + \"_\"+version+\".json\")\n",
    "    dcd = {};\n",
    "    err = {};\n",
    "    ans = np.zeros(len(testset), dtype=\"int\")\n",
    "    \n",
    "    dcd['glitch_L'] = nn.Sigmoid()(models['glitch_WSC_L'](torch.FloatTensor(testset[:,:101]))).detach().numpy().reshape(-1)\n",
    "    dcd['glitch_H'] = nn.Sigmoid()(models['glitch_WSC_H'](torch.FloatTensor(testset[:,101:]))).detach().numpy().reshape(-1)\n",
    "    dcd['noise_WSC'] = nn.Sigmoid()(models[\"noise_WSC\"](torch.FloatTensor(testset))).detach().numpy().reshape(-1)\n",
    "    dcd['Final_WSC'] = nn.Sigmoid()(models[\"Final_WSC\"](torch.FloatTensor(testset))).detach().numpy().reshape(-1)\n",
    "    \n",
    "    not_select = np.array([True]*len(testset));\n",
    "\n",
    "    # for iStep in range(len(list_datatype_withoutsignal)):\n",
    "    #     datatype = ind2datatype[iStep];\n",
    "    #     if datatype == 'sg':\n",
    "    #         ind_pass = np.logical_and(not_select, err[datatype] <= cutList[datatype][ic[iStep]]);\n",
    "    #     else:\n",
    "    #         ind_pass = np.logical_and(not_select, dcd[datatype] <= 0.5);\n",
    "    #     ans[ind_pass] = iStep;\n",
    "    #     not_select[ind_pass] = False;\n",
    "        \n",
    "    # Pass glitch first\n",
    "    \n",
    "    datatype = 'glitch'\n",
    "    ind_pass = np.logical_and(not_select, np.logical_or(dcd['glitch_L'] < 0, dcd['glitch_H'] <= 0.5))\n",
    "    ans[ind_pass] = 0;\n",
    "    not_select[ind_pass] = False;\n",
    "    # print(dcd['glitch'])\n",
    "    \n",
    "    # Leftover are noise and signals\n",
    "    \n",
    "    # datatype = 'noise_WSC'\n",
    "    datatype = 'Final_WSC'\n",
    "    ind_pass = np.logical_and(not_select, dcd[datatype] <= np.sort(dcd[datatype][-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:][not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:]])[int(0.1 * (np.sum(not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:])))])\n",
    "    # ind_pass = np.logical_and(not_select, err[datatype] <= np.sort(err[datatype][-N_wsl['bbh']-N_wsl['sg']:])[int(0.1 * (N_wsl['bbh']+N_wsl['sg']))])\n",
    "    noise_number = np.sum(np.logical_and(not_select, correct_ans_withoutsignal == 1))\n",
    "    passed_noise_number = noise_number - np.sum(np.logical_and(ind_pass, correct_ans_withoutsignal == 1))\n",
    "    ans[ind_pass] = 1;\n",
    "    not_select[ind_pass] = False;\n",
    "    \n",
    "    ans[not_select] = -1\n",
    "    \n",
    "    FPR = passed_noise_number / noise_number\n",
    "    \n",
    "    # print(dcd['noise'])\n",
    "    print('For cnt = {}, totally {} noise events passed the glitch WSL, and {} noise events within the threshold for TPR=0.9'.format(cnt, noise_number, passed_noise_number))\n",
    "    listResult['FPR'][cnt] = FPR\n",
    "    listResult['cut'][cnt] = ic_withoutsignal\n",
    "    listResult['ans'][cnt] = ans\n",
    "        \n",
    "    # ans[not_select] = -1;\n",
    "\n",
    "    # listResult[\"cut\"][cnt] = ic_withoutsignal;\n",
    "    # listResult[\"ans\"][cnt] = ans;\n",
    "\n",
    "    # acc = np.zeros(len(ind2datatype));\n",
    "\n",
    "    # for i in range(len(ind2datatype)):\n",
    "    #     acc[i] = np.sum(np.logical_and(ans==i, correct_ans_withoutsignal==i))/N_wsl[ind2datatype[i]];\n",
    "        \n",
    "    # listResult[\"accuracy_4\"][cnt] = acc;\n",
    "\n",
    "    # listResult[\"accuracy_2\"][cnt] = [ np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"glitch\", \"noise\"])/np.sum(N_wsl[dtype] for dtype in [\"glitch\", \"noise\"]), \n",
    "    #                                     np.sum(acc[datatype2ind[dtype]]*N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])/np.sum(N_wsl[dtype] for dtype in [\"bbh\", \"sg\"])]\n",
    "\n",
    "    cnt += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_wsl['noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(listResult['FPR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listResult['cut'][29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"104\" + \"_\"+version+\".json\")['Final_WSC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.FloatTensor(dataset_wsl_fft['noise'])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_bkg = nn.Softmax(dim = 1)(model(torch.FloatTensor(dataset_wsl_fft['noise'])))[:,2].detach().numpy()\n",
    "score_glitch = nn.Softmax(dim = 1)(model(torch.FloatTensor(dataset_wsl_fft['glitch'])))[:,2].detach().numpy()\n",
    "\n",
    "for type in ['bbh', 'sglf', 'sghf']:\n",
    "    \n",
    "    plt.hist(score_bkg, bins = 50, range = (0,1), histtype = 'step', density = True, label = 'noise')\n",
    "    plt.hist(score_glitch, bins = 50, range = (0,1), histtype = 'step', density = True, label = 'glitch')\n",
    "    \n",
    "    for snr in ['5-12', '12-24', '24-48', '48-96']:\n",
    "        \n",
    "        score = nn.Softmax(dim = 1)(model(torch.FloatTensor(dataset_wsl_fft[type+snr])))[:,2].detach().numpy()\n",
    "        plt.hist(score, bins = 50, range = (0,1), histtype = 'step', density = True, label = type+snr)\n",
    "        \n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPR_list = {}\n",
    "\n",
    "score_bkg = nn.Softmax(dim = 1)(model(torch.FloatTensor(dataset_wsl_fft['noise'])))[:,2].detach().numpy()\n",
    "score_glitch = nn.Softmax(dim = 1)(model(torch.FloatTensor(dataset_wsl_fft['glitch'])))[:,2].detach().numpy()\n",
    "\n",
    "for type in ['bbh', 'sglf', 'sghf']:\n",
    "    \n",
    "    for snr in ['5-12', '12-24', '24-48', '48-96']:\n",
    "        \n",
    "        score = nn.Softmax(dim = 1)(model(torch.FloatTensor(dataset_wsl_fft[type+snr])))[:,2].detach().numpy()\n",
    "        \n",
    "        noise_events_passed = np.sum(score_bkg > np.sort(score)[int(0.1 * len(score))])\n",
    "        glitch_events_passed = np.sum(score_glitch > np.sort(score)[int(0.1 * len(score))])\n",
    "        \n",
    "        FPR = (noise_events_passed + glitch_events_passed) / (len(score_bkg) + len(score_glitch))\n",
    "        \n",
    "        FPR_list[type+snr] = FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPR_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_list = ['5-12', '12-24', '24-48', '48-96']\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "for type in ['bbh', 'sglf', 'sghf']:\n",
    "    \n",
    "    FPR_data = np.empty(4)\n",
    "    \n",
    "    for i in range(4):\n",
    "        \n",
    "        FPR_data[i] = FPR_list[type+snr_list[i]]\n",
    "        \n",
    "    central_point = np.array([9,18,36,72])\n",
    "        \n",
    "    # plt.scatter(['5-12','12-24','24-48','48-96'], FPR_data, label = type)\n",
    "    plt.plot(['5-12','12-24','24-48','48-96'], FPR_data, marker = 'o', label = type)\n",
    "    \n",
    "    \n",
    "# plt.xlim(0,96)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlabel('SNR range')\n",
    "plt.ylabel('FPR')\n",
    "\n",
    "plt.title('FPR distribution versus SNR, TPR fixed at 0.9')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR_list = {}\n",
    "\n",
    "score_bkg = nn.Softmax(dim = 1)(model(torch.FloatTensor(dataset_wsl_fft['noise'])))[:,2].detach().numpy()\n",
    "score_glitch = nn.Softmax(dim = 1)(model(torch.FloatTensor(dataset_wsl_fft['glitch'])))[:,2].detach().numpy()\n",
    "\n",
    "score_bkgevents = np.concatenate((score_bkg, score_glitch))\n",
    "\n",
    "threshold = np.sort(score_bkgevents)[-int(0.001 * len(score_bkgevents))]\n",
    "\n",
    "for type in ['bbh', 'sglf', 'sghf']:\n",
    "    \n",
    "    for snr in ['5-12', '12-24', '24-48', '48-96']:\n",
    "        \n",
    "        score = nn.Softmax(dim = 1)(model(torch.FloatTensor(dataset_wsl_fft[type+snr])))[:,2].detach().numpy()\n",
    "        \n",
    "        TPR = np.sum(score > threshold) / len(score)\n",
    "        \n",
    "        TPR_list[type+snr] = TPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_list = ['5-12', '12-24', '24-48', '48-96']\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "for type in ['bbh', 'sglf', 'sghf']:\n",
    "    \n",
    "    TPR_data = np.empty(4)\n",
    "    \n",
    "    for i in range(4):\n",
    "        \n",
    "        TPR_data[i] = TPR_list[type+snr_list[i]]\n",
    "        \n",
    "    # central_point = np.array([9,18,36,72])\n",
    "        \n",
    "    # plt.scatter(['5-12','12-24','24-48','48-96'], FPR_data, label = type)\n",
    "    plt.plot(['5-12','12-24','24-48','48-96'], TPR_data, marker = 'o', label = type)\n",
    "    \n",
    "    \n",
    "# plt.xlim(0,96)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlabel('SNR range')\n",
    "plt.ylabel('TPR')\n",
    "\n",
    "plt.title('TPR distribution versus SNR, FPR fixed at 0.001')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_list = ['5-12', '12-24', '24-48', '48-96']\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "for type in ['bbh', 'sglf', 'sghf']:\n",
    "    \n",
    "    TPR_data = np.empty(4)\n",
    "    \n",
    "    for i in range(4):\n",
    "        \n",
    "        TPR_data[i] = TPR_list[type+snr_list[i]]\n",
    "        \n",
    "    # central_point = np.array([9,18,36,72])\n",
    "        \n",
    "    # plt.scatter(['5-12','12-24','24-48','48-96'], FPR_data, label = type)\n",
    "    plt.plot(['5-12','12-24','24-48','48-96'], TPR_data, marker = 'o', label = type)\n",
    "    \n",
    "    \n",
    "# plt.xlim(0,96)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlabel('SNR range')\n",
    "plt.ylabel('TPR')\n",
    "\n",
    "plt.title('TPR distribution versus SNR, FPR fixed at 0.01')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "\n",
    "np.sum(listResult['ans'][cnt][-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 20\n",
    "\n",
    "np.sum(listResult['ans'][cnt][-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "\n",
    "np.sum(listResult['ans'][cnt][N_wsl['glitch']:N_wsl['glitch']+N_wsl['noise']] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcd[datatype][-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:][not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(not_select[-N_wsl['bbh']-N_wsl['sglf']-N_wsl['sghf']:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = torch.load(\"../Data_cached/SequentialTraining/WSL/trained_model\" + \"\".join(['4','0']) + \"_\"+version+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['noise_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "# glitch_filtered = dataset_wsl_fft['glitch']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    plt.title(\"Performance of Final WSC\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['BBH_CNN_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "# glitch_filtered = dataset_wsl_fft['glitch']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    plt.title(\"Performance of Final WSC\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v12\n",
    "\n",
    "model = models['Final_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "# glitch_filtered = dataset_wsl_fft['glitch']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    plt.title(\"Performance of Final WSC\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v15\n",
    "\n",
    "model = models['Final_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "# glitch_filtered = dataset_wsl_fft['glitch']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    plt.title(\"Performance of Final WSC\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['glitch_WSC_L']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['glitch_WSC_H']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,101:]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,101:]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC H\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,101:]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14\n",
    "\n",
    "model = models['Final_WSC']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise']\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "# glitch_filtered = dataset_wsl_fft['glitch']\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "# dcd_glitch = model(torch.FloatTensor(glitch_fft))[1].detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    plt.title(\"Performance of Final WSC\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14\n",
    "\n",
    "model = models['glitch_WSC_L']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14\n",
    "\n",
    "model = models['glitch_WSC_H']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,101:]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,101:]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC H\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,101:]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutList['glitch_L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14\n",
    "\n",
    "model = trainAE_Onedetector(dataset_ae_fft['glitch'][:,:101], 'test', 'test', 'test')\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,:101]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,:101]))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,:101]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,:101]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "plt.axvline(cutList['glitch_L'][ic[2]], color=\"k\", linestyle=\"--\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14\n",
    "\n",
    "model = models['glitch_L']\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,:101]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,:101]))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,:101]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,:101]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "plt.axvline(cutList['glitch_L'][ic[2]], color=\"k\", linestyle=\"--\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14\n",
    "\n",
    "model = models['glitch_H']\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,101:]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,101:]))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,101:]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,101:]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "plt.axvline(cutList['glitch_H'][ic[3]], color=\"k\", linestyle=\"--\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test training the glitch AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {};\n",
    "models[\"glitch_L\"] = torch.load('../Model_cached/1_det_L_glitch_trained_with_permutation_1.pt')\n",
    "models[\"glitch_H\"] = torch.load('../Model_cached/1_det_H_glitch_trained_with_permutation_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wsl_fft_filtered = dataset_wsl_fft_collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = trainAE_Onedetector(dataset_ae_fft['glitch'][:,:101], 'test', 'test', 'glitch')\n",
    "# models['glitch_L'] = model\n",
    "\n",
    "# model = trainAE_Onedetector(dataset_ae_fft['glitch'][:,101:], 'test', 'test', 'glitch')\n",
    "# models['glitch_H'] = model\n",
    "\n",
    "dcd = models['glitch_L'](torch.FloatTensor(dataset_wsl_fft_filtered[:,:101]))[1].detach().numpy()\n",
    "\n",
    "dataset0 = dataset_ae_fft['glitch'][:,:101]\n",
    "dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered[:,:101]-dcd, axis=1) > cutList['glitch_L'][1]][:,:101]\n",
    "\n",
    "model_L = trainWSC_Onedetector(dataset0, dataset1, 'test', 'test', 'glitch')\n",
    "models[\"glitch_WSC_L\"] = model_L\n",
    "\n",
    "\n",
    "dcd = models['glitch_H'](torch.FloatTensor(dataset_wsl_fft_filtered[:,101:]))[1].detach().numpy()\n",
    "\n",
    "dataset0 = dataset_ae_fft['glitch'][:,101:]\n",
    "dataset1 = dataset_wsl_fft_filtered[np.var(dataset_wsl_fft_filtered[:,101:]-dcd, axis=1) > cutList['glitch_H'][0]][:,101:]\n",
    "\n",
    "model_H = trainWSC_Onedetector(dataset0, dataset1, 'test', 'test', 'glitch')\n",
    "models[\"glitch_WSC_H\"] = model_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcd = models['glitch_L'](torch.FloatTensor(dataset_wsl_fft['noise'][:,:101]))[1].detach().numpy()\n",
    "\n",
    "dataset0 = dataset_ae_fft['glitch'][:,:101]\n",
    "dataset1 = dataset_wsl_fft['noise'][np.var(dataset_wsl_fft['noise'][:,:101]-dcd, axis=1) > cutList['glitch_L'][3]][:,:101]\n",
    "\n",
    "model_L = trainWSC_Onedetector(dataset0, dataset1, 'test', 'test', 'glitch')\n",
    "models[\"glitch_WSC_L\"] = model_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_L']\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,:101]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,:101]))[1].detach().numpy()\n",
    "\n",
    "for key in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,:101]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,:101]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "\n",
    "    for snr in snr_range:\n",
    "        key_to_plot = key+snr\n",
    "        dcd_signal = model(torch.FloatTensor(dataset_wsl_fft[key_to_plot][:,:101]))[1].detach().numpy()\n",
    "        plt.hist(np.var(dataset_wsl_fft[key_to_plot][:,:101] - dcd_signal, axis = 1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=key_to_plot)\n",
    "\n",
    "\n",
    "\n",
    "    plt.title(\"trained with Livinston noise\")\n",
    "    plt.axvline(cutList['glitch_L'][0], color=\"k\", linestyle=\"--\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_WSC_L']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_WSC_L']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_WSC_L']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,:101]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,:101]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC L\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,:101]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.ylim(0,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_H']\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,101:]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,101:]))[1].detach().numpy()\n",
    "\n",
    "foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,101:]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "# foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "# foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,101:]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "plt.title(\"trained with noise\")\n",
    "plt.axvline(cutList['glitch_H'][0], color=\"k\", linestyle=\"--\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "model = models['glitch_H']\n",
    "\n",
    "dcd_bkg = model(torch.FloatTensor(dataset_wsl_fft['noise'][:,101:]))[1].detach().numpy()\n",
    "# dcd_bbh = model(torch.FloatTensor(bbh_filtered))[1].detach().numpy()\n",
    "# dcd_sg = model(torch.FloatTensor(sg_filtered))[1].detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(dataset_ae_fft['glitch'][:,101:]))[1].detach().numpy()\n",
    "\n",
    "for key in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    foo = plt.hist(np.var(dataset_wsl_fft['noise'][:,101:]-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(np.var(dataset_ae_fft['glitch'][:,101:]-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "\n",
    "    for snr in snr_range:\n",
    "        key_to_plot = key+snr\n",
    "        dcd_signal = model(torch.FloatTensor(dataset_wsl_fft[key_to_plot][:,101:]))[1].detach().numpy()\n",
    "        plt.hist(np.var(dataset_wsl_fft[key_to_plot][:,101:] - dcd_signal, axis = 1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=key_to_plot)\n",
    "\n",
    "\n",
    "\n",
    "    plt.title(\"trained with Hanford noise\")\n",
    "    plt.axvline(cutList['glitch_H'][1], color=\"k\", linestyle=\"--\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "model = models['glitch_WSC_H']\n",
    "\n",
    "bkg_filtered = dataset_wsl_fft['noise'][:,101:]\n",
    "# bbh_filtered = dataset_wsl_fft['bbh']\n",
    "# sg_filtered = dataset_wsl_fft['sg']\n",
    "glitch_filtered = dataset_wsl_fft['glitch'][:,101:]\n",
    "\n",
    "# c0[currentStep] = 0.0022;\n",
    "    \n",
    "dcd_bkg = nn.Sigmoid()(model(torch.FloatTensor(bkg_filtered))).detach().numpy()\n",
    "# dcd_bbh = nn.Sigmoid()(model(torch.FloatTensor(bbh_filtered))).detach().numpy()\n",
    "# dcd_sg = nn.Sigmoid()(model(torch.FloatTensor(sg_filtered))).detach().numpy()\n",
    "dcd_glitch = model(torch.FloatTensor(glitch_filtered)).detach().numpy()\n",
    "\n",
    "for signal_type in ['bbh', 'sglf', 'sghf']:\n",
    "\n",
    "    # foo = plt.hist(np.var(bkg_filtered-dcd_bkg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(np.var(bbh_filtered-dcd_bbh, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(np.var(sg_filtered-dcd_sg, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"sg\")\n",
    "    # foo = plt.hist(np.var(glitch_fft-dcd_glitch, axis=1), histtype=\"step\", range=(0, 0.01), bins=50, density=True, label=\"glitch\")\n",
    "    foo = plt.hist(dcd_bkg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"noise\")\n",
    "    # foo = plt.hist(dcd_bbh, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"bbh\")\n",
    "    # foo = plt.hist(dcd_sg, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"sg\")\n",
    "    foo = plt.hist(dcd_glitch, histtype=\"step\", range=(0, 1), bins=50, density=True, label=\"glitch\")\n",
    "    plt.title(\"Performance of Glitch WSC H\")\n",
    "        \n",
    "        \n",
    "    for snr in snr_range:\n",
    "\n",
    "        key = signal_type+snr\n",
    "        dcd_signal = nn.Sigmoid()(model(torch.FloatTensor(dataset_wsl_fft[key][:,101:]))).detach().numpy()\n",
    "        foo = plt.hist(dcd_signal, histtype=\"step\", range=(0, 1), bins=50, density=True, label=key)\n",
    "\n",
    "\n",
    "    # plt.axvline(c0[currentStep], color=\"k\", linestyle=\"--\")\n",
    "    # plt.axvline(np.sort(dcd_bbh.flatten())[int(0.1 * len(bbh_filtered))], color = 'k', linestyle = '--')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
